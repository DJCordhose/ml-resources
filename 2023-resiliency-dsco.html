<!doctype html>
<html lang="de">

<head>
    <meta charset="utf-8">

    <title>Resilient Machine Learning</title>

    <meta name="description" content="Resilient Machine Learning">
    <meta name="author" content="Oliver Zeigermann">
	<link rel="shortcut icon" href="/img/favicon.ico" >

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<!-- <link rel="stylesheet" href="reveal.js/dist/theme/black.css"> -->
	<link rel="stylesheet" href="reveal.js/dist/theme/white.css">

	<!-- Theme used for syntax highlighted code -->
	<!-- <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css"> -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/zenburn.css">
	<style>
		.right-img {
			margin-left: 10px !important;
			float: right;
			height: 500px;
		}

		.todo:before {
			content: 'TODO: ';
		}

		.todo {
			color: red !important;
		}

		code span.line-number {
			color: lightcoral;
		}

		.reveal pre code {
			max-height: 1000px !important;
		}

		img {
			border: 0 !important;
			box-shadow: 0 0 0 0 !important;
			height: 450px;
		}

		.reveal {
			-ms-touch-action: auto !important;
			touch-action: auto !important;
		}

		.reveal h1,
		.reveal h2,
		.reveal h3,
		.reveal h4 {
			/* letter-spacing: 2px; */
			font-family: 'Calibri', sans-serif;
			/* font-family: 'Times New Roman', Times, serif; */
			/* font-weight: bold; */
			color: black;
			/* font-style: italic; */
			/* letter-spacing: -2px; */
			text-transform: none !important;
		}

		.reveal em {
			font-weight: bold;
		}

		.reveal section img {
			background: none;
		}

		.reveal img.with-border {
			border: 1px solid #586e75 !important;
			box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
		}

		.reveal li {
			margin-bottom: 8px;
		}

		/* For li's that use FontAwesome icons as bullet-point */
		.reveal ul.fa-ul li {
			list-style-type: none;
		}

		.reveal {
			/* font-family: 'Work Sans', 'Calibri'; */
			font-family: 'Calibri';
			color: black !important;
			font-size: xx-large;
		}

		.container {
			display: flex;
		}

		.col img {
			height: auto;
		}

		.col,
		col-1 {
			flex: 1;
		}

		.col-2 {
			flex: 2;
		}

	/* https://intranet.openknowledge.de/plugins/servlet/mobile?contentId=18612939#content/view/18612939
      https://intranet.openknowledge.de/pages/viewpage.action?pageId=16157867
      https://openknowledgede.sharepoint.com/:p:/s/slides/EYWbMPz2ejhBprBrP8BYGxsBA3BrPNhDxJBrTXXvHi3-ZQ?e=jMZihi
       */
       body:after {
        content: url(img/ok/logo.png) ;
        position: fixed;
        bottom: 80px;
        left: -1080px;
        transform: scale(.06);
        height: 10px;
    }

	</style>
</head>

<body style="background-color: whitesmoke;">

<div class="reveal">
    <div class="slides">


<!-- 
---

Resilient Machine Learning
- Unwanted Bias
  - https://www.techtarget.com/searchenterpriseai/definition/machine-learning-bias-algorithm-bias-or-AI-bias
- Adversarial Attacks
  - http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture16.pdf
    - https://www.youtube.com/watch?v=CIfsB_EYsVI&list=PLSVEhWrZWDHQTBmWZufjxpw3s8sveJtnJ
  - https://github.com/smasis001/mlconf-2022
  - https://odsc.medium.com/detecting-adversarial-attacks-with-subset-scanning-8a53c360512f
  * Seldon Alibi Detect: https://docs.seldon.io/projects/alibi-detect/en/stable/ad/methods.html
  * ART
    * https://developer.ibm.com/articles/applying-the-adversarial-robustness-toolbox/
    * https://github.com/Trusted-AI/adversarial-robustness-toolbox

- Out-of-distribution Robustness
  - http://www.gatsby.ucl.ac.uk/~balaji/balaji-odsc-talk.pdf
- Reliable measure of certainty of prediction  
  - http://www.gatsby.ucl.ac.uk/~balaji/balaji-odsc-talk.pdf
- Drift
- Stability
  - https://djcordhose.github.io/ai/2019_m3_embeddings.html#/32
  - https://djcordhose.github.io/ai/2019_m3_embeddings.html#/36
- Fallbacks / Ensembles
  - https://danshiebler.com/2022-07-04-resilient-machine-learning/


Resilience is known as the ability to adapt to difficult or unexpected situations. Such a phenomenon also exists in the field of machine learning, where we have to deal with adversarial attacks, 
out-of-distribution robustness, and drift. Model stability must be ensured during retraining and post-training.
Measures include monitoring machine learning models, detecting outliers, and running with fallbacks and/or multiple models as an ensemble.
Clever model selection for production can also go a long way. In this talk, I explain the phenomena mentioned and address the appropriate measures for each.

---
https://dsco.usfdatainstitute.org

Resilient Machine Learning

Resilience is known as the ability to adapt to difficult or unexpected situations. Such a phenomenon also exists in the field of machine learning, where we have to deal with adversarial attacks, out-of-distribution robustness, drift, and unwanted bias. Additionally, model stability must be ensured during retraining and post-training.
Measures include monitoring machine learning models, detecting outliers, and running with fallbacks and/or multiple models as an ensemble.
Clever model selection for production can also go a long way. 

In this talk, I explain the phenomena mentioned and address the appropriate measures based on an example in the field of computer vision, which poses additional challenges.


Bio: Oliver is a software developer and architect from Hamburg, Germany. He has been developing software with different
approaches and programming languages for more than 3 decades. Lately, he has been focusing on Machine Learning and its interactions with humans.

Title: Head of AI / AI Strategist

https://www.linkedin.com/in/oliver-zeigermann-34989773/
---
M3
https://www.m3-konferenz.de/cfp.php

Resilientes Machine Learning

Resilienz ist die Fähigkeit der Anpassung an schwierige oder unerwartete Situationen.
Eine solche Widerstandskraft kann es bei Menschen geben oder eben auch in Softwaresystemen.
Bei klassischen Softwaresystemen betrifft Resilienz häufig nur die Ausfallsicherheit.
Mit der Nutzung von Machine Learning innerhalb der Systeme kommen viele weitere Aspekte hinzu. Eine Besonderheit ist
hier eine komplexe Geschäftslogik, die als ein abstraktes Artefakt vorliegt. Diese unterliegt nicht mehr vollständig der
Kontrolle der Entwickler.
Als Konsequenz müssen wir uns mit Phänomenen wie Adversarial Attacks, Out-of-Distribution Robustness und Drift
auseinandersetzen.
Die Stabilität des Modells muss hier bei Neutraining und Nachtraining gewährleistet werden.
Maßnahmen umfassen das Monitoring von Machine Learning-Modellen, die Erkennung von Ausreißern und den Betrieb mit
Fallbacks und/oder mehreren Modellen als Ensemble.
Auch eine geschickte Auswahl des Modells für die Produktion kann schon viel bewirken.
In diesem Talk erläutere ich die genannten Phänomene und gehe auf die jeweils passenden Maßnahmen ein.

Kurze Version


Resilienz ist die Fähigkeit der Anpassung an schwierige oder unerwartete Situationen.

Im Bereich des Machine Learnings müssen wir uns dabei mit Phänomenen wie Adversarial Attacks, Out-of-Distribution Robustness und Drift auseinandersetzen.
Die Stabilität des Modells muss hier bei Neutraining und Nachtraining gewährleistet werden.
Maßnahmen umfassen das Monitoring von Machine Learning-Modellen, die Erkennung von Ausreißern und den Betrieb mit Fallbacks und/oder mehreren Modellen als Ensemble.
Auch eine geschickte Auswahl des Modells für die Produktion kann schon viel bewirken.
In diesem Talk erläutere ich die genannten Phänomene und gehe auf die jeweils passenden Maßnahmen ein.

Vorkenntnisse

Grundverständnis vom Training von Machine Learning Modellen

Lernziele

Ein Einblick in die Welt des Machine Learnings in Produktion und was man beachten muss, Nachts gut schlafen zu können.
 -->

<!-- 
 <section data-markdown class="todo">
	<textarea data-template>

https://github.com/google-research/tuning_playbook
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Blue Collar Machine Learning

Näschste Version des Talks?
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>

Rex "garbage in" Douglass Ph.D. (@RexDouglass) twitterte um 7:15 PM on Mo., Jan. 30, 2023:
How to avoid machine learning pitfalls:
a guide for academic researchers
https://t.co/UdnvAiYVHU https://t.co/WnmoiKtAFg
(https://twitter.com/RexDouglass/status/1620123641626382338?t=5-CZHBLw5f7iE0TyHjPfJw&s=03) 
</textarea>
</section>


 <section data-markdown class="todo">
	<textarea data-template>
### Entwicklung

- M3 Talk, länger
  - Längere Agenda nutzen
  Mehr auf die Stabilisierung eingehen? (https://djcordhose.github.io/ai/2019_m3_embeddings.html#/29)
- https://mlconference.ai/machine-learning-advanced-development/resilience-machine-learning/
- Ansehen: https://mindfulmodeler.substack.com/p/the-way-of-model-agnostic-machine

- MLOps für OOP von diesem hier ableiten
- Nächste Version des Talks?
  - Enterprise ML: It's all about managing uncertainty
  - ML is all about managing uncertainty
  
  
</textarea>
</section>





<section data-markdown class="todo">
	<textarea data-template>
### Generalization and Bias are two sides of the same medal

The notion that the world has a simple structure that, once learned, enables good generalization everywhere is flawed. Many aspects---names of new people, location of potholes in a new city, etc.---are unpredictable. One has to adapt to them when needed by learning continually.
(https://twitter.com/KhurramJaved_96/status/1628410677101735936?t=HBWvj0f_KcxcTO3-jx4gow&s=03) 

</textarea>
</section> -->


 <section data-markdown>
	<textarea data-template>
# Resilient Machine Learning

March 2023, San Francisco, https://dsco.usfdatainstitute.org

Oliver Zeigermann

Slides: https://bit.ly/dsco-resiliency-2023

<!-- https://djcordhose.github.io/ml-resources/2023-resiliency-dsco.html -->

</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### TODO für DSCO

- Drift
 - kürzen
 - auf Englisch übersetzen
  
 - Fallbacks / Ensembles
   - https://danshiebler.com/2022-07-04-resilient-machine-learning/

- Zusammenfassung
  - anpassen
  - auf Englisch übersetzen

- Am Ende kürzen
  - max. 30 Folien 
</textarea>
</section>


<!-- <section data-markdown>
  <textarea data-template>
### Who is Olli

<div style="display: flex;">
<div style="flex: 50%;">
  <img src='img/olli-T.jpg' height="400">
</div>
<div style="flex: 50%; font-size: x-large;">
  <img src='img/olli-opa.jpeg'>
</div>
</div>

<p>
<a target="_blank" href="mailto:oliver.zeigermann@openknowledge.de">Oliver Zeigermann</a>:
Blue Collar Architect(ML)@<a href='https://www.openknowledge.de/'>OPEN KNOWLEDGE</a>
</p>    

</textarea>
</section>
 -->

<section data-markdown>
	<textarea data-template>
## Resilience

### ability to adapt to difficult or unexpected situations


https://en.wikipedia.org/wiki/Resilience
</textarea>
</section>


<!--
More complete agenda for talks longer than 30 minutes
  
<section data-markdown>
	<textarea data-template>
## Agenda

1. managing uncertainty
1. adversarial attacks and stability
1. out-of-distribution robustness
1. drift and monitoring
1. unwanted bias
1. fallbacks and ensembles
	</textarea>
</section>

-->


<section data-markdown>
	<textarea data-template>
## Agenda

1. managing uncertainty
1. adversarial attacks and stability
1. drift and monitoring
1. fallbacks and ensembles
	</textarea>
</section>


<section data-markdown>
	<textarea data-template>
## Agenda

1. *managing uncertainty*
1. adversarial attacks and stability
1. drift and monitoring
1. fallbacks and ensembles
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### ML comes with a lot of uncertainty

- model and training
  - score
  - confidence
  - training vs test vs out of sample, real world
- process
  - will the approach work at all
    - depends on what "work" means
    - what score is good enough?
    - what about the other requirements

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Uncertainty is hard to bear

- emotionally
- risk for business

- You need to manage the uncertainty / introduce resilient concepts to handle the stress 
- also process
  - PoC
  - try and infer life time of model
  </textarea>
</section>


<section data-markdown>
  <textarea data-template>
### Machine Learning Projects can be structured in phases

<img src='img/sketch/phases-ml.png'>

</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### What steps would you take when you want to start a new machine learning project

1. Enable your organization 
1. Identify a reasonable candidate
1. Enter stage 1 of technical experiments

</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### How you whish ML projects work

<img src='img/sketch/objective-joke.png'>

</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### How ML projects really work

<img src='img/sketch/objective-truth.png'>

</textarea>
</section>

<!-- <section data-markdown class="todo">
	<textarea data-template>
### Besonders bei LLM: Manage Uncertainty

- Stack Systems
- Ensemble
- Make a judgement what you would want to output 
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Besonders bei LLM: Staging

Manage Uncertainty using "staging" / proposals / augmentation and assistance 

https://ai.google/responsibilities/responsible-ai-practices/

---

Software has been basically deterministic until now

Until we fully trust LLM outputs, we will start to see a UX I call "staging"

Here are a few examples
(https://twitter.com/RealKevinYang/status/1616646347888943104?t=CGNrfDJl_8Zk7Tvq3j2HmA&s=03) 

</textarea>
</section>
 -->

<section data-markdown>
	<textarea data-template>
## Agenda

1. managing uncertainty
1. *adversarial attacks and stability*
1. drift and monitoring
1. fallbacks and ensembles
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Hacking the system / Adversarial Attacks

<img src="img/hacking-the-system.png">

https://www.instagram.com/reel/CkQUhLov9_u/?igshid=MDJmNzVkMjY=
</textarea>
</section>
    
<section data-markdown>
	<textarea data-template>
### Hacking the system is more common than you might think

taxes, laws, regulations, contracts, embargoes

- this is actually the job of a lot of people
- transparency vs hackability
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Attempted Manipulation / Adversarial Attacks

Certain inputs may intentionally cause a grossly incorrect prediction to be made
* can be used to systematically manipulate the output of the system
* how sensible a system is to such an attempt is determined by the inner complexity of such a system
  * often directly contradicting properties like explainability

	</textarea>
</section>

<!-- <section data-markdown>
	<textarea data-template>
### Explainability and Check against Overfitting

<img src="img/alibi-anchor-squirrel.png" style="height:400px;" class="fragment">		

https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html#Images		
</textarea>
</section>
 -->

<section data-markdown style="font-size: x-large;">
<textarea data-template>
### Instability

<div class="container">
<div class="col">
<br>
<br>


<img src="img/architecture/twitter-stability-1.png" style="height: 300px;">    
</div>
<div class="col">
	<img src="img/architecture/twitter-stability-2.png" style="height: 550px;">    
</div>

</div>

<small>

* https://twitter.com/elonmusk/status/1595457703739944976
* https://twitter.com/elonmusk/status/1593673339826212864

</small>

</textarea>
</section>
  

<section data-markdown>
	<textarea data-template>
### Model stability and adversarial vulnerability

Main question: does slightly perturbing the input data yield a drastically different risk or a different risk group. If so
- there is an additional attack vector because people could learn decision boundaries and by slightly tweaking features that do not require exact entry (like estimated miles per year) get into a better category, thus being able to hack the system
- high local variation hints towards undetected overfitting
- high local variation makes it likely that retraining with new data will yield a completely new model which also requires new interpretation etc. 

Links:
- https://en.wikipedia.org/wiki/Metamorphic_testing

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Check against Adversarial Attacks

Adversarial perturbation gives significantly different outputs at x and x'.
		
<img src="img/adversarialae.png" style="height:400px;" class="fragment">

<small>

https://docs.seldon.io/projects/alibi-detect/en/stable/ad/methods/adversarialae.html		
https://github.com/SeldonIO/alibi-detect#adversarial-detection

</small>

</textarea>
</section>


<!-- <section data-markdown>
  <textarea data-template>
### Handling Adversarial Attacks

* Bestimmte Eingaben können absichtlich eine grob falsche Vorhersage herbei führen 
  * Solche Eingaben können erkannt und korrigiert werden
  * Dazu kann z.B. ein Autoencoder benutzt werden, der die Eingabe korrigiert
* Libs
  * Seldon Alibi Detect: https://docs.seldon.io/projects/alibi-detect/en/stable/ad/methods.html
  * ART
    * https://developer.ibm.com/articles/applying-the-adversarial-robustness-toolbox/
    * https://github.com/Trusted-AI/adversarial-robustness-toolbox
</textarea>
</section>



<section data-markdown class="fragments">
<textarea data-template>
### Was kann man sonst noch machen

* Outlier-Detection
  * Unser Modell wird nicht extrapolieren können
  * Werte außerhalb des Trainings-Bereichs werden wahrscheinlich unrealistisch sicher vorhergesagt
  * Ausreißer müssen ohne Ground Truth entdeckt werden 
* Adversarial Detection
  * Bestimmte Eingaben können absichtlich eine grob falsche Vorhersage herbei führen 
  * Solche Eingaben können erkannt und korrigiert werden
  * Dazu kann z.B. ein Autoencoder benutzt werden, der die Eingabe korrigiert


<small>https://docs.seldon.io/projects/alibi-detect/en/stable/od/methods.html
<br>
https://docs.seldon.io/projects/alibi-detect/en/stable/ad/methods.html
</small>
</textarea>
</section>
 -->

 <section data-markdown>
	<textarea data-template>
## Agenda

1. managing uncertainty
1. adversarial attacks and stability
1. *drift and monitoring*
1. fallbacks and ensembles
	</textarea>
</section>



<section data-markdown>
  <textarea data-template>
### ML Systeme sind dynamisch

<img src='img/verfall-2.PNG'>

</textarea>
</section>

<section data-markdown class="fragments">
  <textarea data-template>
### Woher weiß man, dass man ein neues Modell in Produktion braucht?

* Schon in der Explorationsphase prüfen, wie sich das Modell auf neueren Daten verhält
  * Wie schnell degradiert die Performance?
  * Mindestens einmal im Jahr, damit man überhaupt noch weiß, wie es geht
* Wenn die Metrik des Modells in Produktion nachlässt
  * Dafür braucht man die Ground Truth der Daten aus Produktion
  * Manchmal bekommt man diese unmittelbar nach der Vorhersage durch die Reaktion eines menschlichen Benutzers
  * Oft aber auch erst nach nennenswerter Verzögerung 
* *Wenn sich die Verteilung der Daten der Anfragen oder Vorhersagen deutlich von der des Trainings unterscheidet* 

</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Verteilung?

* die wichtigste: Normalverteilung aka Gauß-Verteilung
* Histogramme (Binning)

<img src="img/nobel.svg">

</textarea>
</section>


<section data-markdown class="fragments">
  <textarea data-template>
### Verteilungen

<img src="img/causal-insurance/age-reference.png">

Das Alter der Versicherten zum Zeitpunkt des Trainings
</textarea>
</section>


<section data-markdown class="fragments">
<textarea data-template>
### Zum Zeitpunkt des Trainings?

<img src="img/drift_entschlackt.png" style="height: 100%;">

</textarea>
</section>

<!-- <section data-markdown class="fragments">
<textarea data-template>
### So sieht die Verteilung jetzt aus

<img src="img/causal-insurance/age-current.png">

</textarea>
</section>
-->
<section data-markdown class="fragments">
<textarea data-template>
### Driftet das?

<img src="img/causal-insurance/age_no_drift_p75.png">

</textarea>
</section>

<section data-markdown class="fragments">
<textarea data-template>
### Und das?

<img src="img/causal-insurance/age_drift_p0.png">

</textarea>
</section>

<section data-markdown class="fragments">
<textarea data-template>
### Wie erkennen wir Drift?

* Es wird ein statistischer Test auf den Verteilungen ausgeführt
* Die Anfragen in Produktion werden verglichen mit dem Referenz-Datensatz, den wir zum Training benutzt haben

</textarea>
</section>

<section data-markdown class="fragment">
<textarea data-template>
## Welcher statistischer Test / welche Metrik?

es gibt leider nicht den einen passenden Test

* manche passen nur gut für kleine (< 1000) Datenmengen
* unsere Datenmengen sind größer als 1000
* manche können nicht nur auf numerischen, sondern auch auf kategorischen Daten arbeiten
* wir brauchen beides
<!-- * manche sind zwischen 0 und 1 normiert
* das ist uns eher egal -->
</textarea>
</section>


<section data-markdown>
<textarea data-template>
### Speziell für Drift-Erkennung in Frage kommende Tests

* Kolmogorov-Smirnov-Test
* Population Stability Index
* Kullback-Leibler-Divergenz
* Jensen-Shannon-Distanz  
* Wasserstein-Distanz

https://evidentlyai.com/blog/data-drift-detection-large-datasets
</textarea>
</section>

<section data-markdown  class="fragments">
<textarea data-template>
### Unser Beispiel: Kolmogorov-Smirnov-Test

* numerisch
* wird gerne als Default gewählt, kann bei großen Datenmengen aber zu empfindlich sein
* Vorteil: keine Normalverteilung vorausgesetzt
* ein 'typischer' statistischer Test mit p als Rückgabewert
* Nullhypothese: die beiden Verteilungen sind gleich
* Drift bei p-Wert unter 0.05

*Wann nutzen?* Wenn Drift schnell entdeckt werden muss oder die Datenmengen kleiner ausfallen

</textarea>
</section>

<section data-markdown class="fragments">
<textarea data-template>
### P-Werte

* die statistische Verteilung der jeweiligen Features
* weicht diese in Produktion signifikant von der Verteilung im Training ab? (Nullhypothese: nein)
* diese Abweichung wird über eine Metrik berechnet
* es kommt eine Konfidenz heraus, ob die Nullhypothese stimmt
* unter 5% Konfidenz geht man von einer Abweichung aus <span style="font-size: 0.95rem">(1% oder 0.1% auch möglich)</span>
* das bedeutet, dass es eine 5%-Wahrscheinlichkeit gibt, dass die Verteilungen eigentlich doch gleich sind, man aber nur gerade schlechte Beispiele sieht

</textarea>
</section>

<section data-markdown>
<textarea data-template>
### P-Werte für unsere beiden potentiellen Drifts

<div class="container">
<div class="col">
<img src="img/causal-insurance/age_no_drift_p75.png" style="height: 100%;">
<em>p-Wert (Wahrscheinlichkeit der Nullhypothese) = 75%, also kein Drift</em>
</div>
<div class="col">
<img src="img/causal-insurance/age_drift_p0.png" style="height: 100%;">
<em>p-Wert (Wahrscheinlichkeit der Nullhypothese) < 0,1%, also Drift</em>

</div>
</div>

</textarea>
</section>

<section data-markdown style="font-size: x-large;">
	<textarea data-template>
## Drift erfordert Interpretation

Wenn die Welt sich ändert, ist Drift zu erwarten und damit ok

|   | Positive Interpretation, keine Maßnahme erforderlich  | Negative Interpretation, Maßnahme erforderlich  |
|---|---|---|
| *Data und Prediction Drift*  | wichtige Features haben sich geändert, Modell kommt klar und extrapoliert gut, z.B.: Höheres Alter, mehr Risiko  |  wichtige Features haben sich geändert, Modell extrapoliert nicht sinnvoll |
| *Data aber kein Prediction Drift*  | keine wichtigen Features geändert, das Modell ist robust genug für den Drift  | wichtige Features geändert, Modell extrapoliert nicht sinnvoll |
| *Prediction aber kein Data Drift*  | ???  | wahrscheinlich Concept Drift, neue Analyse der Situation notwendig |
|   |   |   |

</textarea>
</section>

<section data-markdown class="fragments">
## Maßnahmen bei Drift

* *Neue Version des Modells trainieren*
  * Neue Daten aufnehmen (und labeln)
  * Neue Features erzeugen
  * Modell-Architektur ändern (oder fixen) und neu trainieren
* Schnelle Maßnahme
  * Pre-/Post-Processing des Modells neu kalibrieren
  * Schwellwerte für Anwendung anpassen
  * Bestimmte Bereiche ausklammern 
* Sehr schnelle Maßnahme: Fallback
  * Manuell
  * Heuristik / Baseline
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Drift Detection für Bilder

* Input Drift mit Histogrammen von Low-Level-Features (HSV): https://towardsdatascience.com/detecting-semantic-drift-within-image-data-6a59a0e768c6
* Input Drift mit Dimensions-Reduktion: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_ks_cifar10.html 
* Model Drift durch Vergleich mit destilliertem Modell: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_distillation_cifar10.html 
* Ein komplettes Beispiel mit Alibi-Detect: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/alibi_detect_deploy.html#4.-Drift-detection-with-Kolmogorov-Smirnov
* Anomalien in Bildern: https://29a.ch/photo-forensics
* Drift kann auch auf Text festgestellt werden: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_text_imdb.html
</textarea>
</section>


<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Wann liegt Ground Truth vor?

*Menschliche Experten können die Ground Truth*
* *bestimmen* 
  * sobald ein Mensch die Entscheidungen nachprüft / revidiert
  * bei einem Proposal-System kann das sehr schnell sein
  * bei Dunkelverarbeitung sollte dies in regelmäßigen Abständen passieren
* *nicht bestimmen*
  * wir müssen Realitäten abwarten
  * bei Zeitreihen wird auf die nahe Zukunft vorhergesagt, sobald diese Eintritt kann überprüft werden
  * oft sind solche Realitäten erst nach einiger Zeit wahrnehmbar und unterliegen statistischen Schwankungen (wie bei uns)
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Unsere Vorhersage selbst erzeugt Drift

* Ein Deployment ändert das unsere Rolle von Beobachter zu Akteur
* Wir versichern nur Leute mit einer guten Risiko Prognose
  * Wenn nicht, warum sollten wir dann eine überhaupt eine Prognose machen?
* Unsere GT wird mehr und mehr gute Fahrer haben
  * Zumindest ist das unsere Hoffnung (sonst hätte die Prognose nicht geklappt)
* Falls nicht (False Negative, Type II Fehler)
  * haben Menschen gelernt, unser System auszutricksen?
  * "Dann versichert eben meine Tochter den Wagen"
* Haben wir gute Fahrer aus verstehen nicht versichert (False Positive, Type I Fehler)
  * Möglichkeit: *epsilon-greedy* meistens der Vorhersage glauben, aber manchmal (epsilon) auch einen Fahrer mit schlechter Prognose versichern 
  * Vorhersage mit allen Wahrscheinlichkeiten geht in unsere Datenbank ein

https://twitter.com/ChristophMolnar/status/1569644089724764160
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Agenda

1. managing uncertainty
1. adversarial attacks and stability
1. drift and monitoring
1. *fallbacks and ensembles*
	</textarea>
</section>



<section data-markdown>
	<textarea data-template>
### Realistischer ist das Deployment eines kompletten Services 

<img src="img/Typical_Deployment_of_a_Machine_Learning_Service.jpg">

Neues Problem: wir haben in Adapter Code einmal in Python und einmal in Java 
</textarea>
</section>


<section data-markdown class="fragments">
<textarea data-template>
### Zusammenfassung

1. Machine Learning Projekte können in Phasen gedacht werden
1. In der ersten Phase macht man möglichst schnelle Experimente
1. Sollte sich eine Idee als tragfähig erweisen, professionalisiert man die Idee
1. Dies ist Voraussetzung und Grundlage für Produktion
1. In Produktion ergeben sich besondere Herausforderung im Bereich Monitoring
1. Typischerweise müssen Machine Learning Systeme regelmäßig nachtrainiert und gepflegt werden
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
# Thanks a lot
## Resilient Machine Learning

Stay in Contact

https://www.linkedin.com/in/oliver-zeigermann-34989773/

oliver@zeigermann.de

Twitter: @DJCordhose

Slides: https://bit.ly/dsco-resiliency-2023
</textarea>
</section>




</div>
</div>

<script src="reveal.js/dist/reveal.js"></script>
<script src="lib/jquery.js"></script>
<script>
    const printMode = window.location.search.match(/print-pdf/gi);
    const isLocal = window.location.hostname.indexOf('localhost') !== -1 ||
        window.location.hostname.indexOf('127.0.0.1') !== -1;
    const isPresentation = isLocal && !printMode;
    const isPublic = !isPresentation;

    $('.hide').remove();

    if (isPresentation) {
    } else {
        // only applies to public version
        $('.todo').remove();
        $('.preparation').remove();
        $('.local').remove();
    }

    Reveal.addEventListener('ready', function (event) {
        // applies to all versions
        $('code').addClass('line-numbers');

        $('.fragments li').addClass('fragment')

        // make all links open in new tab
        $('a').attr('target', '_blank')

        if (isPresentation) {
            // only applies to presentation version
            Reveal.configure({ controls: false });
        } else {
            // only applies to public version
            $('.fragment').removeClass('fragment');
        }

        // we do not like fragments
        // $('.fragment').removeClass('fragment');

    });

</script>

<script src="reveal.js/plugin/notes/notes.js"></script>
<script src="reveal.js/plugin/markdown/markdown.js"></script>
<script src="reveal.js/plugin/highlight/highlight.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,
        controls: true,
        progress: true,
        history: true,
        center: true,
        width: 1100,
        slideNumber: true,
        hideInactiveCursor: false,
        transition: 'none', // none/fade/slide/convex/concave/zoom


        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
    });
</script>


</body>

</html>