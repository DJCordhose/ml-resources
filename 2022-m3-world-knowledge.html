<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>M3: World Knowledge Priors</title>

    <link rel="stylesheet" href="reveal.js/dist/reset.css">
    <link rel="stylesheet" href="reveal.js/dist/reveal.css">
    <!-- <link rel="stylesheet" href="reveal.js/dist/theme/black.css"> -->
    <link rel="stylesheet" href="reveal.js/dist/theme/white.css">

    <!-- Theme used for syntax highlighted code -->
    <!-- <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css"> -->
    <link rel="stylesheet" href="reveal.js/plugin/highlight/zenburn.css">
    <style>
        .right-img {
            margin-left: 10px !important;
            float: right;
            height: 500px;
        }

        .todo:before {
            content: 'TODO: ';
        }

        .todo {
            color: red !important;
        }

        code span.line-number {
            color: lightcoral;
        }

        .reveal pre code {
            max-height: 1000px !important;
        }

        img {
            border: 0 !important;
            box-shadow: 0 0 0 0 !important;
            height: 450px;
        }

        .reveal {
            -ms-touch-action: auto !important;
            touch-action: auto !important;
        }

        .reveal h1,
        .reveal h2,
        .reveal h3,
        .reveal h4 {
            /* letter-spacing: 2px; */
            font-family: 'Calibri', sans-serif;
            /* font-family: 'Times New Roman', Times, serif; */
            /* font-weight: bold; */
            color: black;
            /* font-style: italic; */
            /* letter-spacing: -2px; */
            text-transform: none !important;
        }

        .reveal em {
            font-weight: bold;
        }

        .reveal section img {
            background: none;
        }

        .reveal img.with-border {
            border: 1px solid #586e75 !important;
            box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
        }

        .reveal li {
            margin-bottom: 8px;
        }

        /* For li's that use FontAwesome icons as bullet-point */
        .reveal ul.fa-ul li {
            list-style-type: none;
        }

        .reveal {
            /* font-family: 'Work Sans', 'Calibri'; */
            font-family: 'Calibri';
            color: black !important;
            font-size: xx-large;
        }

        .container {
            display: flex;
        }

        .col,
        col-1 {
            flex: 1;
        }

        .col-2 {
            flex: 2;
        }
    </style>

</head>

<body style="background-color: whitesmoke;">

    <div class="reveal">
        <div class="slides">

            <!-- 
https://www.m3-konferenz.de/veranstaltung-14113-0-wie-kann-man-weltwissen-ins-machine-learning-einbringen.html
3.6.2022: 10:15 - 11:00
45 Minuten

Wie kann man Weltwissen ins Machine Learning einbringen?

Machine-Learning-Modelle verf√ºgen a priori nicht √ºber allgemeines Weltwissen. Dieser Mangel l√§sst sie oft kl√§glich
scheitern, vor allem, wenn sie auf Bereiche extrapolieren, die nicht durch Trainingsdaten abgedeckt sind.

Es gibt viele Beispiele f√ºr Dinge, die wir Menschen wissen, aber einem ML-System erst einmal beigebracht werden m√ºssen.
Dazu z√§hlen bestimmte Invarianten in der Bilderkennung wie, z.B. die Tatsache, dass ein Objekt im Wesentlichen gleich
aussieht, egal wo es sich befindet. Weitere Beispiele sind das Wissen, dass kein Mensch √ºber 150 Jahre alt ist und Autos
typischerweise nicht die Schallgeschwindigkeit erreichen.

Dieser Vortrag ist ein √úberblick √ºber bekannte Methoden, dieses Wissen in einen Machine-Learning-Prozess einzubringen:
die Wahl des richtigen Losses, die Erzwingung von Sparsity, die Wahl guter Dimensionen, Lattices, Arten von
Netzwerkschichten und ‚Äì nicht zuletzt ‚Äì augmentierte Trainingsdaten.
            -->

<section data-markdown>
    <textarea data-template>
## Wie kann man Weltwissen ins Machine Learning einbringen?                

M3 2022, https://www.m3-konferenz.de/veranstaltung-14113-0-wie-kann-man-weltwissen-ins-machine-learning-einbringen.html

Oliver Zeigermann / oliver.zeigermann@openknowledge.de

Mikio Braun / mikiobraun@gmail.com

### Folien: https://bit.ly/m3-2022-world-knowledge

</textarea>
</section>


<section data-markdown>
    <textarea data-template>
### Wo ist der Schlumpf?

<video src="img/smurf/smurf-short.mp4" controls>

</textarea>
</section>

<section data-markdown class="fragments">
    <textarea data-template>
## Hier geht es nicht so sehr darum, ob dies ein gro√üartiger Trick ist, sondern eher darum, dass...

* **Objektpermanenz** https://en.wikipedia.org/wiki/Object_permanence
* ein Wissen √ºber die Welt ist, dessen wir uns so sicher sind
* dass wir gestresst werden, wenn es in Frage gestellt wird
* und denken sofort: Wo ist es hin?
* Grundlage f√ºr eine gro√üe Anzahl von Zaubertricks
        
</textarea>
</section>

<section data-markdown class="fragments">
    <textarea data-template>
### Weltwissen 

* Wir Menschen haben ein grundlegendes Verst√§ndnis von der Welt.
* sei es √ºber die Physik normaler Objekte oder √ºber die Eigenschaften von Menschen. 
* wir erwarten dies ebenso von einem intelligenten automatischen System
* erf√ºllt ein System dies nicht sind wir entt√§uscht und unser Vertrauen in das System sinkt 

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Wer ist Mikio
		<img src='img/mikio-data.jpeg'>

		<a target="_blank" href="mailto:mikiobraun@gmail.com">Mikio Braun</a>:
		Ex-ML Researcher, Architekt, Berater und Mentor f√ºr Machine Learning
	</textarea>
</section>


<section data-markdown>
	<textarea data-template>
		### Wer ist Olli

		<div style="display: flex;">
			<div style="flex: 50%;">
				<a href='https://oreilly.de/produkt/machine-learning-kurz-gut-2/'>
				<img src='img/ml-buch-v2.jpg' height="400">
				</a>
			</div>
			<div style="flex: 50%; font-size: x-large;">
				<img src='img/olli-opa.jpeg'>
			</div>
		</div>
		<p>
			<a target="_blank" href="mailto:OliverZeigermann@gmail.com">Oliver Zeigermann</a>:
		Entwickler, Architekt, Berater und Coach f√ºr Machine Learning
		</p>    
	</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Modeling choices encode strong assumptions about the data

<img src="img/world-knowledge/fchollet-model-priors.png">

https://twitter.com/fchollet/status/1439799099176357894
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Priors

<div class="container">
<div class="col">
<img src="img/world-knowledge/weak-prior.jpeg">
</div>
<div class="col">
    <img src="img/world-knowledge/strong-prior.jpeg">
    </div>
    </div>

https://twitter.com/fchollet/status/1450871559803916290
</textarea>
</section>

<section data-markdown>
    <textarea data-template>

        ## ML: The High-Level View

        <img src="img/mikio/ml-wk-overview.png">
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Agenda

1. Basic Model Architecture
1. Advanced Model Architecture
1. Smart regularization
1. What else is there?
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>

<!-- Part 1 --------------------------------------------------------------------------- -->
<div class="container">
    <div class="col">

## Agenda

1. _Basic Model Architecture_
1. Advanced Model Architecture
1. Smart regularization
1. What else is there?
        
    </div>
    <div class="col">
<img src="img/mikio/ml-wk-basic-model.png" style="height:300px;">

    </div>
</div>

    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Example: Regression

<img src="img/world-knowledge/regression.png">

https://colab.research.google.com/github/DJCordhose/ml-resources/blob/main/notebooks/priors/regression.ipynb?hl=en

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## What is the true model for that data?

<img src="img/world-knowledge/regression.png">

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## This or that?

<div class="container">
    <div class="col">
        <img src="img/world-knowledge/regression-linear.png">
    </div>
    <div class="col">
        <img src="img/world-knowledge/regression-non-linear.png">
    </div>
</div>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## ... and for that?

<img src="img/world-knowledge/seq.png">

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## MLP does not have all the knowledge we have

<img src="img/world-knowledge/seq-mlp.png">

3 layers, 1500 nodes each, relu acitvation
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## This is a sequence, each point is a continuation of the previous ones

<img src="img/world-knowledge/seq-rnn.png">

Simple RNN, 1 layer with 50 nodes, relu acitvation, 30 previous values used
</textarea>
</section>

<section data-markdown class="fragments">
    <textarea data-template>
### Encode priors in neural network architecture
## Basics

* _capacity of the model_
  * more complex model architectures lead to more complex functions
  * it can be shown: 3 layers with enough neurons and ReLU activation can learn any function
  * simplest case: a single neuron giving a linear model
* if values are _sequential_, i.e. the next value depends on what we had before
  * use RNN to model
  * LSTM / GRU allow to take more of the past into account and create a complex model
  * simple RNN only uses near past (right choice for our example)

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Encode priors in neural network architecture
## Regularization
    
* L1 as sparsity enforcing prior
    * Use L1 if you know your images only contain few pixels
* L2 enforcing small values as prior
    * keep a certain parameter close a specific value
    * great visual example: https://twitter.com/matthen2/status/1520427990420791298   
    
</textarea>
</section>

<!-- Part 2 --------------------------------------------------------------------------- -->
<section data-markdown>
    <textarea data-template>
<div class="container">
    <div class="col">

## Agenda

1. Basic Model Architecture
1. _Advanced Model Architecture_
1. Smart regularization
1. What else is there?
    </div>
    <div class="col">
<img src="img/mikio/ml-wk-advanced.png" style="height:250px;">
    </div>
</div>
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Example: Denoising using CNNs with Autoencoders

<img src="img/world-knowledge/denoising.png">

https://colab.research.google.com/github/DJCordhose/ml-resources/blob/main/notebooks/priors/autoencoder-denoising.ipynb?hl=en

</textarea>
</section>

<section data-markdown class="fragments">
    <textarea data-template>
### Encode priors in neural network architecture
## CNNs for Images
        
* in images, adjacency of pixels has a meaning
* objects are connected
* features in an objects are translation invariant  
* use CNN to encode that knowledge into the network
* same filter going over all parts of the image
* much less parameters

</textarea>
</section>

<section data-markdown class="fragments">
    <textarea data-template>
### Encode priors in neural network architecture
## Autoencoders for Abstraction
        
* assumption: there are underlying concepts that can be used for abstraction
* project instances to latent representation
* the less the capacity of the latent representation, the higher the abstraction
* means to restrict capacity:
  * _undercomplete_ (capacity of latent representation << dim of inputs)
    * L2 to further compress latent a bit
  * _overcomplete_ (capacity of latent representation >= dim of inputs)
    * use L1 to increase sparsity of embedding when having dim > 2, to have an additional restriction (often gives better results)

</textarea>
</section>

<!-- Part 3 --------------------------------------------------------------------------- -->

<section data-markdown>
    <textarea data-template>
        <div class="container">
            <div class="col">

## Agenda

1. Basic Model Architecture
1. Advanced Model Architecture
1. _Smart regularization_
1. What else is there?
                            
</div>
<div class="col">

<img src="img/mikio/ml-wk-regularization.png" style="height:300px;">

</div>
        </div>

    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Example: Classification for areas with no training data

<div class="container">
    <div class="col">
        <img src="img/world-knowledge/extrapolation.png" style="height: 300px">
        Like this?
    </div>
    <div class="col">
        <img src="img/world-knowledge/adversarial_extrapolation.png" style="height: 300px">
        Or like that?
    </div>
</div>
<small>

https://colab.research.google.com/github/DJCordhose/ml-resources/blob/main/notebooks/priors/interpolation.ipynb?hl=en
<br>
https://colab.research.google.com/github/DJCordhose/ml-resources/blob/main/notebooks/priors/extrapolate_nsl.ipynb?hl=en
<!-- <br>
https://colab.research.google.com/github/DJCordhose/ml-resources/blob/main/notebooks/priors/extrapolate_lattice.ipynb?hl=en -->

</small>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Interpolation

<img src="img/world-knowledge/interpolation.png">

Works well 
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Extrapolation

<img src="img/world-knowledge/extrapolation.png">

How would we even assume to extrapolate?
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### How to extrapolate?

* Requires general world knowledge
* Often also domain knowledge
* Also: making active choices in modelling
  * no right or wrong

</textarea>
</section>

<section data-markdown class="fragments">
    <textarea data-template>
### Do we need more means of control?

* standard regularizers usually result in "sensible" extrapolation
* no guarantee across the whole space, esp. for high dimensions
* simpler models give more control, but accuracy might be much worse
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Lattice based models

* lattices are functions that interpolate between a points on a (high-dim) regular grid
* encode domain knowledge like monotonicity, convexity, feature relationships
* avoid unexpected model behavior on data far from training data

https://www.tensorflow.org/lattice
<br>
https://blog.tensorflow.org/2020/02/tensorflow-lattice-flexible-controlled-and-interpretable-ML.html
<br>
https://github.com/tensorflow/lattice

</textarea>
</section>

<section>
<h3>How would this look like in Code?</h3>

<pre>
    <code data-trim data-line-numbers="1|3-5|7-11|13-17|19-20,30|21-22|23-26|27-28"><script type="text/template">
model = tf.keras.Sequential()

# combine all calibrators in parallel
combined_calibrators = tfl.layers.ParallelCombination()
model.add(combined_calibrators)

# age, piecewise linear function
combined_calibrators.append(tfl.layers.PWLCalibration(
    input_keypoints=np.linspace(18, 100),
    # feeding into a lattice with 2 vertices
    output_min=0.0, output_max=1.0))

# speed, piecewise linear function
combined_calibrators.append(tfl.layers.PWLCalibration(
    input_keypoints=np.linspace(80, 160),
    # feeding into a lattice with 3 vertices
    output_min=0.0, output_max=2.0))

# lattice is an interpolated look-up table that can approximate arbitrary input-output relationships
lattice = tfl.layers.Lattice(
    # one per calibrator, needs to match their domain
    lattice_sizes=[2, 3],
    # one per calibartor, all increasing monotonically
    monotonicities=[
        'increasing', 'increasing'
    ],
    # normalized
    output_min=0.0, output_max=1.0)
) 
model.add(lattice)
    </script></code>
</pre>

<p>API is pretty wild, cohesion of information often not given, many examples still use outdated Estimator API</p>

</section>

<section data-markdown>
    <textarea data-template>
### Neural Structured Learning (NSL)

Two examples:
* _Neural Graph Learning_: extending graph regularization (= close nodes should be more similar) to DL https://research.google/pubs/pub46568.pdf
* _Adversarial Learning_:  https://arxiv.org/pdf/1412.6572.pdf
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Adversarial Learning

* original objective is to prevent adversarial attacks
* learning can be modified to protect against sensitive inputs
* also leads to smoother decision boundaries

<img src="img/mikio/ml-wk-adversarial.png" style="height:200px;">

https://arxiv.org/pdf/1412.6572.pdf
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Adversarial Interpolation

<img src="img/world-knowledge/adversarial_interpolation.png">

Subtle details
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Adversarial Extrapolation

<img src="img/world-knowledge/adversarial_extrapolation.png">

Strong difference, actively avoiding the linear extrapolation
</textarea>
</section>

<section>
<h3>Encoding soft Penalties instead of hard Constraints</h3>

<p>Adding to the loss as some form of regularization makes this easy to embed into standard backprop</p>
<div class="container">
    <div class="col"  style='width: 500px;'>
<pre>
    <code data-trim data-line-numbers="1-4|6-10|12-14"><script type="text/template">
model = tf.keras.Sequential()
model.add(InputLayer(input_shape=(3,)))
# ...
model.add(Dense(units=3, activation='softmax'))

# Wrap the model with adversarial regularization
adv_config = make_adv_reg_config(multiplier=0.2, 
                                 adv_step_size=0.05)
model = AdversarialRegularization(model, 
                                  adv_config=adv_config)

model.compile(loss='sparse_categorical_crossentropy',
                optimizer='adam',
                metrics=['accuracy'])

    </script></code>
</pre>
            </div>
    <div class="col">
        <img src="img/world-knowledge/adversarial-training.png" style='width: 500px;'>
    </div>

</section>


<!-- <section data-markdown>
    <textarea data-template>
### Example constraints

* Monotonicity: risk should increase with respect to age and speed
* Unimodality: there is a single sweet spot in age (between 30 and 60)


</textarea>
</section> -->

<!-- Part 4 --------------------------------------------------------------------------- -->

<section data-markdown>
    <textarea data-template>
## Agenda

1. Basic Model Architecture
1. Advanced Model Architecture
1. Smart regularization
1. _What else is there?_
    </textarea>
</section>


<section data-markdown class="fragments">
    <textarea data-template>
### In a nutshell: Encoding World Knowledge

What objective do you phrase in your learning experiment?

- _type of learning_
  - supervised learning, unsupervised learning, reinforcement learning
- _input and output_
  - feature extraction / preprocessing: what is really relevant?
- _general architecture_
  - type of model, network capacity, parametric
- _reward and observation for RL_
  - shape learning objective
</textarea>
</section>

<section data-markdown class="fragments">
### Data

* real world projects typically don't operate on pre-collected data sets
* so you have to collect the data, but you can also choose what data to collect
* make sure the data you collect covers the real world or at least the domain you are interested in
</section>

<section data-markdown class="fragments">
### Be aware of the natural bias of the data

* e.g. concerning people
  * height
  * age
  * gender
  * skin tone / hair styles
  * body ratio
  * disabilities
    * wheel chair
    * crutches
* take care of negative classes
  * dogs
  * background
  * noise from sensors    

</section>

<section data-markdown class="fragments">
### General preprocessing

encode your assumptions, e.g.
* only general form and structure seen in an image are important
  * reduce resolution, apply closing
  * leads to faster training and better generalization
* a certain feature is highly correlated with the target value
  * but you know this is a bias in your data set
  * remove that feature from the input
  * live with the *worse* classifier

</section>

<section data-markdown class="fragments">
### Augmentation

* idea: generate more data from existing
* using invariants of the real world
  * e.g. a person is still a person no matter how far away
* concerns
  - might explode with dimensions
  - augmented data might outweigh "real" data
  - don't fool yourself: are you just using standard augmentation or real domain knowledge? 
* alternative: generate purely synthetic data
  * like GANs 
  * reality gap ‚Äî the small differences between real and synthetic data that models may fixate on incorrectly, harming generalization (https://twitter.com/russelljkaplan/status/1490303023267999744)

</section>

<section data-markdown class="fragments">
### Foundation Models

_foundational models: trained on broad data at scale and are adaptable to a wide range of downstream tasks_

* promise to store the knowledge about the world in text form
* have a high capacity for learning
* can be trained on all data available in text form
* one day a general abstraction for everything?

https://arxiv.org/abs/2108.07258
</section>

<section data-markdown class="fragments">
### What else can you do?

* Markov-chains
* pre- and post-processing
* parametric models
  * strong assumptions about the model (e.g. normally distributed)
* causality models
  * often work by generating a set of models and seeing which performs best
</section>

<section data-markdown>
    <textarea data-template>

        ## Recap ML: The High-Level View

        <img src="img/mikio/ml-wk-overview.png">
    </textarea>
</section>

<section data-markdown class="fragments">
### Summary

* a prior, ML models do not even have the most basic world knowledge
* typical data sets also do not contain that information
* a human user of the model will, however, assume such knowledge
* unfortunately, encoding world knowledge as priors into ML models is not straight forward
* neural networks have the most opportunities for such an encoding 
* you need deeper understanding of the way models work, as well as a bunch of poorly structured approaches in your tool belt üõ†
</section>

<section data-markdown>
    <textarea data-template>
## Vielen Dank

### Wie kann man Weltwissen ins Machine Learning einbringen?

M3 2022, https://www.m3-konferenz.de/veranstaltung-14113-0-wie-kann-man-weltwissen-ins-machine-learning-einbringen.html

Bleibt gern im Kontakt

Oliver Zeigermann

Mikio Braun 

https://www.linkedin.com/in/oliver-zeigermann-34989773/  https://twitter.com/DJCordhose oliver.zeigermann@openknowledge.de

https://www.linkedin.com/in/mikiobraun / [@mikiobraun](https://twitter.com/mikiobraun)

### Diese Folien: https://bit.ly/m3-2022-world-knowledge

</textarea>
</section>

<!-- 
<section data-markdown>
### Themen

Grundlagen
* Encoding World Knowledge just applied statistics and mathematics?
* Einfache Priors
* Parametrische Modelle
* Regularisierung
* Augmentation

Fortgeschrittenes
* Markov-Ketten
* Kausalit√§t
* Foundation Models
</section>

<section data-markdown class="todo">

- Hidden Markov Models
  * https://en.wikipedia.org/wiki/Markov_chain
- Kalman Filter
  * https://en.wikipedia.org/wiki/Kalman_filter

Weltwissen haupts√§chlich Frage der Architektur
- Traditionelle Systeme instrumentieren viele kleine Modelle als Pattern matcher viel besser als
- Gro√ües System als Blackbox
</section>

<section data-markdown>
## Einfache Priors    
</section>

<section data-markdown class="todo">
### Symbole und Suche

https://nautil.us/deep-learning-is-hitting-a-wall-14467/
</section>

<section data-markdown class="todo">
### Bayesian Models, Weltwissen ist Prior
</section>
        
<section data-markdown class="todo">

World Knowledge as 
* pre-processing, feature engineering
* post processing
</section>

<section data-markdown class="todo">
### Training Objective is part of world Knowledge

Well explained blog post about over-optimizing reward models using simple best-of-n sampling: https://t.co/O3ocQ49etY

By Jacob Hilton and @nabla_theta
(https://twitter.com/janleike/status/1514304430824300546?t=7PmXFiyCHY9615YgAdP3VQ&s=03) 
</section>

<section data-markdown>
    <textarea data-template>
### Case: Order

- https://raschka-research-group.github.io/coral-pytorch/
- Ok, aber die Tatsache dass man weiss dass da eine Ordnung ist ist vielleicht eine Art Prior. Aber das ist sehr weitgegriffen vielleicht
</textarea>
</section>

<section data-markdown class="todo">
Learning Setting: Unsupervised, we know what is normal. Normal derives from being the majority of cases. But we assume normality.
</section>

<section data-markdown class="todo">
    <textarea data-template>
### RL

Indirektes Domainwissen in RL √ºber Observation und Reward und Action Modellierung
</textarea>
</section>
    
<section data-markdown>
## Parametrische Modelle    
</section>

<section data-markdown class="todo">
### Parametric Solutions    

We assume certain properties of the model, like being normally distributed, and just learn the parameters of such properties, like mean and std.

</section>



<section data-markdown class="todo">
### World Knowledge in Parametric Models
    
Christoph Molnar (@ChristophMolnar) twitterte um 0:40 PM on So., M√§rz 27, 2022:
The modeling mindset of statisticians in one tweet

1) Measure random variables, e.g. water temperature
2) Assume distribution, e.g. Normal distribution
3) Goal: Find optimal distribution parameters, e.g. mean
4) Solution: Maximize the likelihood

Parameters = Insight about world https://t.co/vfpjA9sCfT
(https://twitter.com/ChristophMolnar/status/1508031278590873607?t=R2uQnLifwuc8MOPKGtyLPQ&s=03) 
</textarea>
</section>


<section data-markdown class="todo">

Miles Cranmer (@MilesCranmer) twitterte um 11:54 PM on Sa., Apr. 16, 2022:
In a neural network, is there a type of  regularization which encourages one learned feature to be independent, **including nonlinearly,** of other features in the same layer?

I can‚Äôt use a bottleneck or sparsity constraint‚ÄîI actually want to maximize the dimensionality!
(https://twitter.com/MilesCranmer/status/1515448502377234436?t=fFix03REi1KsQ64zcz8kVQ&s=03) 
</section>

    


<section data-markdown>
## Augmentation    
</section>
    
<section data-markdown class="todo">
https://lilianweng.github.io/posts/2022-04-15-data-gen/

</section>


<section data-markdown>
## Kausalit√§t    
</section>

<section data-markdown class="todo">
### causality    

Dan Roberts (@danintheory) twitterte um 4:41 PM on So., Feb. 20, 2022:
I respectfully disagree w/ this and @ylecun's perspectives on causality: (a) causality is an essential part of microscopic physics and (b) even at the effective level of Newtonian physics, F=ma is *not* symmetric since it's a differential equation, not an algebraic equality.

1/
(https://twitter.com/danintheory/status/1495423336519770112?t=hbKV7pkdpDDK6oCgdNKktg&s=03) 
</section>


<section data-markdown class="todo">

* Die meisten Ans√§tze erzeugen Kandidaten und bewerten anhand von der Realit√§t mit Scores oder Unabh√§ngigkeitstests

Nan Rosemary Ke (@rosemary_ke) twitterte um 5:53 AM on Mi., Apr. 20, 2022:
Supervised causal induction: In this work, we learn to induce causal structure by
treating the inference process as a black box and design a neural network architecture
that learns the mapping from data to graph
structures via supervised training.https://t.co/fHVQlkJLxg https://t.co/dsBvaU2kbe
(https://twitter.com/rosemary_ke/status/1516626111182028803?t=ZG3F-ReQmn7c1fbODdM8dg&s=03)
</section>


<section data-markdown>
## Markov-Ketten
</section>

<section data-markdown class="todo">
### Markov Chains as Matrices

Tivadar Danka üá∫üá¶ (@TivadarDanka) twitterte um 10:30 AM on Fr., M√§rz 11, 2022:
The single most undervalued fact of linear algebra: matrices are graphs, and graphs are matrices.

Encoding matrices as graphs is a cheat code, making complex behavior simple to study.

Let me show you how! https://t.co/8rBIkA8ZbZ
(https://twitter.com/TivadarDanka/status/1502215264544296962?t=gMcg-3niXfPHyXfw4wx8_g&s=03) 

</section>
 -->
        </div>
    </div>


    <script src="reveal.js/dist/reveal.js"></script>
    <script src="lib/jquery.js"></script>

    <script>
        const printMode = window.location.search.match(/print-pdf/gi);
        const isLocal = window.location.hostname.indexOf('localhost') !== -1 ||
            window.location.hostname.indexOf('127.0.0.1') !== -1;
        const isPresentation = isLocal && !printMode;
        const isPublic = !isPresentation;

        $('.hide').remove();

        if (isPresentation) {
        } else {
            // only applies to public version
            $('.todo').remove();
            $('.preparation').remove();
            $('.local').remove();
        }

        Reveal.addEventListener('ready', function (event) {
            // applies to all versions
            $('code').addClass('line-numbers');

            $('.fragments li').addClass('fragment')

            // make all links open in new tab
            $('a').attr('target', '_blank')

            if (isPresentation) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $(':not(code).fragment').removeClass('fragment');
            }

            // we do not like fragments
            // $('.fragment').removeClass('fragment');

        });

    </script>

    <script src="reveal.js/plugin/notes/notes.js"></script>
    <script src="reveal.js/plugin/markdown/markdown.js"></script>
    <script src="reveal.js/plugin/highlight/highlight.js"></script>
    <script>
        // More info about initialization & config:
        // - https://revealjs.com/initialization/
        // - https://revealjs.com/config/
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            history: true,
            center: true,
            width: 1100,
            slideNumber: true,
            hideInactiveCursor: false,


            // Learn about plugins: https://revealjs.com/plugins/
            plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
        });
    </script>


</body>

</html>