<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Introduction to Machine Learning for Software Developers</title>

    <link rel="stylesheet" href="reveal.js/dist/reset.css">
    <link rel="stylesheet" href="reveal.js/dist/reveal.css">
    <!-- <link rel="stylesheet" href="reveal.js/dist/theme/black.css"> -->
    <link rel="stylesheet" href="reveal.js/dist/theme/white.css">

    <!-- Theme used for syntax highlighted code -->
    <!-- <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css"> -->
    <link rel="stylesheet" href="reveal.js/plugin/highlight/zenburn.css">
    <style>
        .right-img {
            margin-left: 10px !important;
            float: right;
            height: 500px;
        }

        .todo:before {
            content: 'TODO: ';
        }

        .todo {
            color: red !important;
        }

        code span.line-number {
            color: lightcoral;
        }

        .reveal pre code {
            max-height: 1000px !important;
        }

        img {
            border: 0 !important;
            box-shadow: 0 0 0 0 !important;
            height: 450px;
        }

        .reveal {
            -ms-touch-action: auto !important;
            touch-action: auto !important;
        }

        .reveal h1,
        .reveal h2,
        .reveal h3,
        .reveal h4 {
            /* letter-spacing: 2px; */
            font-family: 'Calibri', sans-serif;
            /* font-family: 'Times New Roman', Times, serif; */
            /* font-weight: bold; */
            color: black;
            /* font-style: italic; */
            /* letter-spacing: -2px; */
            text-transform: none !important;
        }

        .reveal em {
            font-weight: bold;
        }

        .reveal section img {
            background: none;
        }

        .reveal img.with-border {
            border: 1px solid #586e75 !important;
            box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
        }

        .reveal li {
            margin-bottom: 8px;
        }

        /* For li's that use FontAwesome icons as bullet-point */
        .reveal ul.fa-ul li {
            list-style-type: none;
        }

        .reveal {
            /* font-family: 'Work Sans', 'Calibri'; */
            font-family: 'Calibri';
            color: black !important;
            font-size: xx-large;
        }

        .container {
            display: flex;
        }

        .col,
        col-1 {
            flex: 1;
        }

        .col-2 {
            flex: 2;
        }
    </style>

</head>

<body style="background-color: whitesmoke;">

    <div class="reveal">
        <div class="slides">

<!-- 

Introduction to Machine Learning for Software Developers
Montag, 16.05.2022, 09:00 - 17:00 Uhr

Machine Learning is a special form of software development starting with data instead of an explicit algorithm to solve
a problem. Sometimes this approach is far superior to traditional software development, sometimes it is not. So it makes
sense to get at least a basic idea of how it works and when it might be applicable for all software developers.

We will cover the following topics
- understanding the approach of data driven development
- using Notebooks and understanding the style of development that comes along with it
- from hand written rules to machine learning
- when does machine learning make sense
- understanding different types of algorithms for machine learning
- different types of machine learning: supervised, unsupervised reinforcement, reinforcement learning
- how to bring a machine learning model in production and interface with traditional code

All topics are backed by code in Python that you will either change or extend in hands-on exercises. As the nature of
our programming will be scripting rather than programming, experience in any object oriented programming language is the
only prerequisite. No installation except for a recent version of Chrome is required as we will use the cloud based
Colab service to develop and execute our code. For this you will need a Google account, but it is also possible to
create one as part of the workshop.

This workshop will be held in English, unless all registered participants speak German. -->


<section data-markdown>
                <textarea data-template>
# Introduction to Machine Learning for Software Developers

Oliver Zeigermann

    </textarea>
            </section>

			<section data-markdown>
				<textarea data-template>
### Who is Olli

<div style="display: flex;">
    <div style="flex: 50%;">
        <a href='https://oreilly.de/produkt/machine-learning-kurz-gut-2/'>
        <img src='img/ml-buch-v2.jpg' height="400">
        </a>
    </div>
    <div style="flex: 50%; font-size: x-large;">
        <img src='img/olli-opa.jpeg'>
    </div>
</div>
<p>
    <a target="_blank" href="mailto:oliver.zeigermann@openknowledge.de">Oliver Zeigermann</a>:
Heat of AI@OpenKnowledge
</p>    
</textarea>
			</section>

<section data-markdown>
    <textarea data-template>
### Olli        

<img src='img/olli-T.jpg'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### AI overview

<img src='img/AI.png'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src='img/everybody-gansta.png' style="height: 600px">

<small>

https://twitter.com/karpathy/status/1486215976559398915
</small>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Machine Learning Projects can be structured in phases

<img src='img/sketch/phases-ml.png'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### What steps would you take when you want to start a new machine learning project

1. Enable your organization 
1. Identify a reasonable candidate
1. Enter stage 1 of technical experiments

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### How you whish ML projects work

<img src='img/sketch/objective-joke.png'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### How ML projects really work

<img src='img/sketch/objective-truth.png'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### And most of the time they do not work at all

<img src='img/ml-success.png'>

https://twitter.com/burkov/status/1505011374279307266
</textarea>
</section>


<section data-markdown>
    <textarea data-template>
# Schedule

1. Basic conceptual knowledge, training a machine
1. Traditional Machine Learning vs Deep Learning
1. Foundation Models
1. Outlier detection
1. Reinforcement Learning
1. Production
1. Kicking off your machine learning solution

    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Part I        
## Basic conceptual knowledge, training a machine

<img src='img/booster/abfahrt.jpg'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Challenge: Is an object a person or a thing?

* How would you solve this?
* What do you need?
* How much effort would it take to make this work?

</textarea>
</section>
            <section data-markdown>
                <textarea data-template>
<img src='img/booster/classic-development.jpg' style="height: 600px;">
</textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
<img src='img/booster/supervised-ml.jpg' style="height: 600px;">
</textarea>
            </section>

<section data-markdown>
    <textarea data-template>
### Formulating an optimization problem instead of writing code

<img src='img/booster/software-complexity.png'>

<small>
Andrej Karpathy - TRAIN AI 2018 - Building the Software 2.0 Stack

https://vimeo.com/272696002

</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Hands-On: Solution using Machine Learning
### Teach, don't program!

Assignment: Pair up in teams of 2-3 people and teach the machine to tell which one is which. After training, try to trick the model to make (more) mistakes

<a href='https://teachablemachine.withgoogle.com/'>
    <img src='img/teachable-machine.png' style="height: 300px;">
</a>

<small>    

https://teachablemachine.withgoogle.com/
</small>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## What's inside?    

<img src='img/booster/container-ship-cut-in-half.jpg'>

https://twitter.com/CutInHalfPic/status/1498601900081483779 
</textarea>
</section>
    
            <!-- <section data-markdown>
                <textarea data-template>
## Types of Machine Learning                    
<img src='img/sketch/ml-taxonomy-small.jpg'>
    </textarea>
            </section> -->

            <section data-markdown>
                <textarea data-template>
## Data for ML Type - Supervised Learning        
<img src='img/data_for_ml_types/supervised.jpg'>
    </textarea>
            </section>

<section data-markdown>
    <textarea data-template>
### Overview: Supervised Learning Process Flow

<img src='img/flow-train.jpg'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Prediction

<img src='img/flow-prediction.jpg' height="500">

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Machine Learning only works if you can go from known to unknown data
    
<img src='img/data-and-the-world.jpg' height="550px">

    </textarea>
    </section>



<section data-markdown class="fragments">
    <textarea data-template>
### When to apply Supervised Learning

_classify categories or predict values_

* you have matching pairs of input and output and want the model to generate output for unknown input
* the solution to the problem at hand is unknown or hard to specify
* solving the problem can tolerate some error or uncertainty
* there is a clear, simple input and output
* there are patterns in your input that can be used to predict the output

_what we see applied most of the time_
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Part II       
## Traditional Machine Learning vs Deep Learning

</textarea>
</section>

<section data-markdown class="fragments">

### Traditional Machine Learning vs Deep Learning

* **Traditional Machine Learning**
    * e.g. regressions, decision trees, nearest neighbors, support vector machines.
    * needs selection and preprocessing of features
    * can be used with few examples in some cases
    * usually needs little computing power for training
* **Deep Learning**
    * can learn which features are relevant and pre-process them by successively connected layers
    * is typically implemented with neural networks
    * Can have several billion parameters
    * usually needs GPUs at least for training
</section>

<section data-markdown>
    <textarea data-template>
## Playtime        
### Traditional Machine Learning

<a href='https://ml-playground.com'><img src='img/ml-playground.png' style="height: 400px"></a>

<small>

https://ml-playground.com
<br>
https://www.freecodecamp.org/news/a-no-code-intro-to-the-9-most-important-machine-learning-algorithms-today/

</small>
    </textarea>
</section>


<section data-markdown>
    <textarea data-template>
## Deep Learning
<img src='img/deep-learning-lecun.png' height="500">

<small>(c) Yann LeCun</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Playtime        
### Deep Learning

<a href='https://playground.tensorflow.org/'>
<img src='img/tf-playground.png' height="500px">
</a>

<small>

https://playground.tensorflow.org/
</small>
    </textarea>
    </section>


<section data-markdown>
    <textarea data-template>
### Part III       
## Foundation Models

</textarea>
</section>


<section data-markdown>
    <textarea data-template>
### Challenge: Is a text negative or positive?

* There are nasty things circulating on social media about your company
  * A negative tweet trending on twitter might impact your company
* A furious customer might need special or immediate attention
* How many stars would a textual review equate to?

</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Our examples

* I don't think its a good idea to have people driving 40 miles an hour through a light that *just* turned green, especially with the number of people running red lights, or the number of pedestrians running across at the last minute being obscured by large cars in the lanes next to you.
* MANY YEARS ago, When I was a teenager, I delivered pizza. I had a friend who, just for the fun of it, had a CB. While on a particular channel, he could key the mike with quick taps and make the light right out in front of the pizza place turn green. It was the only light that it worked on, and I was in the car with him numerous times to confirm that it worked. It was sweet.
* The "green" thing to do is not to do anything ever, don't even breath!  Oh, and if you are not going to take that ridiculous standpoint then I guess this is relevant to Green because it uses Bio-fuels in one of the most harsh environments in the world, showing that dependence on tradition fuels is a choice not a necessity.

https://www.reddit.com/r/transport/
</textarea>
</section>

<section data-markdown class="fragments">
### Issues in Supervised Learning

* linear effort in labelling data
* significant error rate to be expected
  * all standard data sets contain up to 10% of errors
  * https://labelerrors.com/
* differences between different labelers
* change in label definition might require to start all over

_impractical with large data sets_
</section>


<section data-markdown>
    <textarea data-template>
## Data for ML Type - Self-Supervised Learning        
<img src='img/data_for_ml_types/self_supervised.jpg'>
</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Transformers use a Self-Supervised Learning approach

<img src='img/transformers/deepl-example.png'>

<small>

https://www.deepl.com/translator#de/en/Transformer%20sind%20das%20Schweizer%20Messer%20der%20Sprachverarbeitung
</small>
        </textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Bring on the big ships
    
<!-- <img src='img/booster/ladebruecke.jpg'> -->
<img src='img/booster/hmm_algeciras.jpg'>

</textarea>
</section>

<section data-markdown class="fragments">
### Foundation Models: Transformer Core ideas

1. have a generalized language model
1. predict probabilities of sequences of words
1. train on a very large corpus
1. zero- or one-shot learning
1. self-attention for encoding long range dependencies
1. self-supervision for leveraging large unlabeled datasets (aka unsupervised pre-training)
1. additional supervised training for downstream tasks, e.g.
    - translation (lang1 & lang2 pairs)
    - question answering (Q&A pairs)
    - sentiment analysis (text & mood pairs) 
    - etc.
  
  <!-- https://www.youtube.com/watch?v=iFhYwEi03Ew -->
  </section>
 
<section data-markdown style="font-size: x-large;">
  <textarea data-template>
## Huggingface is the place to go

Models out of the box: https://colab.research.google.com/github/DJCordhose/ml-resources/blob/main/notebooks/foundation/transformers-pipelines.ipynb?hl=en

https://huggingface.co/transformers/
main framework is Pytorch, but supports TensforFlow and JAX as well: https://huggingface.co/transformers/#supported-frameworks
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### But now this

<a href='https://huggingface.co/models?sort=downloads'>
<img src='img/transformers/hugging-zoo.png' style="height: 500px;">      
</a>
<small>

https://huggingface.co/models?sort=downloads

</small>

</textarea>
</section>

<section data-markdown class="fragments">
### Transformer Zoo

* the original transformer was meant for translation tasks
* usage has broadened ever since
* spawning a whole zoo of transformers
* some use encoder only
* some use decoder only
* some use a combination of encoder/decoder just like the original transformer 

<!-- https://huggingface.co/transformers/model_summary.html
https://huggingface.co/transformers/#supported-models -->
</section>

<section data-markdown>
<textarea data-template>
### Transformer Architecture: Encoder/Decoder

<div style="display: flex; align-items: center; justify-content: space-around;">
  <div>
    <ul>
      <li>left side is encoder</li>
      <li>right side is decoder</li>
      <li>encoder feeds in embeddings of input into decoder</li>
      <li>decoder needs context to work</li>
    </ul>
</div>
  <div>
    <img src='img/transformers/transformer-encoder-decoder.png' >
</div>
</div>
<small>

https://arxiv.org/pdf/1706.03762.pdf
</small>

        </textarea>
</section>

<!-- <section data-markdown>
  <textarea data-template>
### Evolution of Transformers (original paper is from 2017)

<img src='https://huggingface.co/course/static/chapter1/model_parameters.png' style="height: 500px;">

<small>

https://huggingface.co/course/chapter1/4?fw=pt</small>
  </textarea>
</section> -->

<section data-markdown class="fragments">
### Encoder only (BERT-like)

_also called auto-encoding Transformer models_

* encodes words into vectors (latent representation)
* some problems only need the encoder part of the original transformer
* "understanding" texts and their semantics is sufficient to e.g.  
  * answer questions about a text with individual original text passages or to  
  * sort the mood of a text into "positive" or "negative", 
  * filling individual word gaps in texts
* in all three cases, no "complex" answer is required for which a decoder would be needed. 
* BERT and derived models are famous off-the-shelf representatives for encoder only models
  
  </section>

<section data-markdown class="fragments">
### Embeddings

* vectors representing words
* aka latent representation
* maybe turn a sentence into a vector
* contextual embeddings: turn a word into a vector, but also take the context into account
* visualizing embeddings: https://github.com/koaning/whatlies
</section>
  
<section data-markdown>
### What can BERT do?
    
* token classification 
* sentence classification
* multiple choice classification 
* question answering
</section>
  
  <!-- <section data-markdown>
  ### Example for encoder only BERT: Classifier
  
  Downstream task: Sentence Classification
  
  Email Classifier, urgent oder not urgend:
    * https://twitter.com/ClementDelangue/status/1409728768915214337
    * https://huggingface.co/clem/autonlp-test3-2101787?text=I+would+be+nice+if+this+is+done+by+next+year vs
    * https://huggingface.co/clem/autonlp-test3-2101787?text=I+would+be+nice+if+this+is+done+by+yesterday
  </section> -->
  
  <section data-markdown class="fragments">
### Training BERT

* first objective
  * input corrupted by using random masking
  * model must predict the original sentence
    * only masked words are predicted rather than reconstructing the entire input (because BERT can not do this)
* second objective: 
  * inputs are two sentences A and B
  * model has to predict if the sentences are consecutive or not

https://huggingface.co/transformers/model_summary.html#bert
https://arxiv.org/abs/1810.04805
  </section>
  
  <section data-markdown>
    <textarea data-template>
### Demo - Bert

Bert finetuned for sentiment analysis 

<img src='img/booster/t0pp.png' style="height: 350px;">      

https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment
</textarea>
</section>

  <section data-markdown class="fragments">
### Decoder only (GPT-like)

  _also called auto-regressive Transformer models_

* the decoder part can transform given inputs into complete sentences
* e.g. useful in itself, to complete started sentences
* generates a response iteratively ("auto regressive")
* GPT would be an example for this kind of application 
  * unidirectional: trained to predict next word
  * by OpenAI 
  
  </section>
  
<section data-markdown>
### Training GPT

* self-supervised training
* predict the next word, given all of the previous words within some text
* has a limited context

https://huggingface.co/transformers/model_summary.html#original-gpt
https://huggingface.co/transformers/model_doc/gpt2.html
</section>

<section data-markdown class="fragments">
### Evolution of GPT

GPT: Generative Pre-Trained Transformer

* GPT-1: 2018, 110 million parameters (https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), https://www.youtube.com/watch?v=LOCzBgSV4tQ
* GPT-2: 2019, 1.5 billion parameters (https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), https://www.youtube.com/watch?v=BXv1m9Asl7I
* GPT-3: 2020, 175 billion parameters (https://arxiv.org/abs/2005.14165), https://www.youtube.com/watch?v=wYdKn-X4MhY
* GPT-4: 2022, probably not much larger, but trained on more data and more context (4096 instead of 2048) (https://analyticsindiamag.com/gpt-4-sam-altman-confirms-the-rumours/)

</section>

<section data-markdown>
  <textarea data-template>
  ### Typical example for decoder only GPT: completing a text (zero-shot)
  
  <img src='img/transformers/gpt-3-article.png' style="height: 450px;">
  
  <small>
  
  https://arxiv.org/abs/2005.14165</small>
          </textarea>
  </section>
  
<section data-markdown>
<textarea data-template>
### Completion (One-Shot / Few-Shot)

<img src='img/transformers/gpt-few-shot.png' style="height: 450px;">

<small>

https://arxiv.org/abs/2005.14165</small>
        </textarea>
</section>
  
<section data-markdown>
<textarea data-template>
### Demo Github Copilot - Autocempletion on Steroids

<img src='img/transformers/copilot.png' style="height: 100%;">

<small>

* https://copilot.github.com/
* https://github.com/github/copilot-docs/tree/main/docs
* https://github.com/github/copilot-docs/blob/main/docs/visualstudiocode/gettingstarted.md#getting-started-with-github-copilot-in-visual-studio-code

</small>

</textarea>
</section>
  
<section data-markdown class="fragments">
### Complete transformer (BART/T5-like)

_also called sequence-to-sequence transformer model_

* combined use of encoder and decoder, as in the original transformer approach
* allows summaries of texts in addition to translations
* name is probably most appropriate as texts really get transformed
* models like T5 and BART are most common here
* to be able to operate on all NLP tasks, it transforms them into text-to-text problems by using specific prefixes: 
  * summarize: 
  * question: 
  * translate English to German: 
  * etc.

https://arxiv.org/abs/1910.10683
</section>

<section data-markdown>
### Training T5

pretraining includes both self-supervised and supervised learning
* self-supervised training randomly removes a fraction of the tokens and replacing them with individual sentinel tokens
  * input of the encoder is the corrupted sentence
  * input of the decoder is the original sentence 
  * target is then the dropped out tokens delimited by their sentinel tokens
* supervised training on downstream tasks provided by the GLUE and SuperGLUE benchmarks
  *  converting them into text-to-text tasks as explained in previous slide

https://huggingface.co/transformers/model_summary.html#t5
</section>

<section data-markdown>
  <textarea data-template>
### Demo - Bart 

Natural language inference (NLI): do "hypotheses" match a "premise"?

<img src='img/transformers/t0pp.png' style="height: 400px;">

https://huggingface.co/facebook/bart-large-mnli
</textarea>
</section>

<section data-markdown style="font-size: x-large;">
    <textarea data-template>
## Hands-On: Models also have a Python API

Understand the concept of a notebook and use both models from their Python API

https://colab.research.google.com/github/DJCordhose/ml-resources/blob/main/notebooks/foundation/transformers-sentiment.ipynb?hl=en

</textarea>
</section>
  
  
  
<section data-markdown class="fragments">
  <textarea data-template>
## If that is not enough

### Can I train my own transformer model?

* Do you have the compute to train a transformer model in the first place?
* Do you have a lot of (labelled) data specific to your domain of application?

</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### How expensive is it to train a Foundational Model

<img src='https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg'>

<small>

https://huggingface.co/course/chapter1/4?fw=pt
<br>
https://mlco2.github.io/impact/
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Remedy: Transfer Learning

<img src='https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg'>

<small>

https://huggingface.co/course/chapter1/4?fw=pt</small>
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Fine-tuning for Downstream Task

<img src='https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg'>

<small>

https://huggingface.co/course/chapter1/4?fw=pt
<br>
https://huggingface.co/docs/transformers/training#finetune-a-pretrained-model
</small>
  </textarea>
</section>

<section data-markdown style="font-size: x-large;">
    <textarea data-template>
### Code: Fine tuning sentiment

https://colab.research.google.com/github/DJCordhose/ml-resources/blob/main/notebooks/foundation/transformers-fine-tuning.ipynb?hl=en

</textarea>
</section>

<section data-markdown class="fragments">
### Don't forget: Transformers are language models

* No abstract reasoning like it is in our brains takes place
* The basis is the expression of thoughts in texts and code, etc.
* That's the way the system is trained
* Whether this is also intelligent is a pointed question
* Turing would probably say it doesn't matter
* One can argue that this system passes his test
  * https://twitter.com/glouppe/status/1438496208343949318
</section>

<section data-markdown class="fragments">
### On the Opportunities and Risks of Foundational Models

* foundational models: trained on broad data at scale and are adaptable to a wide range of downstream tasks
* ML is undergoing a paradigm shift with the rise of these models
* their scale results in new emergent capabilities 
* defects of the foundation model are inherited by all the adapted models downstream
* lack of clear understanding of how they work, when they fail, and what they are even capable of

https://arxiv.org/abs/2108.07258
</section>



<section data-markdown>
    <textarea data-template>
### German models

* Base: https://huggingface.co/dbmdz/bert-base-german-cased
* Q&A: https://huggingface.co/deutsche-telekom/electra-base-de-squad2
* Overview: https://huggingface.co/models?language=de&sort=downloads

</textarea>
</section>


<section data-markdown class="fragments">
### More examples for applications in the corporate context

* Summary of (scientific) articles
* Classification of incoming mail (email)
* Summarization: long on short texts (product description)
* Advanced automated Q&A service 
* What is your example?

</section>

<section data-markdown>
    <textarea data-template>
### Part IV       
## Outlier detection

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Challenge: Does something look off?

* Systems expose complex behavior
* Collected data on things might be the first step
* From that: how can you tell if data looks the way it should?

</textarea>
</section>
    
<section data-markdown>
    <textarea data-template>
## Data for ML Type - Unsupervised Learning        
<img src='img/data_for_ml_types/unsupervised.jpg'>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Solution using Machine Learning
### Learn an abstract concept as should state and compare to is state

<a href='https://victordibia.github.io/anomagram/#/'>
<img src='img/booster/anomagram-inference.gif' height="450">
</a>
<br>
<small>

https://github.com/victordibia/anomagram        
https://victordibia.github.io/anomagram/#/

</small>
</textarea>
</section>

<section data-markdown class="fragments">
### Wrap-Up

* Unsupervised Learning only needs plain data, i.e. no labels
* Can be clustering (what is similar and what is different)
* Can be used in advanced and unexpected ways
* Setup of ML experiment defines the problem and brings in world knowledge
* Autoencoders are a well understood and versatile tool for deep unsupervised learning
* GANs are a new approach to unsupervised learning, useful when you want to generate something realistic  


</section>

<section data-markdown>
    <textarea data-template>
### Part V       
## Reinforcement Learning

</textarea>
</section>


<section data-markdown>
    <textarea data-template>
## Data for ML Type - Reinforcement Learning        
<img src='img/data_for_ml_types/reinforcement.jpg'>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Learning to park by making experiments

<br>
<br>
<div class="container" style="align-items: center;">
    <div class="col">
<img src='img/rl2/rl-park.png' style="height: 300px;">
</div>
<div class="col">

* Setting: https://youtu.be/VMp6pq6_QjI
* 20k trials, first success: https://youtu.be/VMp6pq6_QjI?t=140
* 300k trials, starts looking good: https://youtu.be/VMp6pq6_QjI?t=572

</div>
</div>
</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Real challenge is to model the problem for RL

<div class="container">
<div class="col">
    <img src='img/rl2/rl.png' style="height: 300px;">

</div>    
<div class="col">
    <br>
<ol>
    <li>Based on <em>Observations</em> an <em>Agent</em> executes </li>
<li> <em>Actions</em> within a given  
<li><em>Environment</em> which lead to positive or negative 
<li><em>Rewards</em>.
</ol>
</div>    
</div>
The Agent’s job is to maximize the cumulative Reward

http://gym.openai.com/docs/

</textarea>
</section>

<section data-markdown class="fragments">
### Modelling of parking problem

* input eight depth sensors, current speed and position, relative position to the target
* outputs engine force, braking force and turning force 
* Policy as NN with 3 hidden layers of 128 neurons each
* learning algorithm PPO
* small reward for getting closer, large reward for hitting the spot
* small penalty for driven away, large penalty for crashing

</section>


<section data-markdown>
    <textarea data-template>
### More fun parking

* two AI Agents fighting for the same parking spot: https://twitter.com/SamuelArzt/status/1175738904055562240
* AI Learns to parallel park: https://twitter.com/SamuelArzt/status/1248642920875528194
* bits of code: https://twitter.com/SamuelArzt/status/1176487746195603457

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Structure of Observation and Reward are crucial

agents can learn short cuts and unexpected behavior
</textarea>
</section>

<section data-markdown style="font-size: x-large;" class="fragments">
    <textarea data-template>
### What machines learned as opposed what the designers intended them to learn

* `Reward shaping a soccer robot for touching the ball caused it to learn to get to the ball and vibrate touching it as fast as possible`
* `Robot hand pretending to grasp an object by moving between the camera and the object`
* `Simulated pancake making robot learned to throw the pancake as high in the air as possible in order to maximize time away from the ground`
* `Agent pauses the game indefinitely to avoid losing`
* `In an artificial life simulation where survival required energy but giving birth had no energy cost, one species evolved a sedentary lifestyle that consisted mostly of mating in order to produce new children which could be eaten (or used as mates to produce more edible children).`
* `Agent kills itself at the end of level 1 to avoid losing in level 2`
</textarea>
</section>

<section data-markdown style="font-size: x-large;" class="fragments">
    <textarea data-template>
### What machines learned as opposed what the designers intended them to learn - cont'd

* `Evolved player makes invalid moves far away in the board, causing opponent players to run out of memory and crash`
* `Creatures exploited physics simulation bugs by twitching, which accumulated simulator errors and allowed them to travel at unrealistic speeds`
* `Reward-shaping a bicycle agent for not falling over & making progress towards a goal point (but not punishing for moving away) leads it to learn to circle around the goal in a physically stable loop.`
* `... algorithm learns to bait an opponent into following it off a cliff, which gives it enough points for an extra life, which it does forever in an infinite loop.`
* `The PPO algorithm discovers that it can slip through the walls of a level to move right and attain a higher score.`

<small>

https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/
<br>
https://arxiv.org/abs/1803.03453 
<br>
https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml
https://twitter.com/mogwai_poet/status/1060286856493813760

</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Part VI        
## Production

A demo of https://github.com/DJCordhose/insurance-ml

</textarea>
</section>


<section data-markdown>
    <textarea data-template>
### Part VII        
## What to bring back from the trip

<!-- <img src='img/booster/koehlbrand.jpg'> -->
<img src='img/booster/ladebruecke.jpg'>


</textarea>
</section>

<!-- <section data-markdown>
    <textarea data-template>
<img src='img/sketch/ml-taxonomy.jpg' style="height: 650px;">

</textarea>
</section> -->

<!-- <section data-markdown>
### Applications

All the hype about machine learning for big applications like self-driving cars, playing Go, passing  programming tests.

I'm more exited by the "small" ML successes that solve concrete problems. Hidden in organizations, no PR. Just a data scientist (team) doing the real work.
(https://twitter.com/ChristophMolnar/status/1492039361227014168?t=93VKM1xCrnGsX57_AVgKiA&s=03) 
</section>
         -->
<!-- <section data-markdown>
    <textarea data-template>
### Joint exercise: Which learning strategy?    

<div style="display: flex;">
<div style="flex: 60%;">
<img src='img/sketch/ml-taxonomy-small.jpg' style="height: 100%;">
</div>
<div style="flex: 30%; font-size: x-large; margin-left: 20px;">

1. detecting tuberculosis on x-ray images 
1. optimal pricing of airline tickets
1. deciding on the risk class of potential customers for an insurance policy
1. detection of credit card fraud 
1. which soccer players are similar?
1. recommendation for news
1. when does a machine need maintenance?
1. encode legal rules into software
1. *your application or idea*   
</div>
</div>
</textarea>
</section> -->

<section data-markdown class="fragments">
    <textarea data-template>
### Understand the business context

* Why do you need it? 
* Do you have a current solution (e.g. what is the baseline)? 
* Who is going to use the predictions and how? 
* What is the impact of the model's downtime or mistakes?
* Which metrics do we care about? How do we know if we are doing well?

https://twitter.com/Fra_Pochetti/status/1502708228173578240
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### User wants to have control when

* the task is fun
* they feel personally responsible for the outcome
* the stakes are high (health, safety, finances, emotions)
* personal preferences (creative vision) are difficult to formulate
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### User wants to give up control when

* a task overwhelms them (workload, time, execution)
* a task is dangerous or unpleasant
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Calibrating Trust

<img src='img/booster/confidence-impact.png' style="height: 500px;">

<small>

https://pair.withgoogle.com/guidebook/
https://pair.withgoogle.com/worksheet/explainability-trust.pdf

</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Business Context is driving your solution

* Make up a project involving machine learning and describe it in 1-2 sentences
  * can be derived from something you have in your company or could have
  * can also be inspired by a project you have done in the past or
  * by the examples we had before
* Go through the items of the business context and fill in the cells    
* Add items when something important is missing
        
https://djcordhose.github.io/ml-resources/exercise/ml-business-context.docx
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Reflection        
### What are the most important parts
        
</textarea>
</section>


<section data-markdown class="fragments">
    <textarea data-template>
### Organizational and Conceptual Challenges

* There simply is not data and none can be generated.
* Data is available, but you can't get access, e.g. to sensitive information.
* Data is available, but does not match reality, either from the beginning or drifting.
* Process for machine learning clashes with established processes.
* Corporate culture does not allow for failures which are inevitable in the first phase of an ML project.
  * Effort is put into an approach, although it has not been it has not been confirmed by experiment.
* Expectations of ML are unrealistic. Really useful solutions are often unspectacular.

</textarea>
</section>

<!-- <section data-markdown>
    <textarea data-template>
### Roles

<img src='img/raci.png'>

</textarea>
</section> -->

<section data-markdown>
    <textarea data-template>
### What challenges could you face within your organization or project?

* Consider the organization you work in or the organizations you worked for before
* What would be a likely challenge?
* Is there one missing in the columns given?
        
https://djcordhose.github.io/ml-resources/exercise/ml-business-context.docx
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Reflection        
### What are the most important parts
        
</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Final words    

<img src='img/dl-expert.png'>

https://twitter.com/fchollet/status/1386373123889528835
</textarea>

</section>

<section data-markdown>
    <textarea data-template>
### Minimum Requirements to start with Machine Learning        

- you don't need any maths skills beyond what you learned at school
- there is no minimum requirement
- growth mindset is important
- experience is king
- idea of scientific work can help
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Make of Buy

* buy: you can't influence much, but you also don't have to
* make: you can influence a lot, but you also have to

https://medium.com/swlh/build-vs-buy-decision-619414efafc4
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Buy

* AWS: https://aws.amazon.com/machine-learning/ai-services/
* Google Cloud: https://cloud.google.com/solutions/ai
* Azure
  * https://azure.microsoft.com/en-us/overview/ai-platform
  * https://azure.microsoft.com/en-us/product-categories/applied-ai-services
* IBM: https://www.ibm.com/cloud/ai
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Make

* Classic Machine Learning: https://scikit-learn.org
* High Level Neural Networks / Deep Learning
  * https://www.tensorflow.org
  * https://keras.io/
* Low Level Neural networks
  * https://pytorch.org/
  * https://github.com/google/jax
* Base Models
  * Language: https://huggingface.co/transformers/
  * Recommendation: https://github.com/tensorflow/recommenders
* Assorted
  * https://www.tensorflow.org/hub
  * https://keras.io/examples/
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Thanks for attending        
# Essentials of Machine Learning / Artificial Intelligence

### Stay in Contact if you like

Oliver Zeigermann, <a href='https://twitter.com/DJCordhose'>@DJCordhose</a>

</textarea>
</section>


<!-- 
            <section data-markdown>
                <textarea data-template>
# MATERIAL                    
                </textarea>
            </section>
            <section data-markdown>
                <textarea data-template>
### Was macht das denn hier?

<img src='img/booster/insel.jpg'>

    </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
### Wie oft neu trainieren? Einmal im Jahr starten

<img src='img/booster/bagger1.jpg'>
<img src='img/booster/bagger2.jpg'>

        </textarea>
    </section>
    <section data-markdown>
                <textarea data-template>
### 2020 waren das die größten Schiffe der Welt

HMM Algeciras und GPT-3

<img src='img/booster/hmm_algeciras.jpg' style="height: 450px;">

<small>

https://de.wikipedia.org/wiki/HMM_Megamax-Klasse
<br>
https://en.m.wikipedia.org/wiki/List_of_largest_container_ships
</small>
    </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
### 2020 ist vorbei, jetzt ist 2021

<img src='img/booster/ever_ace_hidden.jpg'>

https://www.ndr.de/nachrichten/hamburg/Groesstes-Containerschiff-der-Welt-im-Hamburger-Hafen,containerschiff476.html

</textarea>
            </section>

<section data-markdown class="todo">
### Applications

All the hype about machine learning for big applications like self-driving cars, playing Go, passing  programming tests.

I'm more exited by the "small" ML successes that solve concrete problems. Hidden in organizations, no PR. Just a data scientist (team) doing the real work.
(https://twitter.com/ChristophMolnar/status/1492039361227014168?t=93VKM1xCrnGsX57_AVgKiA&s=03) 
</section>

<section data-markdown class="todo">
    ### High Level Strategy ist wie ML Prozess
    
    John Cutler (@johncutlefish) twitterte um 5:12 PM on Sa., Feb. 19, 2022:
    Most teams jump from high level strategy/goals straight to feature ideas (w/ "success metrics")
    
    The most successful teams
    1. Have a strategy
    2. Translate that into models
    3. Add minimally viable measurement
    4. Identify leverage points
    5. Explore options
    6. Run experiments https://t.co/EW8flIoA2M
    (https://twitter.com/johncutlefish/status/1495068676508176385?t=U5bOFyDJOwl4eZdRhfmuLQ&s=03) 
    </section>
    
    <section data-markdown class="todo">
### Scrum sucks for ML
    
    John Cutler (@johncutlefish) twitterte um 4:51 AM on So., Feb. 20, 2022:
    I've been hard on Scrum over the years.
    
    If you view it in the context of environments with
    
    * low-trust/transparency
    * unpredictability
    * scattered focus
    * overwhelmed teams
    * burnout, low sustainability
    * stakeholders interrupting
    * functional silos
    
    ...it makes sense. 1/2 https://t.co/agYjDusamo
    (https://twitter.com/johncutlefish/status/1495244604253237251?t=nHoSer1nNyxK-XQy06enzA&s=03) 
</section>

<section data-markdown class="todo">
    <textarea data-template>

### Roles
- Users
- User Experience
- Engineers
- What should ML model communicate?
- What a complete system
- Stakeholders
- How do communicate performance of systems to stakeholders

</textarea>
</section>


<section data-markdown class="todo">
    <textarea data-template>
### Interpretabilty / Explainability

* https://queue.acm.org/detail.cfm?id=3511299
* Everyone racking their brains about explainable AI should have a look at the cognitive science literature on how people explain their own behavior. People have very detailed explanations. The only problem is that these explanations can be very wrong!
  * https://twitter.com/gershbrain/status/1490696366405914626
  * The social aspects can play a large role as well, ie. who the explanation is for. https://t.co/slfEgGtcFi (https://twitter.com/mptouzel/status/1490721279825088513?t=t6Zl_LUMb8Wo1kNJDLxjRg&s=03) 
</textarea>
</section>

<section data-markdown class="todo">
    <textarea data-template>
### Ux and ML

[ ] Often humans come in direct contact with Machine Learning
[ ] Requires special attention as it enters the domain of human intelligence
[ ] This is not always the case, often other systems are in between
[ ] https://developers.google.com/machine-learning/guides/rules-of-ml
[ ] http://martin.zinkevich.org/rules_of_ml/
[ ] For end users
[ ] Does a door open?
[ ] Does the laptop log you in?
[ ] Need to be sutble: all these changes and what has become possible in the lifetime of a person
[ ] For professional users
[ ] Recommendation
[ ] How certain is the model?
[ ] How good does it have to be?
[ ] Shall not get on your nerves
[ ] Only make a proposal when you wait for a while
[ ] The longer you wait the lower the certainty has to be
</textarea>
</section>

 -->

        </div>
    </div>


	<script src="reveal.js/dist/reveal.js"></script>
    <script src="lib/jquery.js"></script>

	<script>
		const printMode = window.location.search.match(/print-pdf/gi);
		const isLocal = window.location.hostname.indexOf('localhost') !== -1 ||
			window.location.hostname.indexOf('127.0.0.1') !== -1;
		const isPresentation = isLocal && !printMode;
		const isPublic = !isPresentation;

		$('.hide').remove();

		if (isPresentation) {
		} else {
			// only applies to public version
			$('.todo').remove();
			$('.preparation').remove();
//			$('.local').remove();
		}

		Reveal.addEventListener('ready', function (event) {
			// applies to all versions
			$('code').addClass('line-numbers');

			$('.fragments li').addClass('fragment')

			// make all links open in new tab
			$('a').attr('target', '_blank')

			if (isPresentation) {
				// only applies to presentation version
				Reveal.configure({ controls: false });
			} else {
				// only applies to public version
				$('.fragment').removeClass('fragment');
			}

			// we do not like fragments
			// $('.fragment').removeClass('fragment');

		});

	</script>

	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			controls: true,
			progress: true,
			history: true,
			center: true,
			width: 1100,
			slideNumber: true,
			hideInactiveCursor: false,


			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
		});
	</script>


</body>

</html>