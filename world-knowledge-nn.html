<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>World Knowledge Priors</title>

    <link rel="stylesheet" href="reveal.js/dist/reset.css">
    <link rel="stylesheet" href="reveal.js/dist/reveal.css">
    <!-- <link rel="stylesheet" href="reveal.js/dist/theme/black.css"> -->
    <link rel="stylesheet" href="reveal.js/dist/theme/white.css">

    <!-- Theme used for syntax highlighted code -->
    <!-- <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css"> -->
    <link rel="stylesheet" href="reveal.js/plugin/highlight/zenburn.css">
    <style>
        .right-img {
            margin-left: 10px !important;
            float: right;
            height: 500px;
        }

        .todo:before {
            content: 'TODO: ';
        }

        .todo {
            color: red !important;
        }

        code span.line-number {
            color: lightcoral;
        }

        .reveal pre code {
            max-height: 1000px !important;
        }

        img {
            border: 0 !important;
            box-shadow: 0 0 0 0 !important;
            height: 450px;
        }

        .reveal {
            -ms-touch-action: auto !important;
            touch-action: auto !important;
        }

        .reveal h1,
        .reveal h2,
        .reveal h3,
        .reveal h4 {
            /* letter-spacing: 2px; */
            font-family: 'Calibri', sans-serif;
            /* font-family: 'Times New Roman', Times, serif; */
            /* font-weight: bold; */
            color: black;
            /* font-style: italic; */
            /* letter-spacing: -2px; */
            text-transform: none !important;
        }

        .reveal em {
            font-weight: bold;
        }

        .reveal section img {
            background: none;
        }

        .reveal img.with-border {
            border: 1px solid #586e75 !important;
            box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
        }

        .reveal li {
            margin-bottom: 8px;
        }

        /* For li's that use FontAwesome icons as bullet-point */
        .reveal ul.fa-ul li {
            list-style-type: none;
        }

        .reveal {
            /* font-family: 'Work Sans', 'Calibri'; */
            font-family: 'Calibri';
            color: black !important;
            font-size: xx-large;
        }

        .container {
            display: flex;
        }

        .col,
        col-1 {
            flex: 1;
        }

        .col-2 {
            flex: 2;
        }
    </style>

</head>

<body style="background-color: whitesmoke;">

    <div class="reveal">
        <div class="slides">

            <!-- 
Welt- und Domänenwissen für neuronalen Netze
 
Neuronale Netze können jede Funktion approximieren, sie haben jedoch nicht die geringste Ahnung von allgemeinem Wissen über die Welt. Dieser Mangel lässt sie oft kläglich scheitern, insbesondere bei der Extrapolation in Bereiche, die nicht durch Trainingsdaten abgedeckt sind.
 
Wir Menschen verfügen über dieses Welt- und Domänenwissen, das Deep-Learning-Modelle viel robuster werden lassen und sogar
Extrapolation erlauben könnte. Zum Beispiel lösen sich Objekte bei der Bilderkennung meistens nicht einfach in Luft auf und es gibt die Tendenz, dass Menschen mit zunehmendem Alter erst schneller, aber dann langsamer werden und irgendwann auch sterben. Nur, wie kodieren wir dieses Wissen?
 
Dieser Vortrag ist ein Überblick über bekannte Methoden, einschließlich der Wahl des richtigen Losses, der Erzwingung von Sparsity,
der Wahl guter Dimensionen, Lattices, Arten von Netzwerkschichten und - nicht zuletzt - augmentierte Trainingsdaten.
 
M3 700 Zeichen kurzverversion:

Neuronale Netze können jede Funktion approximieren, sie haben jedoch nicht die geringste Ahnung von allgemeinem Wissen über die Welt. Zum Beispiel lösen sich Objekte bei der Bilderkennung meistens nicht einfach in Luft auf und es gibt die Tendenz, dass Menschen mit zunehmendem Alter erst schneller, aber dann langsamer werden und irgendwann auch sterben. Nur, wie kodieren wir dieses Wissen?
 
Dieser Vortrag ist ein Überblick über bekannte Methoden, einschließlich der Wahl des richtigen Losses, der Erzwingung von Sparsity,
der Wahl guter Dimensionen, Lattices, Arten von Netzwerkschichten und - nicht zuletzt - augmentierte Trainingsdaten.

Vorkenntnisse:

Ein grundsätzliches Verständnis wie neuronale Netze trainiert werden und Vorhersagen machen.

Lernziele:

Teilnehmer bekommen eine Idee von der Herausforderung Weltwissen in einen Trainingsprozess einfließen zu lassen und einen Überblick über die existierenden Möglichkeiten.
 -->
            <!-- Title: How to teach our world knowledge to a neural network?

Neural networks are powerful approximators for any function. However they do not have the slightest idea of common knowledge of the world which often makes them fail miserably, especially when extrapolating to areas not covered by training data.

We, as human beings have that knowledge about the world and our domains of expertise, allowing deep learning models to become much more robust and even to extrapolate. But how do we encode this?

This talk is a survey on known methods including choosing the right loss, forcing sparsity, choose good dimensions, lattices, types of network layers, and - last but not least - augmented training data. 

There will also be a critique of too much trust in auto tuning libraries. They might win you a Kaggle competition, but might spoil your real-world applicability.

I will show actual code on different examples and share the code for you to take home as a starting point.

None of the techniques shown are new and you might already know a good chunk of them (probably not all of them, though), but maybe you have not looked at them from the perspective of setting priors for your deep learning by encoding the world knowledge you have.

Pitch: Deep Learning models are notorious for not being able to extrapolate from their area of training data. However, by encoding your world knowledge as priors you can at least push it in the right direction. It might even be the most important skill of a deep learning engineer to know how to do that.

Workshop:

Neural networks are powerful approximators for any function. However they do not have the slightest idea of common knowledge of the world which often makes them fail miserably, especially when extrapolating to areas not covered by training data.

We, as human beings have that knowledge about the world and our domains of expertise, allowing deep learning models to become much more robust and even to extrapolate. But how do we encode this?

Based on code examples we will go through the known methods including choosing the right loss, forcing sparsity, choose good dimensions, lattices, types of network layers, and - last but not least - augmented training data. 

None of the techniques shown are new and you might already know a good chunk of them (probably not all of them, though), but maybe you have not looked at them from the perspective of setting priors for your deep learning by encoding the world knowledge you have.


Format: Workshop oder Talk

Level: Intermediate

Konferenzen:
-  ODSC Europe / West: https://odsc.com/europe/call-for-speakers-europe/
- M3
- Scipy

Inhalte:
- GenerelL: Pre oder Post Processing kodiert oder dekodiert Weltwissen
- Data Augmentation using world knowledge
  - might explode with dimensions
  - augmented data might outweight "real" data 
- Sparsity
- CNN
- RNN
- Lattice
- Losses (xent vs mse)
- Why auto tuning might not be such a good idea (overfits on val data), but real priors might not even be obeyed
- Beispiel mit Lattice, Extrapolation und Tweet von Fchollet
- Daten-Beispiele selbst gemalte Bilder oder Sinus
- Hidden Markov Models
  * https://en.wikipedia.org/wiki/Markov_chain
- Kalman Filter
  * https://en.wikipedia.org/wiki/Kalman_filter

Weltwissen hauptsächlich Frage der Architektur
- Traditionelle Systeme instrumentieren viele kleine Modelle als Pattern matcher viel besser als
- Großes System als Blackbox

Großes System als Blackbox
Eigener Talk
- die wichtigste Architektur Frage im Machine Learning
- Ein großes Modell oder viele kleine traditionell instrumentiert

---

Talk: ODSC Europe

Why you should prefer many small models over a single large one

When designing a solution using machine learning one of the central architectural questions is whether you
should use a set of small models orchestrated by traditional code or a single big one that just figures out
things from beginning to end.

In this talk I take the standpoint that a single large one should be avoided for many reasons. The main
reason for choosing small ones is to keep being in control as the human domain expert. This applies to
training as well as putting in domain knowledge your models can not possibly have.

While this might also be of interest in the academic world in context of whether deep learning is the
solution to all in this talk we will look at it from a practical perspective.

Of course this is an over-generalization, but I found this to be true in all the projects I have
participated in and in many other common ones.


---

Talk: QCon

Title: The most important architectural decision in machine learning: many small models over a single large one?

actionable takeaways

What are the most important architectural questions in machine learning? 
Why architecture for machine learning has a special place in the world of software engineering. 
Among the important decisions you need to make early which are the ones that are especially hard to change. 
Why such a seemingly harmless question has so much emotional potential.

While sounding harmless at first sight, there is also a lot of emotional potential in this question.




what is by far the most important and hardly correctable architectural decision in retrospect

As another teaser: doesn't look important at first, and requires much explanation why it is so

-->




            <section data-markdown>
                <textarea data-template>
# World Knowledge Priors

    </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
### Case: Order

- https://raschka-research-group.github.io/coral-pytorch/
- Ok, aber die Tatsache dass man weiss dass da eine Ordnung ist ist vielleicht eine Art Prior. Aber das ist sehr weitgegriffen vielleicht
        </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
### Links

* Why Do Better Loss Functions Lead to Less Transferable Features?" https://twitter.com/skornblith/status/1469132061579620355
* Implicit inference of 3D vision: This short paper in the journal i-Perception presents a disconcerting visual illusion spotted “in the wild”: how stackable chairs, viewed from a certain angle, mess with your head [read more, paper: https://t.co/ppu9j0pyIs] https://t.co/r2Sdie3UdF
(https://twitter.com/Rainmaker1973/status/1489209455489212416?t=gxzQxv4J1yKQWVNnVkQ6bA&s=03)
* What I’ve learned about making synthetic data work for training ML models: (https://twitter.com/russelljkaplan/status/1490303023267999744?t=QHQ_IkP8zs8LEzzr6bWmQQ&s=03)  
    </textarea>
            </section>


        </div>
    </div>


    <script src="reveal.js/dist/reveal.js"></script>
    <script src="lib/jquery.js"></script>

    <script>
        const printMode = window.location.search.match(/print-pdf/gi);
        const isLocal = window.location.hostname.indexOf('localhost') !== -1 ||
            window.location.hostname.indexOf('127.0.0.1') !== -1;
        const isPresentation = isLocal && !printMode;
        const isPublic = !isPresentation;

        $('.hide').remove();

        if (isPresentation) {
        } else {
            // only applies to public version
            $('.todo').remove();
            $('.preparation').remove();
            $('.local').remove();
        }

        Reveal.addEventListener('ready', function (event) {
            // applies to all versions
            $('code').addClass('line-numbers');

            $('.fragments li').addClass('fragment')

            // make all links open in new tab
            $('a').attr('target', '_blank')

            if (isPresentation) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $('.fragment').removeClass('fragment');
            }

            // we do not like fragments
            // $('.fragment').removeClass('fragment');

        });

    </script>

    <script src="reveal.js/plugin/notes/notes.js"></script>
    <script src="reveal.js/plugin/markdown/markdown.js"></script>
    <script src="reveal.js/plugin/highlight/highlight.js"></script>
    <script>
        // More info about initialization & config:
        // - https://revealjs.com/initialization/
        // - https://revealjs.com/config/
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            history: true,
            center: true,
            width: 1100,
            slideNumber: true,
            hideInactiveCursor: false,


            // Learn about plugins: https://revealjs.com/plugins/
            plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
        });
    </script>


</body>

</html>