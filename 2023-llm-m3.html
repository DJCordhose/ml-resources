<!doctype html>
<html lang="de">

<head>
    <meta charset="utf-8">

    <title>M3 2023 - Artificial General Intelligence – die fehlenden 3%</title>

    <meta name="description" content="Resilient Machine Learning">
    <meta name="author" content="Oliver Zeigermann">
	<link rel="shortcut icon" href="/img/favicon.ico" >

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<!-- <link rel="stylesheet" href="reveal.js/dist/theme/black.css"> -->
	<link rel="stylesheet" href="reveal.js/dist/theme/white.css">

	<!-- Theme used for syntax highlighted code -->
	<!-- <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css"> -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/zenburn.css">
	<style>
		.right-img {
			margin-left: 10px !important;
			float: right;
			height: 500px;
		}

		.todo:before {
			content: 'TODO: ';
		}

		.todo {
			color: red !important;
		}

		code span.line-number {
			color: lightcoral;
		}

		.reveal pre code {
			max-height: 1000px !important;
		}

		img {
			border: 0 !important;
			box-shadow: 0 0 0 0 !important;
			height: 450px;
		}

		.reveal {
			-ms-touch-action: auto !important;
			touch-action: auto !important;
		}

		.reveal h1,
		.reveal h2,
		.reveal h3,
		.reveal h4 {
			/* letter-spacing: 2px; */
			font-family: 'Calibri', sans-serif;
			/* font-family: 'Times New Roman', Times, serif; */
			/* font-weight: bold; */
			color: black;
			/* font-style: italic; */
			/* letter-spacing: -2px; */
			text-transform: none !important;
		}

		.reveal em {
			font-weight: bold;
		}

		.reveal section img {
			background: none;
		}

		.reveal img.with-border {
			border: 1px solid #586e75 !important;
			box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
		}

		.reveal li {
			margin-bottom: 8px;
		}

		/* For li's that use FontAwesome icons as bullet-point */
		.reveal ul.fa-ul li {
			list-style-type: none;
		}

		.reveal {
			/* font-family: 'Work Sans', 'Calibri'; */
			font-family: 'Calibri';
			color: black !important;
			font-size: xx-large;
		}

		.container {
			display: flex;
		}

		.col img {
			height: auto;
		}

		.col,
		col-1 {
			flex: 1;
		}

		.col-2 {
			flex: 2;
		}

	/* https://intranet.openknowledge.de/plugins/servlet/mobile?contentId=18612939#content/view/18612939
      https://intranet.openknowledge.de/pages/viewpage.action?pageId=16157867
      https://openknowledgede.sharepoint.com/:p:/s/slides/EYWbMPz2ejhBprBrP8BYGxsBA3BrPNhDxJBrTXXvHi3-ZQ?e=jMZihi
       */
       /* body:after {
        content: url(img/ok/logo.png) ;
        position: fixed;
        bottom: 80px;
        left: -1080px;
        transform: scale(.06);
        height: 10px;
    } */

	</style>
</head>

<body style="background-color: whitesmoke;">

<div class="reveal">
    <div class="slides">


<!-- 
  ChatGPT und andere Large-Language-Modelle haben in den letzten 12 Monaten atemberaubende Fortschritte gemacht. ChatGPT kann mühelos Fragen beantworten, Hinweise verarbeiten und Antworten in Formaten wie Gedichten, Bibelversen oder Rapsongs wiedergeben. Da stellt sich die Frage:

Ist AGI (artifical general intelligence) jetzt gelöst? Oder wartet da nur nächste Welle der Ernüchterung?

In diesem Talk beleuchten wir die letzten Fortschritte und untersuchen im Detail:

* Was fehlt eigentlich noch?
* Worauf werden die Modelle eigentlich trainiert?
* Und bei allem Fortschritt, was sind die fehlenden 3% um 100% Genauigkeit zu bekommen?
* Und wie lange noch, bis KI die Welt übernimmt?!

Keynote:
- 

-->

 <section data-markdown>
	<textarea data-template>
# Artificial General Intelligence - die fehlenden 3%

https://www.m3-konferenz.de/veranstaltung-20071-0-artificial-general-intelligence--die-fehlenden-3%25.html, M3 2023, Karlsruhe 

Mikio Braun, Oliver Zeigermann

<img src="img/bit.ly_m3-llm-2023.png" style="height: 200px;">

Slides: https://bit.ly/m3-llm-2023

<!-- https://djcordhose.github.io/ml-resources/2023-llm-m3.html -->

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Umfrage - wie nahe sind wir an der "Artificial General Intelligence"

1. Bereits da
1. Innerhalb der nächsten 5 Jahren
1. In diesem Jahrhundert
1. Nie
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
## Gottgleiche KI zerstört die Welt?

https://t3n.de/news/gottgleiche-ki-zerstoerung-der-menschheit-1547175/
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Bisschen viel Drama, eher...

</textarea>
</section>

<section data-markdown>
	<textarea data-template>

_"Diese Dinger sind völlig anders als wir. Manchmal denke ich, es ist, als wären Außerirdische gelandet und die Menschen hätten es nicht bemerkt, weil sie sehr gut Englisch sprechen."_


https://www.heise.de/hintergrund/Erschreckend-wenn-man-das-sieht-KI-Pionier-Geoffrey-Hinton-ueber-KI-Modelle-8985819.html
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		_"Ich will den Klimawandel nicht abwerten. Ich will nicht sagen "macht Euch keine Sorgen über den Klimawandel"!
		Das ist auch ein großes Risiko", sagt Hinton. "Aber ich glaube, das könnte noch dringlicher sein!"_

		https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/
	</textarea>
</section>


<section data-markdown class="fragments">
	<textarea data-template>
### Steht AI eher für "Alien Intelligence"?
<!-- ### AI steht für "Alien Intelligence" -->

* Schachprogramme wie Stockfish spielen nicht übermenschlich, sondern unmenschlich stark
  * nicht wie ein besonders guter Mensch
  * sondern wie eine fremdartige Intelligenz
* bei Schach Grusel ala <a href='https://de.wikipedia.org/wiki/Uncanny_Valley'>Uncanny Valley</a>
* andere Bereiche sind aber sensibler
* autonomes Fahren
  * unvorhersehbare Aktionen
  * andersartige Fehler
* Sprachmodelle
  * fehlende Empathie
  * abweichende oder fehlende Ethik
  * dauerhafter "Mount Stupid"
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Heute konzentrieren wir uns auf Sprachmodelle

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Wer ist Olli

		<div style="display: flex;">
			<div style="flex: 50%;">
				<a href='https://oreilly.de/produkt/machine-learning-kurz-gut-2/'>
				<img src='img/ml-buch-v2.jpg' height="400">
				</a>
			</div>
			<div style="flex: 50%; font-size: x-large;">
				<img src='img/olli-opa.jpeg'>
			</div>
		</div>
		<p>
			<a target="_blank" href="mailto:OliverZeigermann@gmail.com">Oliver Zeigermann</a>:
		Entwickler, Architekt, Berater und Coach für Machine Learning
		</p>    
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Wer ist Mikio
		<img src='img/mikio-data.jpeg'>

		<a target="_blank" href="mailto:mikiobraun@gmail.com">Mikio Braun</a>:
		Ex-ML Researcher, Architekt, Berater für Machine Learning, aktuell baut er an https://fedistats.cc/
	</textarea>
</section>



<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. Wann übernimmt KI die Welt?
	</textarea>
</section>
  
<section data-markdown>
	<textarea data-template>
## Agenda

1. *Idee von Transformern / LLMs*
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. Wann übernimmt KI die Welt?
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Intelligenz auf Sprache reduzieren

_kann man ein Sprach-System trainieren, dass sich intelligent anfühlt?_

	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Wir bleiben beim subjektiven Gefühl als Definition von einem intelligenten System 

_Auch Turing meinte schon sinngemäß: eine strikte Definition macht wenig Sinn, wir merken schon wenn etwas intelligent ist_

	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Aber
## Wie trainiert man das denn?

	</textarea>
</section>

 <section data-markdown class="fragments">
### Probleme beim überwachten Lernen

* linearer Aufwand beim Labeln von Daten
* erhebliche Fehlerquote zu erwarten
  * Standard-Datensätze enthalten bis zu 10% Fehler
  * https://labelerrors.com/
* Unterschiede zwischen verschiedenen labelnden Personen
* Änderung der Labeldefinition könnte erfordern, von vorne zu beginnen

_nicht praktikabel bei großen Datensätzen_
</section>

 <section data-markdown class="fragments">
### Foundation Models: Transformer Kernideen

1. Basis ist ein verallgemeinertes Sprachmodell
1. Wahrscheinlichkeiten von Wortfolgen vorhersagen
1. Self-Supervision zur Nutzung großer ungelabelter Datensätze (auch bekannt als unüberwachtes Vortraining)
1. Training auf einem sehr großen Korpus
1. Self-attention zur Kodierung weitreichender Abhängigkeiten
1. zusätzliches überwachtes Training für nachgelagerte Aufgaben, z.B.
    - Übersetzung (lang1 & lang2 Paare)
    - Beantwortung von Fragen (Q&A-Paare)
    - Stimmungsanalyse (Text-/Stimmungspaare) 
    - usw.
</section>
    
<section data-markdown>
<textarea data-template>
### Transformer Architektur: Encoder/Decoder

<div style="display: flex; align-items: center; justify-content: space-around;">
  <div>
    <ul>
      <li>links: Encoder</li>
      <li>rechts: Decoder</li>
      <li>Encoder speist die Embeddings der Eingabe in den Decoder ein</li>
      <li>Der Decoder benötigt Kontext / initialen Prompt, um zu funktionieren</li>
    </ul>
</div>
  <div style="width: 100%;">
    <img src='img/transformers/transformer-encoder-decoder.png'>
</div>
</div>
<small>

https://arxiv.org/pdf/1706.03762.pdf
</small>

        </textarea>
</section>

<section data-markdown class="fragments">
### Transformer Zoo

* der ursprüngliche Transformer war für Übersetzungsaufgaben gedacht
* die Verwendung hat sich seitdem ausgeweitet
* es entstand ein ganzer Zoo von Transformatoren
* einige verwenden nur Encoder
* einige verwenden nur Decoder
* einige verwenden eine Kombination aus Encoder/Decoder, genau wie der ursprüngliche Transformator

<!-- https://huggingface.co/transformers/model_summary.html
https://huggingface.co/transformers/#supported-models -->
</section>


<section data-markdown class="fragments">
### Decoder only (GPT-like)

_auch autoregressive Transformer-Modelle genannt_

* der Decoderteil kann gegebene Eingaben in vollständige Sätze umwandeln
* benötigt Kontext / initialen Prompt
* z.B. nützlich, um 
  * angefangene Sätze zu vervollständigen
  * auf Fragen zu reagieren
  * ein Gespräch zu führen
* erzeugt eine Antwort iterativ ("autoregressiv")
* GPT wäre ein Beispiel für diese Art von Anwendung 
  * unidirektional: trainiert, um das nächste Wort vorherzusagen
  * von OpenAI

  </section>
    
  <section data-markdown class="fragments">
    <textarea data-template>
  ### Training GPT

* self-supervised Training
* Vorhersage des nächsten Wortes anhand der vorherigen Wörter
* Kontext (Anzahl der in Betracht gezogenen vorherigen Wörter) ist begrenzt
* Unterschiedliche Versionen haben unterschiedlich viel Kontext
  * von 2k - GPT-3 
  * über 4k - GPT-3.5 
  * bis 32k - GPT-4

https://huggingface.co/transformers/model_summary.html#original-gpt
https://platform.openai.com/docs/models
    </textarea>
  </section>

  <section data-markdown class="fragments">
### ChatGPT

* GPT optimiert auf Dialoge
* Basiert auf https://openai.com/research/instruction-following
* zusätzliches überwachtes Training
  * was ist eine gute Antwort?
* Reinforcement Learning from Human Feedback (RLHF)
  * PPO
  * Reward kommt weiterem Reward-Modell
  * ebenfalls überwacht trainiert
* das ursprüngliche ChaptGPT basiert auf GPT-3.5, Wissenstand Ende 2021
* aktuellste Beta-Versionen basieren auf GPT-4 (allerdings nur mit 8k Kontext)

https://openai.com/blog/chatgpt

  </section>
  
<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. *Was ist daran toll?*
1. Was ist daran nicht so toll?
1. Wann übernimmt KI die Welt?
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Fähigkeiten Hui / Wissen Pfui

<img src="img/transformers/Wissen-Faehigkeit.jpg" style="height: 600px">

	</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Plugins kompensieren Mangel an Wissen

* Zugriff auf aktuelle Informationen
  * Reisen, Essen
  * Produkte / Shopping
  * Wissen
* Internet Browsing
* Anzapfen andere Datenquellen
* Aktionen
  <!-- * Einkäufe
  * Buchungen
  * Prozessabläufe -->
* Integration eigener Plugins über OpenAPI Spec und natursprachliche Informationen   

https://openai.com/blog/chatgpt-plugins
https://platform.openai.com/docs/plugins/getting-started/
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### LLMs - next level machine learning?

* Wir brauchen nicht mehr viele technische Details
* Stattdessen geben wir den Kontext und und schreiben einen schlauen Prompt
* Wie der Umbruch von Assembler zu Hochsprachen vermittels schlauer Compiler
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### LLMs als letzter Sprung in der Entwicklung der Menschheit

* *Schrift* - nicht nur die Natur, sondern auch wir Menschen können Dinge für die Nachwelt festhalten
* *Druckerpresse* - Demokratisierung des Wissens beginnt
* *Internet* - Auflösung physikalischer Grenzen
* *LLMs* - Konzentrat / Essenz der Inhalte des Internets

Marshall McLuhan in https://philosophygeek.substack.com/p/wrapping-my-head-around-ai
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. Was ist daran toll?
1. *Was ist daran nicht so toll?*
1. Wann übernimmt KI die Welt?
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### KI kann nicht kochen

<img src="img/ki-kochen.jpg">

Kein noch so kluges System hat nennenswerte motorische Fähigkeiten, die "einfachen" Tätigkeiten sind nicht annähernd erreicht 
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
<img src="img/picasso-computer-nutzlos.jpg">

https://twitter.com/szmagazin/status/1652568468976467969
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### "Stochastic Parrot"

Timnit Gebru

<div class="container">
	<img src="img/llm-m3/812px-Timnit_Gebru_crop.jpg" style="height: 300px"/>
<div>
<div>

LLM Modelle "denken" nicht, sie sagen nur das wahrscheinlichste Wort voraus.

Sie sind wie ein professioneller "Bullshitter" haben keine Ahnung, wovon sie reden, aber sagen, was sich plausibel anhört.
	
</div>
</div>

</div>

By TechCrunch - https://www.flickr.com/photos/52522100@N07/30671211838, CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=97995768
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Bias

		<img src="img/llm-m3/bias-mmitchell.png">

		https://twitter.com/mmitchell_ai/status/1650110045781393410
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Trainingsdaten

		<img src="img/llm-m3/ai-generated-art-unhappy.png">

		https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/ 
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Trainingsdaten

		<img src="img/llm-m3/photos-removed.png">

		https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Trainingsdaten

		<img src="img/llm-m3/samsung-data-leak.png">

		https://gizmodo.com/chatgpt-ai-samsung-employees-leak-data-1850307376 
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Völlig neue Angriffsvektoren

		<img src="img/llm-m3/prompt-injection.png">

		https://simonwillison.net/2022/Sep/12/prompt-injection/
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Völlig neue Angriffsvektoren

		<img src="img/llm-m3/prompt-injection.png">

		https://simonwillison.net/2022/Sep/12/prompt-injection/
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Und noch mehr Spam

		<img src="img/llm-m3/as-an-ai-language-model.png">

		https://www.theverge.com/2023/4/25/23697218/ai-generated-spam-fake-user-reviews-as-an-ai-language-model
	</textarea>
</section>


<section data-markdown>
### Der Mensch muss die Fragen stellen und die Antworten bewerten 

- durch den Einsatz eines solchen Systems klarer, was genau menschliche
Fähigkeiten sind und was auch Maschinen erledigen könnten.
- Ein Mensch stellt die Fragen, hat die Motivation 
- Ein KI-System kann Vorschläge für die Antworten machen, die Entscheidung liegt
aber in menschlicher Hand.


- Selbst die Arbeit von Künstlern und Schriftstellern kann hauptsächlich darin bestehen, aus Möglichkeiten auszuwählen 
- William S. Burroughs beschreibt seine Arbeit hauptsächlich als Auswahl: “Out of hundreds of possible sentences that I might have used, I chose one” (Aus
Hunderten von möglichen Sätzen, die ich hätte verwenden können, habe ich einen ausgewählt).

https://internationaltimes.it/william-s-burroughs-the-art-of-fiction/
</section>


<!-- <section data-markdown class="fragments todo">
### Don't forget: Transformers are language models

* No abstract reasoning like it is in our brains takes place
* The basis is the expression of thoughts in texts and code, etc.
* That's the way the system is trained
* Whether this is also intelligent is a pointed question
* Turing would probably say it doesn't matter
* One can argue that this system passes his test
  * https://twitter.com/glouppe/status/1438496208343949318
</section>

<section data-markdown class="fragments todo">
### On the Opportunities and Risks of Foundational Models

* foundational models: trained on broad data at scale and are adaptable to a wide range of downstream tasks
* ML is undergoing a paradigm shift with the rise of these models
* their scale results in new emergent capabilities 
* defects of the foundation model are inherited by all the adapted models downstream
* lack of clear understanding of how they work, when they fail, and what they are even capable of

https://arxiv.org/abs/2108.07258
</section> -->


<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. *Wann übernimmt KI die Welt?*
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Prompt Engineering als Durchbruch?

- Zusammenfassen (z.B. Zusammenfassen von Nutzerbewertungen)
- Vorhersage (z.B. Sentiment Analyse, Themenextraktion)
- Text umwandeln (z.B. Übersetzung, Rechtschreib- und Grammatikkorrektur)
- Erweitern (z.B. automatisches Verfassen von E-Mails)

<img src="img/course-prompt-engineering.png" style="height: 250px;">

https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/
	</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Kann von so einem Ort eine echte Revolution ausgehen?

<img src="img/openAI-HQ.jpeg">

OpenAI HQ in San Francisco - so sieht doch keine richtige Schurkenbude aus!
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Wie wahrscheinlich ist das Ende der Menschheit (durch AI)?

<img src="img/llm-m3/strangeloop.png"/>

https://www.strangeloopcanon.com/p/agi-strange-equation
	</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Bringen erst kleinere Open Source Modelle die große Revolution?

- Foundational Modelle wie LLaMa sind/werden verfügbar
- Finetuning wird immer einfacher
- Effiziente Modelle (ohne GPUs, mit Quantisierung, etc.)

https://www.semianalysis.com/p/google-we-have-no-moat-and-neither
</textarea>
</section>


<section data-markdown>
<textarea data-template>
<img src='img/transformers/gpt-3-2-years.jpg' style="height: 450px;">

<small>

  https://twitter.com/EMostaque/status/1530554442835189761</small>
        </textarea>
</section>
    

<!-- <section data-markdown class="fragments">
### Evolution of GPT

GPT: Generative Pre-Trained Transformer

* GPT-1: 2018, 110 million parameters (https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), https://www.youtube.com/watch?v=LOCzBgSV4tQ
* GPT-2: 2019, 1.5 billion parameters (https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), https://www.youtube.com/watch?v=BXv1m9Asl7I
* GPT-3: 2020, 175 billion parameters (https://arxiv.org/abs/2005.14165), https://www.youtube.com/watch?v=wYdKn-X4MhY
* GPT-4: 2023, probably not much larger, but trained on more data and more context (4096 instead of 2048) (https://analyticsindiamag.com/gpt-4-sam-altman-confirms-the-rumours/)

</section> -->

<!-- <section data-markdown class="todo">
### GPT-4 - Das Ende der Fahnenstange?
  
- Größe scheint ausreichend (https://techcrunch.com/2023/04/14/sam-altman-size-of-llms-wont-matter-as-much-moving-forward)
- Aber die Post geht gerade erst an: "We should expect that the field continues to advance rapidly" (https://ourworldindata.org/ai-investments)

https://arxiv.org/abs/2303.08774

</section> -->

<!-- <section data-markdown class="todo">
### Mehr Context === AGI?

```

Olli
- vielleicht brauchen wir echt nur richtig viel Kontext
- so dass das Ding eine "Persönlichkeit" herausbildet
- alles noch weiß, was passiert ist
- wäre doch ein Schritt: Hallo, Olli, gestern hast du mich das schonmal gefragt. Kann es sein, dass du irgendwie dumm bist? :D

Mikio
- Hehe
- Oder so servil überhöflich
- Lieber Olli, mir ist aufgefallen, und versteh das bitte nicht als Vorwurf, aber ein Gedanke ähnlicher Art äußertest Du schon am 2023-04-13T13:43:00…

Olli
- und man kann sich das Modell aussuchen
- frech oder lieb

Mikio
- So muss das!
- Am besten noch, es passt sich selber so an um dein Engagament zu maximieren
- Findet alle Triggerpunkte
- So wie Kinder

Olli
- dann kannst du auswählen "Kind", "toxischer Partner", "..."
- "übler Chef"

Mikio
- Wir haben so eine wunderbare Zukunft vor uns!
- ich freue mich auch schon

Olli
- aber irgendwie ist das ja auch schon so
- also fast schon Gegenwart
```

https://arxiv.org/abs/2304.11062
- https://twitter.com/dr_cintas/status/1650527281066962947
- https://www.youtube.com/watch?v=4Cclp6yPDuw

</section>

<section data-markdown class="todo">

https://techcrunch.com/2023/04/24/snapchat-sees-spike-in-1-star-reviews-as-users-pan-the-my-ai-feature-calling-for-its-removal/
</section> -->


<section data-markdown>
### Braucht echte AGI nicht einen Körper?

- Ein KI System kann nicht aktiv Erfahrungen sammeln
- Reinforcement Learning als eine Art, Erfahrungen in der Welt zu machen?
- Aber in der physischen Welt kann sich auch so ein System immer noch nicht bewegen
- Kann zum Teil zurückgeführt werden auf das ungelöste Leib-Seele-Problem 
  - https://de.wikipedia.org/wiki/Philosophie_des_Geistes#Das_Leib-Seele-Problem
  - https://bigthink.com/thinking/the-mind-body-problem/

</section>


<section data-markdown>
  <textarea data-template>
<img src='img/transformers/twitter-fchollet-one-shot.png'>

<small>

  https://twitter.com/fchollet/status/1528069621047128065</small>
        </textarea>
  </section>

  <section data-markdown>
	<textarea data-template>
### Also, was fehlt zu den 100%?

- Introspektion, nicht nur stumpf Worte aneinanderreihen.
- Wie kann man neues Wissen hineinbringen?
- Empathie & soziale Beziehungen, nicht nur psychopatisches Manipulieren.
</textarea>
</section>

  <section data-markdown class="fragments" style="font-size: x-large;">
	<textarea data-template>
### Zusammenfassung: Was kann den Durchbruch bringen?

* Mehr Breite
  * Destillation
  * Unterschiedliche OS Modelle
  * Diversität in der Trainingsdatenbasis
  * Prompt Engineering wird so wichtig wie googeln
* Lediglich Inkremente  
  * Besseres Prompt Engineering
  * Mehr Kontext
  * Bessere Schurken
  * Größere Modelle
  * Zugriff auf alle APIs
* Veränderte Ansätze  
  * Modell-Ensemble, um gegen Mainstream Wissen vorzugehen
  * Bessere Methoden der Abstraktion, schnelleres Lernen anhand von Konzepten
  * Ein Körper und Interaktion mit der Umwelt
  * Kombination unterschiedlicher Modalitäten
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Es gibt eine Entwicklung

_Ich habe ChatGPT gefragt, was zu AGI fehlt und bei der Antwort, die ja auch der Datenbasis von Ende 2021 basiert, kamen Punkte
auf, die inzwischen durch ChatGPT adressiert wurden_

1. Understanding and reasoning
2. Transfer learning
3. Common sense reasoning
4. Scalability and efficiency
5. Learning from limited data and experience
6. Integrating different modalities
7. Emotional and social intelligence
8. Ethics and safety

_Zumindest 2 und 3 ist adressiert, evtl. auch 1 und 6, 4 sind in der Mache_

>>>>>>> 0121e52fe4544b94acd77af4bb664b730e353bf9
	</textarea>
</section>


  <section data-markdown>
	<textarea data-template>
## Umfrage - wie nahe sind wir an der "Artificial General Intelligence"

1. Bereits da
1. Innerhalb der nächsten 5 Jahren
1. In diesem Jahrhundert
1. Nie
</textarea>
</section>
  
<!-- <section data-markdown class="fragments todo">
  <textarea data-template>
### Zusammenfassung

1. ChatGPT ist ein auf Dialoge spezialisiertes Sprachmodell
1. Es basiert auf der GPT-Reihe von OpenAI
1. Sprachmodellen fehlen viele wichtige menschliche Eigenschaften 
  * fehlende Empathie
  * abweichende oder fehlende Ethik
  * dauerhafter "Mount Stupid"
  * Lernfähigkeit

</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Weitere Quellen als Referenz

- https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/
- https://philosophygeek.substack.com/p/wrapping-my-head-around-ai
- https://bigthink.com/thinking/the-mind-body-problem/
- Wolfram ChatGPT Plugin Blends Symbolic AI with Generative AI: https://thenewstack.io/wolfram-chatgpt-plugin-blends-symbolic-ai-with-generative-ai/
- Technische Details wenn man es echt ernst meint: https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/
- Engineering: https://huyenchip.com/2023/04/11/llm-engineering.html
- Vicuna als Open Source Alternative
  - https://chat.lmsys.org/
  - https://www.linkedin.com/posts/christoph-henkelmann_fastchat-activity-7050938707561861120-gSJt

</textarea>
</section> -->

<section data-markdown>
	<textarea data-template>
# Vielen Dank
## Artificial General Intelligence - die fehlenden 3%

Bleibt im Kontakt

Mikio Braun mikiobraun@gmail.com, @mikiobraun, https://www.linkedin.com/in/mikiobraun

Oliver Zeigermann: oliver@zeigermann.de, @DJCordhose, https://www.linkedin.com/in/oliver-zeigermann-34989773

Slides: https://bit.ly/m3-llm-2023

</textarea>
</section>




</div>
</div>

<script src="reveal.js/dist/reveal.js"></script>
<script src="lib/jquery.js"></script>
<script>
    const printMode = window.location.search.match(/print-pdf/gi);
    const isLocal = window.location.hostname.indexOf('localhost') !== -1 ||
        window.location.hostname.indexOf('127.0.0.1') !== -1;
    const isPresentation = isLocal && !printMode;
    const isPublic = !isPresentation;

    $('.hide').remove();

    if (isPresentation) {
    } else {
        // only applies to public version
        $('.todo').remove();
        $('.preparation').remove();
        $('.local').remove();
    }

    Reveal.addEventListener('ready', function (event) {
        // applies to all versions
        $('code').addClass('line-numbers');

        $('.fragments li').addClass('fragment')

        // make all links open in new tab
        $('a').attr('target', '_blank')

        if (isPresentation) {
            // only applies to presentation version
            Reveal.configure({ controls: false });
        } else {
            // only applies to public version
            $('.fragment').removeClass('fragment');
        }

        // we do not like fragments
        // $('.fragment').removeClass('fragment');

        if (printMode) {
          Reveal.configure({ 
            controls: false,
            // slideNumber: false 
          });
          $('.fragment').removeClass('fragment');
      
        }


    });

</script>

<script src="reveal.js/plugin/notes/notes.js"></script>
<script src="reveal.js/plugin/markdown/markdown.js"></script>
<script src="reveal.js/plugin/highlight/highlight.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,
        controls: true,
        progress: true,
        history: true,
        center: true,
        width: 1100,
        slideNumber: true,
        hideInactiveCursor: false,
        transition: 'none', // none/fade/slide/convex/concave/zoom


        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
    });
</script>


</body>

</html>