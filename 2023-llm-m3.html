<!doctype html>
<html lang="de">

<head>
    <meta charset="utf-8">

    <title>M3 2023 - Artificial General Intelligence – die fehlenden 3%</title>

    <meta name="description" content="Resilient Machine Learning">
    <meta name="author" content="Oliver Zeigermann">
	<link rel="shortcut icon" href="/img/favicon.ico" >

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<!-- <link rel="stylesheet" href="reveal.js/dist/theme/black.css"> -->
	<link rel="stylesheet" href="reveal.js/dist/theme/white.css">

	<!-- Theme used for syntax highlighted code -->
	<!-- <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css"> -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/zenburn.css">
	<style>
		.right-img {
			margin-left: 10px !important;
			float: right;
			height: 500px;
		}

		.todo:before {
			content: 'TODO: ';
		}

		.todo {
			color: red !important;
		}

		code span.line-number {
			color: lightcoral;
		}

		.reveal pre code {
			max-height: 1000px !important;
		}

		img {
			border: 0 !important;
			box-shadow: 0 0 0 0 !important;
			height: 450px;
		}

		.reveal {
			-ms-touch-action: auto !important;
			touch-action: auto !important;
		}

		.reveal h1,
		.reveal h2,
		.reveal h3,
		.reveal h4 {
			/* letter-spacing: 2px; */
			font-family: 'Calibri', sans-serif;
			/* font-family: 'Times New Roman', Times, serif; */
			/* font-weight: bold; */
			color: black;
			/* font-style: italic; */
			/* letter-spacing: -2px; */
			text-transform: none !important;
		}

		.reveal em {
			font-weight: bold;
		}

		.reveal section img {
			background: none;
		}

		.reveal img.with-border {
			border: 1px solid #586e75 !important;
			box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
		}

		.reveal li {
			margin-bottom: 8px;
		}

		/* For li's that use FontAwesome icons as bullet-point */
		.reveal ul.fa-ul li {
			list-style-type: none;
		}

		.reveal {
			/* font-family: 'Work Sans', 'Calibri'; */
			font-family: 'Calibri';
			color: black !important;
			font-size: xx-large;
		}

		.container {
			display: flex;
		}

		.col img {
			height: auto;
		}

		.col,
		col-1 {
			flex: 1;
		}

		.col-2 {
			flex: 2;
		}

	/* https://intranet.openknowledge.de/plugins/servlet/mobile?contentId=18612939#content/view/18612939
      https://intranet.openknowledge.de/pages/viewpage.action?pageId=16157867
      https://openknowledgede.sharepoint.com/:p:/s/slides/EYWbMPz2ejhBprBrP8BYGxsBA3BrPNhDxJBrTXXvHi3-ZQ?e=jMZihi
       */
       /* body:after {
        content: url(img/ok/logo.png) ;
        position: fixed;
        bottom: 80px;
        left: -1080px;
        transform: scale(.06);
        height: 10px;
    } */

	</style>
</head>

<body style="background-color: whitesmoke;">

<div class="reveal">
    <div class="slides">


<!-- 
  ChatGPT und andere Large-Language-Modelle haben in den letzten 12 Monaten atemberaubende Fortschritte gemacht. ChatGPT kann mühelos Fragen beantworten, Hinweise verarbeiten und Antworten in Formaten wie Gedichten, Bibelversen oder Rapsongs wiedergeben. Da stellt sich die Frage:

Ist AGI (artifical general intelligence) jetzt gelöst? Oder wartet da nur nächste Welle der Ernüchterung?

In diesem Talk beleuchten wir die letzten Fortschritte und untersuchen im Detail:

* Was fehlt eigentlich noch?
* Worauf werden die Modelle eigentlich trainiert?
* Und bei allem Fortschritt, was sind die fehlenden 3% um 100% Genauigkeit zu bekommen?
* Und wie lange noch, bis KI die Welt übernimmt?!
-->

<section data-markdown class="todo">
	<textarea data-template>
## Mikio

- ChatGPT Übersicht im tollen Stil
- Zweiter Teil
- Ende-Folie, Kontakt Mikio

</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
## Olli

- Erster Teil
  - Material ausdünnen und übersetzen
  - Übergang aus Miro nehmen: Wissen vs Fähigkeit
  - Was können Plugins daran tun?
    - https://openai.com/blog/chatgpt-plugins
- Neue Quellen angucken: 
  - https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/
  - https://philosophygeek.substack.com/p/wrapping-my-head-around-ai
  - Wolfram ChatGPT Plugin Blends Symbolic AI with Generative AI: https://thenewstack.io/wolfram-chatgpt-plugin-blends-symbolic-ai-with-generative-ai/
- Technische Details wenn man es echt ernst meint
  - https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/
- Folie Umfrage Stimmung / Meinung 
  - am Anfang
  - am Ende
  - Vergleich
- Zusammenfassung
- bit.ly
- Alien Intelligence
- Closed Source vs Open Source
  - Vicuna
    - https://www.linkedin.com/posts/christoph-henkelmann_fastchat-activity-7050938707561861120-gSJt
    - https://chat.lmsys.org/
- Engineering
  - https://huyenchip.com/2023/04/11/llm-engineering.html

</textarea>
</section>


 <section data-markdown>
	<textarea data-template>
# Artificial General Intelligence - die fehlenden 3%

https://www.m3-konferenz.de/veranstaltung-20071-0-artificial-general-intelligence--die-fehlenden-3%25.html, M3 2023, Karlsruhe 

Mikio Braun

Oliver Zeigermann

Slides: https://bit.ly/m3-llm-2023

<!-- https://djcordhose.github.io/ml-resources/2023-llm-m3.html -->

</textarea>
</section>


<section data-markdown>
	<textarea data-template>
## Gottgleiche KI zerstört die Welt?

https://t3n.de/news/gottgleiche-ki-zerstoerung-der-menschheit-1547175/
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Bisschen viel Drama, eher...

</textarea>
</section>


<section data-markdown class="fragments">
	<textarea data-template>
<!-- ### Steht AI für "Alien Intelligence"? -->
### AI für "Alien Intelligence"

* Schachprogramme wie Stockfish spielen nicht übermenschlich, sondern unmenschlich stark
  * nicht wie ein besonders guter Mensch
  * sondern wie eine fremdartige Intelligenz
* bei Schach Grusel ala <a href='https://de.wikipedia.org/wiki/Uncanny_Valley'>Uncanny Valley</a>
* andere Bereiche sind aber sensibler
* autonomes Fahren
  * unvorhersehbare Aktionen
  * andersartige Fehler
* Sprachmodelle
  * fehlende Empathie
  * abweichende oder fehlende Ethik
  * dauerhafter "Mount Stupid"
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Heute konzentrieren wir uns auf Sprachmodelle

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Wer ist Olli

		<div style="display: flex;">
			<div style="flex: 50%;">
				<a href='https://oreilly.de/produkt/machine-learning-kurz-gut-2/'>
				<img src='img/ml-buch-v2.jpg' height="400">
				</a>
			</div>
			<div style="flex: 50%; font-size: x-large;">
				<img src='img/olli-opa.jpeg'>
			</div>
		</div>
		<p>
			<a target="_blank" href="mailto:OliverZeigermann@gmail.com">Oliver Zeigermann</a>:
		Entwickler, Architekt, Berater und Coach für Machine Learning
		</p>    
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Wer ist Mikio
		<img src='img/mikio-data.jpeg'>

		<a target="_blank" href="mailto:mikiobraun@gmail.com">Mikio Braun</a>:
		Ex-ML Researcher, Architekt, Berater und Mentor für Machine Learning
	</textarea>
</section>



<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. Wann übernimmt KI die Welt?
	</textarea>
</section>
  
<section data-markdown>
	<textarea data-template>
## Agenda

1. *Idee von Transformern / LLMs*
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. Wann übernimmt KI die Welt?
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Intelligenz auf Sprache reduzieren

_kann man ein Sprach-System trainieren, dass sich intelligent anfühlt?_


	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Aber
## Wie trainiert man das denn?

	</textarea>
</section>

 <section data-markdown class="fragments">
### Probleme beim überwachten Lernen

* linearer Aufwand beim labeln von Daten
* erhebliche Fehlerquote zu erwarten
  * Standard-Datensätze enthalten bis zu 10% Fehler
  * https://labelerrors.com/
* Unterschiede zwischen verschiedenen labelnden Personen
* Änderung der Labeldefinition könnte erfordern, von vorne zu beginnen

_nicht praktikabel bei großen Datensätzen_
</section>

 <section data-markdown class="fragments">
### Foundation Models: Transformer Kernideen

1. Self-Supervision zur Nutzung großer ungelabelter Datensätze (auch bekannt als unüberwachtes Vortraining)
1. ein verallgemeinertes Sprachmodell haben
1. Wahrscheinlichkeiten von Wortfolgen vorhersagen
1. Training auf einem sehr großen Korpus
1. Self-attention zur Kodierung weitreichender Abhängigkeiten
1. zusätzliches überwachtes Training für nachgelagerte Aufgaben, z.B.
    - Übersetzung (lang1 & lang2 Paare)
    - Beantwortung von Fragen (Q&A-Paare)
    - Stimmungsanalyse (Text-/Stimmungspaare) 
    - usw.
</section>
    
<section data-markdown>
<textarea data-template>
### Transformer Architektur: Encoder/Decoder

<div style="display: flex; align-items: center; justify-content: space-around;">
  <div>
    <ul>
      <li>links: Encoder</li>
      <li>rechts: Decoder</li>
      <li>Encoder speist die Embeddings der Eingabe in den Decoder ein</li>
      <li>Der Decoder benötigt Kontext / initialen Prompt, um zu funktionieren</li>
    </ul>
</div>
  <div style="width: 100%;">
    <img src='img/transformers/transformer-encoder-decoder.png'>
</div>
</div>
<small>

https://arxiv.org/pdf/1706.03762.pdf
</small>

        </textarea>
</section>

<section data-markdown class="fragments">
### Transformer Zoo

* der ursprüngliche Transformer war für Übersetzungsaufgaben gedacht
* die Verwendung hat sich seitdem ausgeweitet
* es entstand ein ganzer Zoo von Transformatoren
* einige verwenden nur Encoder
* einige verwenden nur Decoder
* einige verwenden eine Kombination aus Encoder/Decoder, genau wie der ursprüngliche Transformator

<!-- https://huggingface.co/transformers/model_summary.html
https://huggingface.co/transformers/#supported-models -->
</section>


<section data-markdown class="fragments">
### Decoder only (GPT-like)

_also called auto-regressive Transformer models_

* the decoder part can transform given inputs into complete sentences
* e.g. useful in itself, to complete started sentences
* generates a response iteratively ("auto regressive")
* GPT would be an example for this kind of application 
  * unidirectional: trained to predict next word
  * by OpenAI 
  
  </section>
    
  <section data-markdown>
  ### Training GPT
  
  * self-supervised training
  * predict the next word, given all of the previous words within some text
  * has a limited context
  
  https://huggingface.co/transformers/model_summary.html#original-gpt
  https://huggingface.co/transformers/model_doc/gpt2.html
  </section>
  
<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. *Was ist daran toll?*
1. Was ist daran nicht so toll?
1. Wann übernimmt KI die Welt?
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Fähigkeiten Hui / Wissen Pfui

<img src="img/transformers/Wissen-Faehigkeit.jpg" style="height: 600px">

	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. Was ist daran toll?
1. *Was ist daran nicht so toll?*
1. Wann übernimmt KI die Welt?
	</textarea>
</section>

<section data-markdown class="fragments">
### Don't forget: Transformers are language models

* No abstract reasoning like it is in our brains takes place
* The basis is the expression of thoughts in texts and code, etc.
* That's the way the system is trained
* Whether this is also intelligent is a pointed question
* Turing would probably say it doesn't matter
* One can argue that this system passes his test
  * https://twitter.com/glouppe/status/1438496208343949318
</section>

<section data-markdown class="fragments">
### On the Opportunities and Risks of Foundational Models

* foundational models: trained on broad data at scale and are adaptable to a wide range of downstream tasks
* ML is undergoing a paradigm shift with the rise of these models
* their scale results in new emergent capabilities 
* defects of the foundation model are inherited by all the adapted models downstream
* lack of clear understanding of how they work, when they fail, and what they are even capable of

https://arxiv.org/abs/2108.07258
</section>


<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. *Wann übernimmt KI die Welt?*
	</textarea>
</section>


<section data-markdown>
<textarea data-template>
<img src='img/transformers/gpt-3-2-years.jpg' style="height: 450px;">

<small>

  https://twitter.com/EMostaque/status/1530554442835189761</small>
        </textarea>
</section>
    

<section data-markdown class="fragments">
### Evolution of GPT

GPT: Generative Pre-Trained Transformer

* GPT-1: 2018, 110 million parameters (https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), https://www.youtube.com/watch?v=LOCzBgSV4tQ
* GPT-2: 2019, 1.5 billion parameters (https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), https://www.youtube.com/watch?v=BXv1m9Asl7I
* GPT-3: 2020, 175 billion parameters (https://arxiv.org/abs/2005.14165), https://www.youtube.com/watch?v=wYdKn-X4MhY
* GPT-4: 2023, probably not much larger, but trained on more data and more context (4096 instead of 2048) (https://analyticsindiamag.com/gpt-4-sam-altman-confirms-the-rumours/)

</section>

<section data-markdown class="todo">
### GPT-4 - Das Ende der Fahnenstange?
  
- Größe scheint ausreichend (https://techcrunch.com/2023/04/14/sam-altman-size-of-llms-wont-matter-as-much-moving-forward)
- Aber die Post geht gerade erst an: "We should expect that the field continues to advance rapidly" (https://ourworldindata.org/ai-investments)
</section>

<section data-markdown>
  <textarea data-template>
<img src='img/transformers/twitter-fchollet-one-shot.png'>

<small>

  https://twitter.com/fchollet/status/1528069621047128065</small>
        </textarea>
  </section>


  

<section data-markdown>
	<textarea data-template>
# Vielen Dank
## Artificial General Intelligence - die fehlenden 3%

Bleibt im Kontakt

https://www.linkedin.com/in/oliver-zeigermann-34989773/

oliver@zeigermann.de

Twitter: @DJCordhose

Slides: https://bit.ly/m3-llm-2023

</textarea>
</section>




</div>
</div>

<script src="reveal.js/dist/reveal.js"></script>
<script src="lib/jquery.js"></script>
<script>
    const printMode = window.location.search.match(/print-pdf/gi);
    const isLocal = window.location.hostname.indexOf('localhost') !== -1 ||
        window.location.hostname.indexOf('127.0.0.1') !== -1;
    const isPresentation = isLocal && !printMode;
    const isPublic = !isPresentation;

    $('.hide').remove();

    if (isPresentation) {
    } else {
        // only applies to public version
        $('.todo').remove();
        $('.preparation').remove();
        $('.local').remove();
    }

    Reveal.addEventListener('ready', function (event) {
        // applies to all versions
        $('code').addClass('line-numbers');

        $('.fragments li').addClass('fragment')

        // make all links open in new tab
        $('a').attr('target', '_blank')

        if (isPresentation) {
            // only applies to presentation version
            Reveal.configure({ controls: false });
        } else {
            // only applies to public version
            $('.fragment').removeClass('fragment');
        }

        // we do not like fragments
        // $('.fragment').removeClass('fragment');

        if (printMode) {
          Reveal.configure({ 
            controls: false,
            // slideNumber: false 
          });
          $('.fragment').removeClass('fragment');
      
        }


    });

</script>

<script src="reveal.js/plugin/notes/notes.js"></script>
<script src="reveal.js/plugin/markdown/markdown.js"></script>
<script src="reveal.js/plugin/highlight/highlight.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,
        controls: true,
        progress: true,
        history: true,
        center: true,
        width: 1100,
        slideNumber: true,
        hideInactiveCursor: false,
        transition: 'none', // none/fade/slide/convex/concave/zoom


        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
    });
</script>


</body>

</html>