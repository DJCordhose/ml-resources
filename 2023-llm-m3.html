<!doctype html>
<html lang="de">

<head>
    <meta charset="utf-8">

    <title>M3 2023 - Artificial General Intelligence ‚Äì die fehlenden 3%</title>

    <meta name="description" content="Resilient Machine Learning">
    <meta name="author" content="Oliver Zeigermann">
	<link rel="shortcut icon" href="/img/favicon.ico" >

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<!-- <link rel="stylesheet" href="reveal.js/dist/theme/black.css"> -->
	<link rel="stylesheet" href="reveal.js/dist/theme/white.css">

	<!-- Theme used for syntax highlighted code -->
	<!-- <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css"> -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/zenburn.css">
	<style>
		.right-img {
			margin-left: 10px !important;
			float: right;
			height: 500px;
		}

		.todo:before {
			content: 'TODO: ';
		}

		.todo {
			color: red !important;
		}

		code span.line-number {
			color: lightcoral;
		}

		.reveal pre code {
			max-height: 1000px !important;
		}

		img {
			border: 0 !important;
			box-shadow: 0 0 0 0 !important;
			height: 450px;
		}

		.reveal {
			-ms-touch-action: auto !important;
			touch-action: auto !important;
		}

		.reveal h1,
		.reveal h2,
		.reveal h3,
		.reveal h4 {
			/* letter-spacing: 2px; */
			font-family: 'Calibri', sans-serif;
			/* font-family: 'Times New Roman', Times, serif; */
			/* font-weight: bold; */
			color: black;
			/* font-style: italic; */
			/* letter-spacing: -2px; */
			text-transform: none !important;
		}

		.reveal em {
			font-weight: bold;
		}

		.reveal section img {
			background: none;
		}

		.reveal img.with-border {
			border: 1px solid #586e75 !important;
			box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
		}

		.reveal li {
			margin-bottom: 8px;
		}

		/* For li's that use FontAwesome icons as bullet-point */
		.reveal ul.fa-ul li {
			list-style-type: none;
		}

		.reveal {
			/* font-family: 'Work Sans', 'Calibri'; */
			font-family: 'Calibri';
			color: black !important;
			font-size: xx-large;
		}

		.container {
			display: flex;
		}

		.col img {
			height: auto;
		}

		.col,
		col-1 {
			flex: 1;
		}

		.col-2 {
			flex: 2;
		}

	/* https://intranet.openknowledge.de/plugins/servlet/mobile?contentId=18612939#content/view/18612939
      https://intranet.openknowledge.de/pages/viewpage.action?pageId=16157867
      https://openknowledgede.sharepoint.com/:p:/s/slides/EYWbMPz2ejhBprBrP8BYGxsBA3BrPNhDxJBrTXXvHi3-ZQ?e=jMZihi
       */
       /* body:after {
        content: url(img/ok/logo.png) ;
        position: fixed;
        bottom: 80px;
        left: -1080px;
        transform: scale(.06);
        height: 10px;
    } */

	</style>
</head>

<body style="background-color: whitesmoke;">

<div class="reveal">
    <div class="slides">


<!-- 
  ChatGPT und andere Large-Language-Modelle haben in den letzten 12 Monaten atemberaubende Fortschritte gemacht. ChatGPT kann m√ºhelos Fragen beantworten, Hinweise verarbeiten und Antworten in Formaten wie Gedichten, Bibelversen oder Rapsongs wiedergeben. Da stellt sich die Frage:

Ist AGI (artifical general intelligence) jetzt gel√∂st? Oder wartet da nur n√§chste Welle der Ern√ºchterung?

In diesem Talk beleuchten wir die letzten Fortschritte und untersuchen im Detail:

* Was fehlt eigentlich noch?
* Worauf werden die Modelle eigentlich trainiert?
* Und bei allem Fortschritt, was sind die fehlenden 3% um 100% Genauigkeit zu bekommen?
* Und wie lange noch, bis KI die Welt √ºbernimmt?!
-->

<section data-markdown class="todo">
	<textarea data-template>
## Mikio

- ChatGPT √úbersicht im tollen Stil
- Zweiter Teil

</textarea>
</section>

 <section data-markdown>
	<textarea data-template>
# Artificial General Intelligence - die fehlenden 3%

https://www.m3-konferenz.de/veranstaltung-20071-0-artificial-general-intelligence--die-fehlenden-3%25.html, M3 2023, Karlsruhe 

Mikio Braun, Oliver Zeigermann

<img src="img/bit.ly_m3-llm-2023.png" style="height: 200px;">

Slides: https://bit.ly/m3-llm-2023

<!-- https://djcordhose.github.io/ml-resources/2023-llm-m3.html -->

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Umfrage - wie nahe sind wir an der "Artificial General Intelligence"

1. Bereits da
1. Innerhalb der n√§chsten 5 Jahren
1. In diesem Jahrhundert
1. Nie
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
## Gottgleiche KI zerst√∂rt die Welt?

https://t3n.de/news/gottgleiche-ki-zerstoerung-der-menschheit-1547175/
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Bisschen viel Drama, eher...

</textarea>
</section>


<section data-markdown class="fragments">
	<textarea data-template>
<!-- ### Steht AI f√ºr "Alien Intelligence"? -->
### AI steht f√ºr "Alien Intelligence"

* Schachprogramme wie Stockfish spielen nicht √ºbermenschlich, sondern unmenschlich stark
  * nicht wie ein besonders guter Mensch
  * sondern wie eine fremdartige Intelligenz
* bei Schach Grusel ala <a href='https://de.wikipedia.org/wiki/Uncanny_Valley'>Uncanny Valley</a>
* andere Bereiche sind aber sensibler
* autonomes Fahren
  * unvorhersehbare Aktionen
  * andersartige Fehler
* Sprachmodelle
  * fehlende Empathie
  * abweichende oder fehlende Ethik
  * dauerhafter "Mount Stupid"
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Heute konzentrieren wir uns auf Sprachmodelle

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Wer ist Olli

		<div style="display: flex;">
			<div style="flex: 50%;">
				<a href='https://oreilly.de/produkt/machine-learning-kurz-gut-2/'>
				<img src='img/ml-buch-v2.jpg' height="400">
				</a>
			</div>
			<div style="flex: 50%; font-size: x-large;">
				<img src='img/olli-opa.jpeg'>
			</div>
		</div>
		<p>
			<a target="_blank" href="mailto:OliverZeigermann@gmail.com">Oliver Zeigermann</a>:
		Entwickler, Architekt, Berater und Coach f√ºr Machine Learning
		</p>    
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
		### Wer ist Mikio
		<img src='img/mikio-data.jpeg'>

		<a target="_blank" href="mailto:mikiobraun@gmail.com">Mikio Braun</a>:
		Ex-ML Researcher, Architekt, Berater und Mentor f√ºr Machine Learning
	</textarea>
</section>



<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. Wann √ºbernimmt KI die Welt?
	</textarea>
</section>
  
<section data-markdown>
	<textarea data-template>
## Agenda

1. *Idee von Transformern / LLMs*
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. Wann √ºbernimmt KI die Welt?
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Intelligenz auf Sprache reduzieren

_kann man ein Sprach-System trainieren, dass sich intelligent anf√ºhlt?_


	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Aber
## Wie trainiert man das denn?

	</textarea>
</section>

 <section data-markdown class="fragments">
### Probleme beim √ºberwachten Lernen

* linearer Aufwand beim labeln von Daten
* erhebliche Fehlerquote zu erwarten
  * Standard-Datens√§tze enthalten bis zu 10% Fehler
  * https://labelerrors.com/
* Unterschiede zwischen verschiedenen labelnden Personen
* √Ñnderung der Labeldefinition k√∂nnte erfordern, von vorne zu beginnen

_nicht praktikabel bei gro√üen Datens√§tzen_
</section>

 <section data-markdown class="fragments">
### Foundation Models: Transformer Kernideen

1. Basis ist ein verallgemeinertes Sprachmodell
1. Wahrscheinlichkeiten von Wortfolgen vorhersagen
1. Self-Supervision zur Nutzung gro√üer ungelabelter Datens√§tze (auch bekannt als un√ºberwachtes Vortraining)
1. Training auf einem sehr gro√üen Korpus
1. Self-attention zur Kodierung weitreichender Abh√§ngigkeiten
1. zus√§tzliches √ºberwachtes Training f√ºr nachgelagerte Aufgaben, z.B.
    - √úbersetzung (lang1 & lang2 Paare)
    - Beantwortung von Fragen (Q&A-Paare)
    - Stimmungsanalyse (Text-/Stimmungspaare) 
    - usw.
</section>
    
<section data-markdown>
<textarea data-template>
### Transformer Architektur: Encoder/Decoder

<div style="display: flex; align-items: center; justify-content: space-around;">
  <div>
    <ul>
      <li>links: Encoder</li>
      <li>rechts: Decoder</li>
      <li>Encoder speist die Embeddings der Eingabe in den Decoder ein</li>
      <li>Der Decoder ben√∂tigt Kontext / initialen Prompt, um zu funktionieren</li>
    </ul>
</div>
  <div style="width: 100%;">
    <img src='img/transformers/transformer-encoder-decoder.png'>
</div>
</div>
<small>

https://arxiv.org/pdf/1706.03762.pdf
</small>

        </textarea>
</section>

<section data-markdown class="fragments">
### Transformer Zoo

* der urspr√ºngliche Transformer war f√ºr √úbersetzungsaufgaben gedacht
* die Verwendung hat sich seitdem ausgeweitet
* es entstand ein ganzer Zoo von Transformatoren
* einige verwenden nur Encoder
* einige verwenden nur Decoder
* einige verwenden eine Kombination aus Encoder/Decoder, genau wie der urspr√ºngliche Transformator

<!-- https://huggingface.co/transformers/model_summary.html
https://huggingface.co/transformers/#supported-models -->
</section>


<section data-markdown class="fragments">
### Decoder only (GPT-like)

_auch autoregressive Transformer-Modelle genannt_

* der Decoderteil kann gegebene Eingaben in vollst√§ndige S√§tze umwandeln
* ben√∂tigt Kontext / initialen Prompt
* z.B. n√ºtzlich, um 
  * angefangene S√§tze zu vervollst√§ndigen
  * auf Fragen zu reagieren
  * ein Gespr√§ch zu f√ºhren
* erzeugt eine Antwort iterativ ("autoregressiv")
* GPT w√§re ein Beispiel f√ºr diese Art von Anwendung 
  * unidirektional: trainiert, um das n√§chste Wort vorherzusagen
  * von OpenAI

  </section>
    
  <section data-markdown class="fragments">
    <textarea data-template>
  ### Training GPT

* self-supervised Training
* Vorhersage des n√§chsten Wortes anhand der vorherigen W√∂rter
* Kontext (Anzahl der in Betracht gezogenen vorherigen W√∂rter) ist begrenzt
* Unterschiedliche Versionen haben unterschiedlich viel Kontext
  * von 2k - GPT-3 
  * √ºber 4k - GPT-3.5 
  * bis 32k - GPT-4

https://huggingface.co/transformers/model_summary.html#original-gpt
https://platform.openai.com/docs/models
    </textarea>
  </section>

  <section data-markdown class="fragments">
### ChatGPT

* GPT optimiert auf Dialoge
* Basiert auf https://openai.com/research/instruction-following
* zus√§tzliches √ºberwachtes Training
  * was ist eine gute Antwort?
* Reinforcement Learning from Human Feedback (RLHF)
  * PPO
  * Reward kommt weiterem Reward-Modell
  * ebenfalls √ºberwacht trainiert
* das urspr√ºngliche ChaptGPT basiert auf GPT-3.5, Wissenstand Ende 2021
* aktuellste Beta-Versionen basieren auf GPT-4 (allerdings nur mit 8k Kontext)

https://openai.com/blog/chatgpt

  </section>
  
<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. *Was ist daran toll?*
1. Was ist daran nicht so toll?
1. Wann √ºbernimmt KI die Welt?
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### F√§higkeiten Hui / Wissen Pfui

<img src="img/transformers/Wissen-Faehigkeit.jpg" style="height: 600px">

	</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Plugins kompensieren Mangel an Wissen

* Zugriff auf aktuelle Informationen
  * Reisen, Essen
  * Produkte / Shopping
  * Wissen
* Internet Browsing
* Anzapfen andere Datenquellen
* Aktionen
  <!-- * Eink√§ufe
  * Buchungen
  * Prozessabl√§ufe -->
* Integration eigener Plugins √ºber OpenAPI Spec und natursprachliche Informationen   

https://openai.com/blog/chatgpt-plugins
https://platform.openai.com/docs/plugins/getting-started/
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. Was ist daran toll?
1. *Was ist daran nicht so toll?*
1. Wann √ºbernimmt KI die Welt?
	</textarea>
</section>

<section data-markdown class="fragments">
### Don't forget: Transformers are language models

* No abstract reasoning like it is in our brains takes place
* The basis is the expression of thoughts in texts and code, etc.
* That's the way the system is trained
* Whether this is also intelligent is a pointed question
* Turing would probably say it doesn't matter
* One can argue that this system passes his test
  * https://twitter.com/glouppe/status/1438496208343949318
</section>

<section data-markdown class="fragments">
### On the Opportunities and Risks of Foundational Models

* foundational models: trained on broad data at scale and are adaptable to a wide range of downstream tasks
* ML is undergoing a paradigm shift with the rise of these models
* their scale results in new emergent capabilities 
* defects of the foundation model are inherited by all the adapted models downstream
* lack of clear understanding of how they work, when they fail, and what they are even capable of

https://arxiv.org/abs/2108.07258
</section>


<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. *Wann √ºbernimmt KI die Welt?*
	</textarea>
</section>


<section data-markdown>
<textarea data-template>
<img src='img/transformers/gpt-3-2-years.jpg' style="height: 450px;">

<small>

  https://twitter.com/EMostaque/status/1530554442835189761</small>
        </textarea>
</section>
    

<section data-markdown class="fragments">
### Evolution of GPT

GPT: Generative Pre-Trained Transformer

* GPT-1: 2018, 110 million parameters (https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), https://www.youtube.com/watch?v=LOCzBgSV4tQ
* GPT-2: 2019, 1.5 billion parameters (https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), https://www.youtube.com/watch?v=BXv1m9Asl7I
* GPT-3: 2020, 175 billion parameters (https://arxiv.org/abs/2005.14165), https://www.youtube.com/watch?v=wYdKn-X4MhY
* GPT-4: 2023, probably not much larger, but trained on more data and more context (4096 instead of 2048) (https://analyticsindiamag.com/gpt-4-sam-altman-confirms-the-rumours/)

</section>

<section data-markdown class="todo">
### GPT-4 - Das Ende der Fahnenstange?
  
- Gr√∂√üe scheint ausreichend (https://techcrunch.com/2023/04/14/sam-altman-size-of-llms-wont-matter-as-much-moving-forward)
- Aber die Post geht gerade erst an: "We should expect that the field continues to advance rapidly" (https://ourworldindata.org/ai-investments)

https://arxiv.org/abs/2303.08774

</section>

<section data-markdown class="todo">
### Mehr Context === AGI?

```
Alvaro Cintas (@dr_cintas) tweeted at 5:49 pm on Mon, Apr 24, 2023:
This paper is going viral.

Why?

Authors were able to find a way to enable the Recurrent Memory Transformer to retain information across up to 2 million tokens ü§Ø

In simpler words, it can process and remember vast amounts of data, significantly more than before.

Just so you‚Ä¶ https://t.co/b4yammG6vC
(https://twitter.com/dr_cintas/status/1650527281066962947?t=mfLeKAJu348d2z_L26J7aw&s=03) 
```

https://arxiv.org/abs/2304.11062

</section>


<section data-markdown>
  <textarea data-template>
<img src='img/transformers/twitter-fchollet-one-shot.png'>

<small>

  https://twitter.com/fchollet/status/1528069621047128065</small>
        </textarea>
  </section>


  <section data-markdown>
	<textarea data-template>
## Umfrage - wie nahe sind wir an der "Artificial General Intelligence"

1. Bereits da
1. Innerhalb der n√§chsten 5 Jahren
1. In diesem Jahrhundert
1. Nie
</textarea>
</section>
  
<section data-markdown class="fragments todo">
  <textarea data-template>
### Zusammenfassung

1. ChatGPT ist ein auf Dialoge spezialisiertes Sprachmodell
1. Es basiert auf der GPT-Reihe von OpenAI
1. Sprachmodellen fehlen viele wichtige menschliche Eigenschaften 
  * fehlende Empathie
  * abweichende oder fehlende Ethik
  * dauerhafter "Mount Stupid"
  * Lernf√§higkeit

</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Weitere Quellen als Referenz

- https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/
- https://philosophygeek.substack.com/p/wrapping-my-head-around-ai
- Wolfram ChatGPT Plugin Blends Symbolic AI with Generative AI: https://thenewstack.io/wolfram-chatgpt-plugin-blends-symbolic-ai-with-generative-ai/
- Technische Details wenn man es echt ernst meint: https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/
- Engineering: https://huyenchip.com/2023/04/11/llm-engineering.html
- Vicuna als Open Source Alternative
  - https://chat.lmsys.org/
  - https://www.linkedin.com/posts/christoph-henkelmann_fastchat-activity-7050938707561861120-gSJt

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
# Vielen Dank
## Artificial General Intelligence - die fehlenden 3%

Bleibt im Kontakt

Mikio Braun: mailto:mikiobraun@gmail.com

Oliver Zeigermann: oliver@zeigermann.de, @DJCordhose, https://www.linkedin.com/in/oliver-zeigermann-34989773

Slides: https://bit.ly/m3-llm-2023

</textarea>
</section>




</div>
</div>

<script src="reveal.js/dist/reveal.js"></script>
<script src="lib/jquery.js"></script>
<script>
    const printMode = window.location.search.match(/print-pdf/gi);
    const isLocal = window.location.hostname.indexOf('localhost') !== -1 ||
        window.location.hostname.indexOf('127.0.0.1') !== -1;
    const isPresentation = isLocal && !printMode;
    const isPublic = !isPresentation;

    $('.hide').remove();

    if (isPresentation) {
    } else {
        // only applies to public version
        $('.todo').remove();
        $('.preparation').remove();
        $('.local').remove();
    }

    Reveal.addEventListener('ready', function (event) {
        // applies to all versions
        $('code').addClass('line-numbers');

        $('.fragments li').addClass('fragment')

        // make all links open in new tab
        $('a').attr('target', '_blank')

        if (isPresentation) {
            // only applies to presentation version
            Reveal.configure({ controls: false });
        } else {
            // only applies to public version
            $('.fragment').removeClass('fragment');
        }

        // we do not like fragments
        // $('.fragment').removeClass('fragment');

        if (printMode) {
          Reveal.configure({ 
            controls: false,
            // slideNumber: false 
          });
          $('.fragment').removeClass('fragment');
      
        }


    });

</script>

<script src="reveal.js/plugin/notes/notes.js"></script>
<script src="reveal.js/plugin/markdown/markdown.js"></script>
<script src="reveal.js/plugin/highlight/highlight.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,
        controls: true,
        progress: true,
        history: true,
        center: true,
        width: 1100,
        slideNumber: true,
        hideInactiveCursor: false,
        transition: 'none', // none/fade/slide/convex/concave/zoom


        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
    });
</script>


</body>

</html>