<!doctype html>
<html lang="de">

<head>
    <meta charset="utf-8">

    <title>M3 2023 - Artificial General Intelligence – die fehlenden 3%</title>

    <meta name="description" content="Resilient Machine Learning">
    <meta name="author" content="Oliver Zeigermann">
	<link rel="shortcut icon" href="/img/favicon.ico" >

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<!-- <link rel="stylesheet" href="reveal.js/dist/theme/black.css"> -->
	<link rel="stylesheet" href="reveal.js/dist/theme/white.css">

	<!-- Theme used for syntax highlighted code -->
	<!-- <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css"> -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/zenburn.css">
	<style>
		.right-img {
			margin-left: 10px !important;
			float: right;
			height: 500px;
		}

		.todo:before {
			content: 'TODO: ';
		}

		.todo {
			color: red !important;
		}

		code span.line-number {
			color: lightcoral;
		}

		.reveal pre code {
			max-height: 1000px !important;
		}

		img {
			border: 0 !important;
			box-shadow: 0 0 0 0 !important;
			height: 450px;
		}

		.reveal {
			-ms-touch-action: auto !important;
			touch-action: auto !important;
		}

		.reveal h1,
		.reveal h2,
		.reveal h3,
		.reveal h4 {
			/* letter-spacing: 2px; */
			font-family: 'Calibri', sans-serif;
			/* font-family: 'Times New Roman', Times, serif; */
			/* font-weight: bold; */
			color: black;
			/* font-style: italic; */
			/* letter-spacing: -2px; */
			text-transform: none !important;
		}

		.reveal em {
			font-weight: bold;
		}

		.reveal section img {
			background: none;
		}

		.reveal img.with-border {
			border: 1px solid #586e75 !important;
			box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
		}

		.reveal li {
			margin-bottom: 8px;
		}

		/* For li's that use FontAwesome icons as bullet-point */
		.reveal ul.fa-ul li {
			list-style-type: none;
		}

		.reveal {
			/* font-family: 'Work Sans', 'Calibri'; */
			font-family: 'Calibri';
			color: black !important;
			font-size: xx-large;
		}

		.container {
			display: flex;
		}

		.col img {
			height: auto;
		}

		.col,
		col-1 {
			flex: 1;
		}

		.col-2 {
			flex: 2;
		}

	/* https://intranet.openknowledge.de/plugins/servlet/mobile?contentId=18612939#content/view/18612939
      https://intranet.openknowledge.de/pages/viewpage.action?pageId=16157867
      https://openknowledgede.sharepoint.com/:p:/s/slides/EYWbMPz2ejhBprBrP8BYGxsBA3BrPNhDxJBrTXXvHi3-ZQ?e=jMZihi
       */
       /* body:after {
        content: url(img/ok/logo.png) ;
        position: fixed;
        bottom: 80px;
        left: -1080px;
        transform: scale(.06);
        height: 10px;
    } */

	</style>
</head>

<body style="background-color: whitesmoke;">

<div class="reveal">
    <div class="slides">


<!-- 
-->

 <section data-markdown>
	<textarea data-template>
# Artificial General Intelligence - die fehlenden 3%

https://www.m3-konferenz.de/veranstaltung-20071-0-artificial-general-intelligence--die-fehlenden-3%25.html, M3 2023, Karlsruhe 

Mikio Braun

Oliver Zeigermann

Slides: https://bit.ly/m3-llm-2023

<!-- https://djcordhose.github.io/ml-resources/2023-llm-m3.html -->

</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
## Olli

- Erster Teil
  - Material ausdünnen und übersetzen
  - Übergang aus Miro nehmen: Wissen vs Fähigkeit
  - Was können Plugins daran tun?
    - https://openai.com/blog/chatgpt-plugins
  - ganz gut: https://philosophygeek.substack.com/p/wrapping-my-head-around-ai
- Wer ist Olli Folie
- Folie Umfrage Stimmung / Meinung 
  - am Anfang
  - am Ende
  - Vergleich
- Zusammenfassung    
- bit.ly
  
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
## Mikio

- Wer ist Mikio Folie
- ChatGPT Folie
- Zweiter Teil
- Ende-Folie, Kontakt Mikio

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Agenda

1. Idee von Transformern / LLMs
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. Wann übernimmt KI die Welt?
	</textarea>
</section>
  
<section data-markdown>
	<textarea data-template>
## Agenda

1. *Idee von Transformern / LLMs*
1. Was ist daran toll?
1. Was ist daran nicht so toll?
1. Wann übernimmt KI die Welt?
	</textarea>
</section>


 <section data-markdown class="fragments">
### Issues in Supervised Learning

* linear effort in labelling data
* significant error rate to be expected
  * all standard data sets contain up to 10% of errors
  * https://labelerrors.com/
* differences between different labelers
* change in label definition might require to start all over

_impractical with large data sets_
</section>

 <section data-markdown class="fragments">
### Foundation Models: Transformer Core ideas

1. have a generalized language model
1. predict probabilities of sequences of words
1. train on a very large corpus
1. zero- or one-shot learning
1. self-attention for encoding long range dependencies
1. self-supervision for leveraging large unlabeled datasets (aka unsupervised pre-training)
1. additional supervised training for downstream tasks, e.g.
    - translation (lang1 & lang2 pairs)
    - question answering (Q&A pairs)
    - sentiment analysis (text & mood pairs) 
    - etc.
  
  <!-- https://www.youtube.com/watch?v=iFhYwEi03Ew -->
  </section>
    
<section data-markdown class="fragments">
### Transformer Zoo

* the original transformer was meant for translation tasks
* usage has broadened ever since
* spawning a whole zoo of transformers
* some use encoder only
* some use decoder only
* some use a combination of encoder/decoder just like the original transformer 

<!-- https://huggingface.co/transformers/model_summary.html
https://huggingface.co/transformers/#supported-models -->
</section>

<section data-markdown>
<textarea data-template>
### Transformer Architecture: Encoder/Decoder

<div style="display: flex; align-items: center; justify-content: space-around;">
  <div>
    <ul>
      <li>left side is encoder</li>
      <li>right side is decoder</li>
      <li>encoder feeds in embeddings of input into decoder</li>
      <li>decoder needs context to work</li>
    </ul>
</div>
  <div>
    <img src='img/transformers/transformer-encoder-decoder.png' >
</div>
</div>
<small>

https://arxiv.org/pdf/1706.03762.pdf
</small>

        </textarea>
</section>

<section data-markdown class="fragments">
### Decoder only (GPT-like)

_also called auto-regressive Transformer models_

* the decoder part can transform given inputs into complete sentences
* e.g. useful in itself, to complete started sentences
* generates a response iteratively ("auto regressive")
* GPT would be an example for this kind of application 
  * unidirectional: trained to predict next word
  * by OpenAI 
  
  </section>
    
  <section data-markdown>
  ### Training GPT
  
  * self-supervised training
  * predict the next word, given all of the previous words within some text
  * has a limited context
  
  https://huggingface.co/transformers/model_summary.html#original-gpt
  https://huggingface.co/transformers/model_doc/gpt2.html
  </section>
  
<section data-markdown>
<textarea data-template>
<img src='img/transformers/gpt-3-2-years.jpg' style="height: 450px;">

<small>

  https://twitter.com/EMostaque/status/1530554442835189761</small>
        </textarea>
</section>
    

<section data-markdown class="fragments">
### Evolution of GPT

GPT: Generative Pre-Trained Transformer

* GPT-1: 2018, 110 million parameters (https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), https://www.youtube.com/watch?v=LOCzBgSV4tQ
* GPT-2: 2019, 1.5 billion parameters (https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), https://www.youtube.com/watch?v=BXv1m9Asl7I
* GPT-3: 2020, 175 billion parameters (https://arxiv.org/abs/2005.14165), https://www.youtube.com/watch?v=wYdKn-X4MhY
* GPT-4: 2022, probably not much larger, but trained on more data and more context (4096 instead of 2048) (https://analyticsindiamag.com/gpt-4-sam-altman-confirms-the-rumours/)

</section>
  
<section data-markdown>
<textarea data-template>
### Typical example for decoder only GPT: completing a text (zero-shot)

<img src='img/transformers/gpt-3-article.png' style="height: 450px;">

<small>

https://arxiv.org/abs/2005.14165</small>
        </textarea>
</section>
    
<section data-markdown>
  <textarea data-template>
<img src='img/transformers/twitter-fchollet-one-shot.png'>

<small>

  https://twitter.com/fchollet/status/1528069621047128065</small>
        </textarea>
  </section>
  
<section data-markdown>
<textarea data-template>
### Completion (One-Shot / Few-Shot)

<img src='img/transformers/gpt-few-shot.png' style="height: 450px;">

<small>

https://arxiv.org/abs/2005.14165</small>
        </textarea>
</section>

<section data-markdown>
<textarea data-template>
### Interactive Teaching

<img src='img/transformers/gpt-math.jpg' style="height: 450px;">

<small>

  https://twitter.com/peterwildeford/status/1522633978305560576
  <br>
  https://twitter.com/kaushikpatnaik/status/1522794898805592066
</small>
        </textarea>
</section>

<section data-markdown class="fragments">
### Don't forget: Transformers are language models

* No abstract reasoning like it is in our brains takes place
* The basis is the expression of thoughts in texts and code, etc.
* That's the way the system is trained
* Whether this is also intelligent is a pointed question
* Turing would probably say it doesn't matter
* One can argue that this system passes his test
  * https://twitter.com/glouppe/status/1438496208343949318
</section>

<section data-markdown class="fragments">
### On the Opportunities and Risks of Foundational Models

* foundational models: trained on broad data at scale and are adaptable to a wide range of downstream tasks
* ML is undergoing a paradigm shift with the rise of these models
* their scale results in new emergent capabilities 
* defects of the foundation model are inherited by all the adapted models downstream
* lack of clear understanding of how they work, when they fail, and what they are even capable of

https://arxiv.org/abs/2108.07258
</section>

<!-- <section data-markdown>
### Why it might make sense to study transformers even when you are not into NLP
  
So even though I'm technically in vision, papers, people and ideas across all of AI are suddenly extremely relevant. 
Everyone is working with essentially the same model, so most improvements and ideas can "copy paste" rapidly across all of AI.

https://twitter.com/karpathy/status/1468370611797852161 
</section> -->



<section data-markdown>
	<textarea data-template>
# Vielen Dank
## Artificial General Intelligence - die fehlenden 3%

Bleibt im Kontakt

https://www.linkedin.com/in/oliver-zeigermann-34989773/

oliver@zeigermann.de

Twitter: @DJCordhose

Slides: https://bit.ly/m3-llm-2023

</textarea>
</section>




</div>
</div>

<script src="reveal.js/dist/reveal.js"></script>
<script src="lib/jquery.js"></script>
<script>
    const printMode = window.location.search.match(/print-pdf/gi);
    const isLocal = window.location.hostname.indexOf('localhost') !== -1 ||
        window.location.hostname.indexOf('127.0.0.1') !== -1;
    const isPresentation = isLocal && !printMode;
    const isPublic = !isPresentation;

    $('.hide').remove();

    if (isPresentation) {
    } else {
        // only applies to public version
        $('.todo').remove();
        $('.preparation').remove();
        $('.local').remove();
    }

    Reveal.addEventListener('ready', function (event) {
        // applies to all versions
        $('code').addClass('line-numbers');

        $('.fragments li').addClass('fragment')

        // make all links open in new tab
        $('a').attr('target', '_blank')

        if (isPresentation) {
            // only applies to presentation version
            Reveal.configure({ controls: false });
        } else {
            // only applies to public version
            $('.fragment').removeClass('fragment');
        }

        // we do not like fragments
        // $('.fragment').removeClass('fragment');

        if (printMode) {
          Reveal.configure({ 
            controls: false,
            // slideNumber: false 
          });
          $('.fragment').removeClass('fragment');
      
        }


    });

</script>

<script src="reveal.js/plugin/notes/notes.js"></script>
<script src="reveal.js/plugin/markdown/markdown.js"></script>
<script src="reveal.js/plugin/highlight/highlight.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,
        controls: true,
        progress: true,
        history: true,
        center: true,
        width: 1100,
        slideNumber: true,
        hideInactiveCursor: false,
        transition: 'none', // none/fade/slide/convex/concave/zoom


        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
    });
</script>


</body>

</html>