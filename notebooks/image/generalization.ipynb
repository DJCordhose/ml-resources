{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DJCordhose/ml-resources/blob/master/notebooks/image/intro_quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdfXfXK2-CiG"
   },
   "source": [
    "# The Quest for Generalization\n",
    "\n",
    "https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (20, 8)\n",
    "mpl.rcParams['axes.titlesize'] = 24\n",
    "mpl.rcParams['axes.labelsize'] = 20\n",
    "\n",
    "figsize_3d = (12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j5uomaug1WV-",
    "outputId": "b37dac79-5a66-4953-aed1-549294f6bf85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 07:28:37.198034: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-10 07:28:37.342914: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-10 07:28:37.342930: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e5zKBVtG2iP",
    "outputId": "ae8b9ee3-58ad-497b-d455-bdc740a81d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeJD3yOPZKD5",
    "outputId": "487ec351-403a-42a3-f128-13efc0f7f736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKLirrM6HaOO",
    "outputId": "289e5620-674f-459b-8fb8-f6614d5eceaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 07:28:41.422441: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-10 07:28:41.422500: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-10 07:28:41.422520: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-BEN73DP): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "F5vgp3XnHxtP"
   },
   "outputs": [],
   "source": [
    "# if we do not want GPU\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def predict(model, img_path):\n",
    "#     img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.load_img(img_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    preds = model.predict(x)\n",
    "    # decode the results into a list of tuples (class, description, probability)\n",
    "    # (one such list for each sample in the batch)\n",
    "    print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "    # https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
    "    # 335 fox squirrel, eastern fox squirrel, Sciurus niger\n",
    "    print(preds.argmax(), preds.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dchjxUQnV_pV",
    "outputId": "00804e11-71bb-43f6-ed7f-405ee3392c13"
   },
   "outputs": [],
   "source": [
    "# https://keras.io/api/applications/\n",
    "# from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "# from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "# mobilnetv2_model = MobileNetV2(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# from tensorflow.keras.applications.resnet import ResNet50\n",
    "# from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions\n",
    "# resenet50_model = ResNet50(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# https://keras.io/api/applications/resnet/#resnet50v2-function\n",
    "# https://arxiv.org/abs/1603.05027\n",
    "# ... makes training easier and improves generalization\n",
    "# from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "# from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
    "# resenet50V2_model = ResNet50V2(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# https://keras.io/api/applications/xception/\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input, decode_predictions\n",
    "xception_model = Xception(weights='imagenet', input_shape=(299, 299, 3))\n",
    "\n",
    "# model = resenet50_model\n",
    "# model = resenet50V2_model\n",
    "model = xception_model\n",
    "\n",
    "# model.summary()\n",
    "# len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\t\t\t\t\t\t       berkeley4.jpg\r\n",
      "Black_New_York_stuy_town_squirrel_amanda_ernlund.jpeg  boston-vrx.JPG\r\n",
      "Michigan-MSU-raschka.jpg\t\t\t       london.jpg\r\n",
      "austin1.jpg\t\t\t\t\t       monster\r\n",
      "austin2.jpg\t\t\t\t\t       san_francisco_1.jpg\r\n",
      "austin3.jpg\t\t\t\t\t       san_francisco_2.jpg\r\n",
      "austin4.jpg\t\t\t\t\t       train\r\n",
      "austin5.jpg\t\t\t\t\t       validation\r\n",
      "berkeley1.jpg\t\t\t\t\t       yosemite.jpg\r\n",
      "berkeley2.jpg\t\t\t\t\t       yosemite2.jpg\r\n",
      "berkeley3.jpg\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/squirrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austin-blurred.jpg  austin-close.jpg\t austin-shadow.jpg  austin-tree.jpg\r\n",
      "austin-bum.jpg\t    austin-drinking.jpg  austin-socket.jpg  through-legs.jpg\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/squirrels/2022/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = '../../data/squirrels/austin1.jpg'\n",
    "# img_path = '../../data/squirrels/yosemite.jpg'\n",
    "# img_path = '../../data/squirrels/comic.jpg'\n",
    "# img_path = '../../data/squirrels/emoji.png'\n",
    "\n",
    "\n",
    "img_path = '../../data/squirrels/Black_New_York_stuy_town_squirrel_amanda_ernlund.jpeg'\n",
    "img_path = '../../data/squirrels/london.jpg'\n",
    "\n",
    "img_path = '../../data/squirrels/2022/austin-close.jpg'\n",
    "img_path = '../../data/squirrels/2022/austin-blurred.jpg'\n",
    "img_path = '../../data/squirrels/2022/austin-drinking.jpg' \n",
    "img_path = '../../data/squirrels/2022/austin-bum.jpg'\n",
    "# img_path = '../../data/squirrels/2022/squirrel/austin-shadow.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<img src=../../data/squirrels/2022/austin-bum.jpg>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "md(f\"<img src={img_path}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 557ms/step\n",
      "Predicted: [('n02356798', 'fox_squirrel', 0.9699071), ('n02326432', 'hare', 0.003918362), ('n02325366', 'wood_rabbit', 0.001425633)]\n",
      "335 0.9699071\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<img src=../../data/squirrels/validation/cat-bonkers.png>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_img_path = '../../data/squirrels/validation/cat-bonkers.png'\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "md(f\"<img src={cat_img_path}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Ey2ZB23Dx6c",
    "outputId": "59f18053-8eab-4713-90e0-3d86f302b1a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 147ms/step\n",
      "Predicted: [('n02124075', 'Egyptian_cat', 0.9397457), ('n02123159', 'tiger_cat', 0.019086512), ('n02123045', 'tabby', 0.010862169)]\n",
      "285 0.9397457\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = cat_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kRWqxkDDwHn"
   },
   "source": [
    "# Re-Training with Austin Squirrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.preprocessing.image_dataset_from_directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 files belonging to 1 classes.\n",
      "Found 7 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "x_size = 299\n",
    "y_size = 299\n",
    "image_size = (y_size, x_size)\n",
    "batch_size = 32\n",
    "seed=1337\n",
    "\n",
    "class_names=['335', '285']\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"../../data/squirrels/train\",\n",
    "#     validation_split=0.2,\n",
    "#     subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    labels='inferred'\n",
    "#     labels=7 * [335]\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"../../data/squirrels/validation\",\n",
    "    seed=seed,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    labels='inferred',\n",
    "    class_names=class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([10, 299, 299, 3]), TensorShape([10]))"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_ds))\n",
    "image_batch.shape, label_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([299, 299, 3])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = image_batch[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def transform_label(labels):\n",
    "    transformed = [335, 295, 485]\n",
    "    for label in labels:\n",
    "        new_label = tf.cond(label == 1,  lambda: 285, lambda: 335)\n",
    "#         if label == 20:\n",
    "#             transformed.append(285)\n",
    "#         else:\n",
    "#             transformed.append(335)\n",
    "        transformed.append(new_label)\n",
    "    print(transformed)\n",
    "    return tf.convert_to_tensor(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([42, 45, 45], dtype=int32)>"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor([42, 45, 45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[335, 295, 485, <tf.Tensor 'while/cond/Identity:0' shape=() dtype=int32>]\n"
     ]
    },
    {
     "ename": "InaccessibleTensorError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_2424/419032334.py\", line 1, in None  *\n        lambda image, label: (image, transform_label(label))\n    File \"/tmp/ipykernel_2424/3815858995.py\", line 12, in transform_label  *\n        return tf.convert_to_tensor(transformed)\n\n    InaccessibleTensorError: <tf.Tensor 'while/cond/Identity:0' shape=() dtype=int32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'while/cond/Identity:0' shape=() dtype=int32> was defined here:\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n          return _run_code(code, main_globals, None,\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/runpy.py\", line 86, in _run_code\n          exec(code, run_globals)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel_launcher.py\", line 16, in <module>\n          app.launch_new_instance()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n          app.start()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 677, in start\n          self.io_loop.start()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n          self.asyncio_loop.run_forever()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n          self._run_once()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n          handle._run()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/events.py\", line 80, in _run\n          self._context.run(self._callback, *self._args)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n          await self.process_one()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n          await dispatch(*args)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n          await result\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n          reply_content = await reply_content\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n          res = shell.run_cell(code, store_history=store_history, silent=silent)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n          return super().run_cell(*args, **kwargs)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2880, in run_cell\n          result = self._run_cell(\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2935, in _run_cell\n          return runner(coro)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n          coro.send(None)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3134, in run_cell_async\n          has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3337, in run_ast_nodes\n          if await self.run_code(code, result, async_=asy):\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3397, in run_code\n          exec(code_obj, self.user_global_ns, self.user_ns)\n        File \"/tmp/ipykernel_2424/419032334.py\", line 1, in <cell line: 1>\n          normalized_train_ds = train_ds.map(lambda image, label: (image, transform_label(label)))\n        File \"/tmp/ipykernel_2424/419032334.py\", line 1, in \n          normalized_train_ds = train_ds.map(lambda image, label: (image, transform_label(label)))\n        File \"/tmp/ipykernel_2424/419032334.py\", line 1, in \n          normalized_train_ds = train_ds.map(lambda image, label: (image, transform_label(label)))\n        File \"/tmp/ipykernel_2424/3815858995.py\", line 4, in transform_label\n          for label in labels:\n        File \"/tmp/ipykernel_2424/3815858995.py\", line 5, in transform_label\n          new_label = tf.cond(label == 1,  lambda: 285, lambda: 335)\n    \n    The tensor <tf.Tensor 'while/cond/Identity:0' shape=() dtype=int32> cannot be accessed from FuncGraph(name=transform_label, id=140360595294816), because it was defined in FuncGraph(name=while_body_44768, id=140360593725984), which is out of scope.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [554]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m normalized_train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m normalized_val_ds \u001b[38;5;241m=\u001b[39m val_ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m image, label: (image, transform_label(label)))\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2048\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2045\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DEBUG_MODE:\n\u001b[1;32m   2046\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2047\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2048\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[1;32m   2051\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2052\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2055\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2056\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5243\u001b[0m, in \u001b[0;36mMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m   5242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[0;32m-> 5243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5247\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m   5249\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmap_dataset(\n\u001b[1;32m   5250\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   5251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5254\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m   5255\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    265\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2567\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2559\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m   2560\u001b[0m \n\u001b[1;32m   2561\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;124;03m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2567\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2569\u001b[0m   graph_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   2570\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2533\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 2533\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2534\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   2535\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   2536\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2711\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m   cache_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mgeneralize(cache_key)\n\u001b[1;32m   2709\u001b[0m   (args, kwargs) \u001b[38;5;241m=\u001b[39m cache_key\u001b[38;5;241m.\u001b[39m_placeholder_value()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 2711\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                          graph_function)\n\u001b[1;32m   2715\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2627\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2622\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2623\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   2624\u001b[0m ]\n\u001b[1;32m   2625\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m   2626\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 2627\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m   2638\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m   2639\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   2640\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   2641\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   2642\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1141\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1139\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1141\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m   1146\u001b[0m     convert, func_outputs, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:248\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;129m@eager_function\u001b[39m\u001b[38;5;241m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    243\u001b[0m     input_signature\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_structure),\n\u001b[1;32m    245\u001b[0m     autograph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:177\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    176\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 177\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n\u001b[1;32m    179\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ret)\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_2424/419032334.py\", line 1, in None  *\n        lambda image, label: (image, transform_label(label))\n    File \"/tmp/ipykernel_2424/3815858995.py\", line 12, in transform_label  *\n        return tf.convert_to_tensor(transformed)\n\n    InaccessibleTensorError: <tf.Tensor 'while/cond/Identity:0' shape=() dtype=int32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'while/cond/Identity:0' shape=() dtype=int32> was defined here:\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n          return _run_code(code, main_globals, None,\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/runpy.py\", line 86, in _run_code\n          exec(code, run_globals)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel_launcher.py\", line 16, in <module>\n          app.launch_new_instance()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n          app.start()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 677, in start\n          self.io_loop.start()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n          self.asyncio_loop.run_forever()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n          self._run_once()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n          handle._run()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/events.py\", line 80, in _run\n          self._context.run(self._callback, *self._args)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n          await self.process_one()\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n          await dispatch(*args)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n          await result\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n          reply_content = await reply_content\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n          res = shell.run_cell(code, store_history=store_history, silent=silent)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n          return super().run_cell(*args, **kwargs)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2880, in run_cell\n          result = self._run_cell(\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2935, in _run_cell\n          return runner(coro)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n          coro.send(None)\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3134, in run_cell_async\n          has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3337, in run_ast_nodes\n          if await self.run_code(code, result, async_=asy):\n        File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3397, in run_code\n          exec(code_obj, self.user_global_ns, self.user_ns)\n        File \"/tmp/ipykernel_2424/419032334.py\", line 1, in <cell line: 1>\n          normalized_train_ds = train_ds.map(lambda image, label: (image, transform_label(label)))\n        File \"/tmp/ipykernel_2424/419032334.py\", line 1, in \n          normalized_train_ds = train_ds.map(lambda image, label: (image, transform_label(label)))\n        File \"/tmp/ipykernel_2424/419032334.py\", line 1, in \n          normalized_train_ds = train_ds.map(lambda image, label: (image, transform_label(label)))\n        File \"/tmp/ipykernel_2424/3815858995.py\", line 4, in transform_label\n          for label in labels:\n        File \"/tmp/ipykernel_2424/3815858995.py\", line 5, in transform_label\n          new_label = tf.cond(label == 1,  lambda: 285, lambda: 335)\n    \n    The tensor <tf.Tensor 'while/cond/Identity:0' shape=() dtype=int32> cannot be accessed from FuncGraph(name=transform_label, id=140360595294816), because it was defined in FuncGraph(name=while_body_44768, id=140360593725984), which is out of scope.\n"
     ]
    }
   ],
   "source": [
    "normalized_train_ds = train_ds.map(lambda image, label: (image, transform_label(label)))\n",
    "normalized_val_ds = val_ds.map(lambda image, label: (image, transform_label(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), 0.0, 255.0)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(normalized_train_ds))\n",
    "image_np = image_batch[0].numpy()\n",
    "image_np.dtype, np.min(image_np), np.max(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([335, 295, 485, 285, 335], dtype=int32)>"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lrcurve import KerasLearningCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2424/2432564219.py\", line 1, in <cell line: 1>\n      get_ipython().run_cell_magic('time', '', \"\\n# learning_rate = 0.001\\n# learning_rate = 0.0001\\n# epochs = 5\\n# learning_rate = 0.00005\\n\\n# to even recognize shadow, probably overfit in that case\\n# epochs = 50\\nepochs = 10\\nlearning_rate = 0.00002\\n\\n# epochs = 50\\n# learning_rate = 0.00001\\n\\nmodel.compile(loss='sparse_categorical_crossentropy',\\n             optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\\n             metrics=['accuracy'])\\nhistory = model.fit(\\n    normalized_train_ds, \\n    validation_data=normalized_val_ds,\\n    epochs=epochs,\\n    callbacks=[KerasLearningCurve()],\\n    verbose=0)\\n\")\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2357, in run_cell_magic\n      result = fn(*args, **kwargs)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/magics/execution.py\", line 1316, in time\n      exec(code, glob, local_ns)\n    File \"<timed exec>\", line 17, in <module>\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1445, in fit\n      val_logs = self.evaluate(\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1756, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1557, in test_function\n      return step_function(self, iterator)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1546, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1535, in run_step\n      outputs = model.test_step(data)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1501, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 948, in compute_loss\n      return self.compiled_loss(\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/losses.py\", line 139, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/losses.py\", line 243, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/losses.py\", line 1860, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/backend.py\", line 5238, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [1,1000] and labels shape [2]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_test_function_43601]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2424/2432564219.py\", line 1, in <cell line: 1>\n      get_ipython().run_cell_magic('time', '', \"\\n# learning_rate = 0.001\\n# learning_rate = 0.0001\\n# epochs = 5\\n# learning_rate = 0.00005\\n\\n# to even recognize shadow, probably overfit in that case\\n# epochs = 50\\nepochs = 10\\nlearning_rate = 0.00002\\n\\n# epochs = 50\\n# learning_rate = 0.00001\\n\\nmodel.compile(loss='sparse_categorical_crossentropy',\\n             optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\\n             metrics=['accuracy'])\\nhistory = model.fit(\\n    normalized_train_ds, \\n    validation_data=normalized_val_ds,\\n    epochs=epochs,\\n    callbacks=[KerasLearningCurve()],\\n    verbose=0)\\n\")\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2357, in run_cell_magic\n      result = fn(*args, **kwargs)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/IPython/core/magics/execution.py\", line 1316, in time\n      exec(code, glob, local_ns)\n    File \"<timed exec>\", line 17, in <module>\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1445, in fit\n      val_logs = self.evaluate(\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1756, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1557, in test_function\n      return step_function(self, iterator)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1546, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1535, in run_step\n      outputs = model.test_step(data)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 1501, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/training.py\", line 948, in compute_loss\n      return self.compiled_loss(\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/losses.py\", line 139, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/losses.py\", line 243, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/losses.py\", line 1860, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/olli/anaconda3/envs/image-recognition/lib/python3.10/site-packages/keras/backend.py\", line 5238, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [1,1000] and labels shape [2]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_test_function_43601]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# learning_rate = 0.001\n",
    "# learning_rate = 0.0001\n",
    "# epochs = 5\n",
    "# learning_rate = 0.00005\n",
    "\n",
    "# to even recognize shadow, probably overfit in that case\n",
    "# epochs = 50\n",
    "epochs = 10\n",
    "learning_rate = 0.00002\n",
    "\n",
    "# epochs = 50\n",
    "# learning_rate = 0.00001\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    normalized_train_ds, \n",
    "    validation_data=normalized_val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[KerasLearningCurve()],\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../../data/squirrels/2022/squirrel/austin-close.jpg'\n",
    "# img_path = '../../data/squirrels/2022/squirrel/austin-blurred.jpg'\n",
    "img_path = '../../data/squirrels/2022/squirrel/austin-drinking.jpg' \n",
    "# img_path = '../../data/squirrels/2022/squirrel/austin-bum.jpg'\n",
    "img_path = '../../data/squirrels/2022/squirrel/austin-shadow.jpg'\n",
    "\n",
    "# img_path = '../../data/squirrels/Black_New_York_stuy_town_squirrel_amanda_ernlund.jpeg'\n",
    "# img_path = '../../data/squirrels/yosemite.jpg'\n",
    "# img_path = '../../data/squirrels/comic.jpg'\n",
    "img_path = '../../data/squirrels/emoji.png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<img src=../../data/squirrels/emoji.png>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "md(f\"<img src={img_path}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 129ms/step\n",
      "Predicted: [('n02356798', 'fox_squirrel', 0.045340814), ('n04023962', 'punching_bag', 0.042812075), ('n03825788', 'nipple', 0.038255494)]\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step\n",
      "Predicted: [('n02124075', 'Egyptian_cat', 0.9539637), ('n02123159', 'tiger_cat', 0.0131202405), ('n02123045', 'tabby', 0.008195268)]\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = cat_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n",
      "Predicted: [('n02101388', 'Brittany_spaniel', 0.6246768), ('n02088364', 'beagle', 0.12675302), ('n02091244', 'Ibizan_hound', 0.029489962)]\n"
     ]
    }
   ],
   "source": [
    "dog_img_path = '440px-Beagle_Upsy.jpg'\n",
    "predict(model = model, img_path = dog_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<img src=emoji-beaver.png>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beaver_img_path = 'emoji-beaver.png'\n",
    "md(f\"<img src={beaver_img_path}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 130ms/step\n",
      "Predicted: [('n02356798', 'fox_squirrel', 0.14552607), ('n02326432', 'hare', 0.07104529), ('n02342885', 'hamster', 0.019014493)]\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = beaver_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<img src=emoji-skunk.png>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skunk_img_path = 'emoji-skunk.png'\n",
    "md(f\"<img src={skunk_img_path}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 130ms/step\n",
      "Predicted: [('n03935335', 'piggy_bank', 0.042116493), ('n03840681', 'ocarina', 0.037744988), ('n03623198', 'knee_pad', 0.028789936)]\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = skunk_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<img src=emoji-rat.png>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = 'emoji-rat.png'\n",
    "md(f\"<img src={img_path}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n",
      "Predicted: [('n03825788', 'nipple', 0.22290152), ('n03476684', 'hair_slide', 0.07337397), ('n03720891', 'maraca', 0.03637218)]\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<img src=emoji-flamingo.png>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = 'emoji-flamingo.png'\n",
    "md(f\"<img src={img_path}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 117ms/step\n",
      "Predicted: [('n02007558', 'flamingo', 0.16787083), ('n12057211', \"yellow_lady's_slipper\", 0.024868976), ('n03532672', 'hook', 0.014446956)]\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<img src=emoji-hedgehog.png>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = 'emoji-hedgehog.png'\n",
    "md(f\"<img src={img_path}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 114ms/step\n",
      "Predicted: [('n02442845', 'mink', 0.1493138), ('n02346627', 'porcupine', 0.08120539), ('n02356798', 'fox_squirrel', 0.07843033)]\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<img src=emoji-swan.png>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = 'emoji-swan.png'\n",
    "md(f\"<img src={img_path}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step\n",
      "Predicted: [('n03825788', 'nipple', 0.09103938), ('n01860187', 'black_swan', 0.032710977), ('n03047690', 'clog', 0.02791088)]\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<img src=emoji-hare.png>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = 'emoji-hare.png'\n",
    "md(f\"<img src={img_path}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "Predicted: [('n02326432', 'hare', 0.52887), ('n02325366', 'wood_rabbit', 0.1475545), ('n02328150', 'Angora', 0.055031)]\n"
     ]
    }
   ],
   "source": [
    "predict(model = model, img_path = img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "intro_quickstart.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f8727d1aeaf26a81d95912e59b1440fe6468a739851dedadc2d8f821b9e4880"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
