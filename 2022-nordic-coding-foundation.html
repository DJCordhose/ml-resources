<!doctype html>
<html lang="de">

<head>
    <meta charset="utf-8">

    <title>Nordic-Coding - Foundation Models</title>

    <link rel="stylesheet" href="reveal.js/dist/reset.css">
    <link rel="stylesheet" href="reveal.js/dist/reveal.css">
    <!-- <link rel="stylesheet" href="reveal.js/dist/theme/black.css"> -->
    <link rel="stylesheet" href="reveal.js/dist/theme/white.css">
  
    <!-- Theme used for syntax highlighted code -->
    <!-- <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css"> -->
    <link rel="stylesheet" href="reveal.js/plugin/highlight/zenburn.css">
    <style>
      .right-img {
        margin-left: 10px !important;
        float: right;
        height: 500px;
      }
  
      .todo:before {
        content: 'TODO: ';
      }
  
      .todo {
        color: red !important;
      }
  
      code span.line-number {
        color: lightcoral;
      }
  
      .reveal pre code {
        max-height: 1000px !important;
      }
  
      img {
        border: 0 !important;
        box-shadow: 0 0 0 0 !important;
        height: 450px;
      }
  
      .reveal {
        -ms-touch-action: auto !important;
        touch-action: auto !important;
      }
  
      .reveal h1,
      .reveal h2,
      .reveal h3,
      .reveal h4 {
        /* letter-spacing: 2px; */
        font-family: 'Calibri', sans-serif;
        /* font-family: 'Times New Roman', Times, serif; */
        /* font-weight: bold; */
        color: black;
        /* font-style: italic; */
        /* letter-spacing: -2px; */
        text-transform: none !important;
      }
  
      .reveal em {
        font-weight: bold;
      }
  
      .reveal section img {
        background: none;
      }
  
      .reveal img.with-border {
        border: 1px solid #586e75 !important;
        box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
      }
  
      .reveal li {
        margin-bottom: 8px;
      }
  
      /* For li's that use FontAwesome icons as bullet-point */
      .reveal ul.fa-ul li {
        list-style-type: none;
      }
  
      .reveal {
        /* font-family: 'Work Sans', 'Calibri'; */
        font-family: 'Calibri';
        color: black !important;
        font-size: xx-large;
      }
  
      .container {
        display: flex;
      }
  
      .col,
      col-1 {
        flex: 1;
      }
  
      .col-2 {
        flex: 2;
      }

      /* https://intranet.openknowledge.de/plugins/servlet/mobile?contentId=18612939#content/view/18612939
      https://intranet.openknowledge.de/pages/viewpage.action?pageId=16157867
      https://openknowledgede.sharepoint.com/:p:/s/slides/EYWbMPz2ejhBprBrP8BYGxsBA3BrPNhDxJBrTXXvHi3-ZQ?e=jMZihi
       */
       body:after {
        content: url(img/on/logo.png) ;
        position: fixed;
        bottom: 80px;
        left: -1080px;
        transform: scale(.06);
        height: 10px;
    }

    </style>
  
  </head>
  
  <body style="background-color: whitesmoke;">
    <div class="reveal">
      <div class="slides">


<!-- 

Nordic Coding

Paradigmenwechsel im Bereich Machine Learning - Sprachmodelle und mehr

Sogenannte Foundation Models werden auf einer breiten Datenbasis in großem Umfang trainiert und können an eine Vielzahl
von nachgelagerten, spezialisierten Aufgaben angepasst werden. Der Ansatz von Machine Learning durchläuft mit dem
Aufkommen dieser Modelle einen Paradigmenwechsel, da diese Modelle ganz neue Fähigkeiten haben und auf eine ganz neue
Art direkt trainiert werden können. Beispiele sind die Familie der Transformer-Modelle wie GPT und dem abgeleiteten
Codex-Modell, das den Ansatz der Programmierung grundlegend verändern könnte. 

In diesem Talk gebe ich einen Überblick
über diese Transformer-Modelle und zeige am konkreten Codebeispiel was mit vortrainierten Modellen möglich ist und wie
man solche Modelle auf spezielle Aufgaben nachtrainiert. Neben dem Codex-Modell werden wir Beispiele mit dem Copilot von
GitHub betrachten und diskutieren, welche Auswirkungen solche Modelle auf unsere zukünftige Arbeit als
Softwareentwickler haben können.

-->


  <section data-markdown class="local hide preparation">
    <textarea data-template>
### Vorbereitung

Codex
* einfaches Beispiel zeigen als 2. Demo?

Copilot
* Predict.predictFromServer
* alles löschen nach
  * var responseDTO = new Gson().fromJson(response.body(), ResponseDTO.class);
* von da Code Completion ctrl+return gibt weitere Vorschläge
* java\app\src\main\java\eu\zeigermann\ml\App.java
  // Python
  // model.predict(input_matrix)
  // Java Code
* Prompt irgendwo
  // calculate the intersection of two boxes

  
    </textarea>
  </section>

<!-- <section data-markdown>
### Flamingo

* https://twitter.com/laurentsifre/status/1523942063821455360
* https://twitter.com/gordic_aleksa/status/1524083015420465154
* https://twitter.com/risi1979/status/1523638082985099266
</section>
 -->

<!-- <section data-markdown class="todo">
  <textarea data-template>
Xavier Amatriain 🇺🇦 (@xamat) twitterte um 7:05 AM on Sa., März 26, 2022:
"Transformers models: an introduction and catalogue — 2022 Edition" - I was looking for a similar post, and could not find it, so I decided to write it. I hope it is useful to some of you.

https://t.co/YTHgGUgi5I
(https://twitter.com/xamat/status/1507599728174768133?s=03) 
</textarea>
</section> -->


<section data-markdown>
    <textarea data-template>
# Paradigmenwechsel im Bereich Machine Learning - Sprachmodelle und mehr

Nordic Coding

Oliver Zeigermann
oliver.zeigermann@openknowledge.de

Diese Folien: https://bit.ly/nordic-ml-2022
</textarea>

</section>

<section data-markdown>
  <textarea data-template>
### Wer ist Olli

<div style="display: flex;">
<div style="flex: 50%;">
  <a href='https://oreilly.de/produkt/machine-learning-kurz-gut-2/'>
  <img src='img/ml-buch-v2.jpg' height="400">
  </a>
</div>
<div style="flex: 50%; font-size: x-large;">
  <img src='img/olli-opa.jpeg'>
</div>
</div>
<p>
<a target="_blank" href="mailto:oliver.zeigermann@openknowledge.de">Oliver Zeigermann</a>:
Head of AI@OpenKnowledge
</p>    
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Demo Github Copilot - Autocempletion on Steroids

<img src='img/transformers/copilot.png' style="height: 100%;">

<small>

* https://copilot.github.com/
* https://github.com/github/copilot-docs/tree/main/docs
* https://github.com/github/copilot-docs/blob/main/docs/visualstudiocode/gettingstarted.md#getting-started-with-github-copilot-in-visual-studio-code

</small>

</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### What else can Copilot do?

* Convert comments to code
* Autofill for repetitive code
* Generate Tests

https://copilot.github.com/

</textarea>
</section>


  
<section data-markdown>
### More Examples for AI based completion

* https://docs.warp.dev/features/ai-command-search
  * https://youtu.be/kSXpwOElFY0?t=113
* OpenAI Codex (basis for Copilot)
  * https://beta.openai.com/codex-javascript-sandbox
  * OpenAI Codex and GitHub Copilot are both models trained on the GPT-3 language prediction model created by OpenAI.
  However, while Copilot writes code alongside you in your text editor (as the name suggests), Codex requires that you
  access it via their API, or Playground.
    * https://aidan-tilgner.medium.com/github-copilot-vs-openai-codex-which-should-you-use-ed67e53e00c0
</section>

<section data-markdown>
### How is this possible?

* Tackle this with Machine Learning
* Model has basically been been trained on all of the Internet including Github
* Github contains code alongside tests and documentation
* A large language model (based on GPT-3) is the basis 
</section>

 <section data-markdown class="fragments">
### Issues in Supervised Learning

* linear effort in labelling data
* significant error rate to be expected
  * all standard data sets contain up to 10% of errors
  * https://labelerrors.com/
* differences between different labelers
* change in label definition might require to start all over

_impractical with large data sets_
</section>

 <section data-markdown class="fragments">
### Foundation Models: Transformer Core ideas

1. have a generalized language model
1. predict probabilities of sequences of words
1. train on a very large corpus
1. zero- or one-shot learning
1. self-attention for encoding long range dependencies
1. self-supervision for leveraging large unlabeled datasets (aka unsupervised pre-training)
1. additional supervised training for downstream tasks, e.g.
    - translation (lang1 & lang2 pairs)
    - question answering (Q&A pairs)
    - sentiment analysis (text & mood pairs) 
    - etc.
  
  <!-- https://www.youtube.com/watch?v=iFhYwEi03Ew -->
  </section>
    
<section data-markdown class="fragments">
### Transformer Zoo

* the original transformer was meant for translation tasks
* usage has broadened ever since
* spawning a whole zoo of transformers
* some use encoder only
* some use decoder only
* some use a combination of encoder/decoder just like the original transformer 

<!-- https://huggingface.co/transformers/model_summary.html
https://huggingface.co/transformers/#supported-models -->
</section>

<section data-markdown>
<textarea data-template>
### Transformer Architecture: Encoder/Decoder

<div style="display: flex; align-items: center; justify-content: space-around;">
  <div>
    <ul>
      <li>left side is encoder</li>
      <li>right side is decoder</li>
      <li>encoder feeds in embeddings of input into decoder</li>
      <li>decoder needs context to work</li>
    </ul>
</div>
  <div>
    <img src='img/transformers/transformer-encoder-decoder.png' >
</div>
</div>
<small>

https://arxiv.org/pdf/1706.03762.pdf
</small>

        </textarea>
</section>

<section data-markdown class="fragments">
### Decoder only (GPT-like)

_also called auto-regressive Transformer models_

* the decoder part can transform given inputs into complete sentences
* e.g. useful in itself, to complete started sentences
* generates a response iteratively ("auto regressive")
* GPT would be an example for this kind of application 
  * unidirectional: trained to predict next word
  * by OpenAI 
  
  </section>
    
  <section data-markdown>
  ### Training GPT
  
  * self-supervised training
  * predict the next word, given all of the previous words within some text
  * has a limited context
  
  https://huggingface.co/transformers/model_summary.html#original-gpt
  https://huggingface.co/transformers/model_doc/gpt2.html
  </section>
  
<section data-markdown>
<textarea data-template>
<img src='img/transformers/gpt-3-2-years.jpg' style="height: 450px;">

<small>

  https://twitter.com/EMostaque/status/1530554442835189761</small>
        </textarea>
</section>
    

<section data-markdown class="fragments">
### Evolution of GPT

GPT: Generative Pre-Trained Transformer

* GPT-1: 2018, 110 million parameters (https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), https://www.youtube.com/watch?v=LOCzBgSV4tQ
* GPT-2: 2019, 1.5 billion parameters (https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), https://www.youtube.com/watch?v=BXv1m9Asl7I
* GPT-3: 2020, 175 billion parameters (https://arxiv.org/abs/2005.14165), https://www.youtube.com/watch?v=wYdKn-X4MhY
* GPT-4: 2022, probably not much larger, but trained on more data and more context (4096 instead of 2048) (https://analyticsindiamag.com/gpt-4-sam-altman-confirms-the-rumours/)

</section>
  
<section data-markdown>
<textarea data-template>
### Typical example for decoder only GPT: completing a text (zero-shot)

<img src='img/transformers/gpt-3-article.png' style="height: 450px;">

<small>

https://arxiv.org/abs/2005.14165</small>
        </textarea>
</section>
    
<section data-markdown>
  <textarea data-template>
<img src='img/transformers/twitter-fchollet-one-shot.png'>

<small>

  https://twitter.com/fchollet/status/1528069621047128065</small>
        </textarea>
  </section>
  
<section data-markdown>
<textarea data-template>
### Completion (One-Shot / Few-Shot)

<img src='img/transformers/gpt-few-shot.png' style="height: 450px;">

<small>

https://arxiv.org/abs/2005.14165</small>
        </textarea>
</section>

<section data-markdown>
<textarea data-template>
### Interactive Teaching

<img src='img/transformers/gpt-math.jpg' style="height: 450px;">

<small>

  https://twitter.com/peterwildeford/status/1522633978305560576
  <br>
  https://twitter.com/kaushikpatnaik/status/1522794898805592066
</small>
        </textarea>
</section>

<section data-markdown class="fragments">
### Don't forget: Transformers are language models

* No abstract reasoning like it is in our brains takes place
* The basis is the expression of thoughts in texts and code, etc.
* That's the way the system is trained
* Whether this is also intelligent is a pointed question
* Turing would probably say it doesn't matter
* One can argue that this system passes his test
  * https://twitter.com/glouppe/status/1438496208343949318
</section>

<section data-markdown class="fragments">
### On the Opportunities and Risks of Foundational Models

* foundational models: trained on broad data at scale and are adaptable to a wide range of downstream tasks
* ML is undergoing a paradigm shift with the rise of these models
* their scale results in new emergent capabilities 
* defects of the foundation model are inherited by all the adapted models downstream
* lack of clear understanding of how they work, when they fail, and what they are even capable of

https://arxiv.org/abs/2108.07258
</section>

<!-- <section data-markdown>
### Why it might make sense to study transformers even when you are not into NLP
  
So even though I'm technically in vision, papers, people and ideas across all of AI are suddenly extremely relevant. 
Everyone is working with essentially the same model, so most improvements and ideas can "copy paste" rapidly across all of AI.

https://twitter.com/karpathy/status/1468370611797852161 
</section> -->

<section data-markdown>
<textarea data-template>
### Foundational Models are not for text only: DALL·E generating images from text  

_DALL·E 2 is a new AI system that can create realistic images and art from a description in natural language._

<img src='img/transformers/dalle2-wifi.jpeg' style="height: 350px;">

<small>

https://twitter.com/benjamin_hilton/status/1519417377720524800
<br>
https://twitter.com/bakztfuture/status/1520576631945015297
<br>
https://openai.com/dall-e-2/
<br>
https://twitter.com/osanseviero/status/1526849837559205888
<br>
https://twitter.com/egrefen/status/1532661064898334722
</small>
        </textarea>
</section>

<!-- <section data-markdown class="fragments" style="font-size: x-large;">
## More cool stuff

* GPT-3: beta, but no longer private beta: https://beta.openai.com/examples
  * https://beta.openai.com/codex-javascript-sandbox
* large language models (like GPT-3) to solve grade school math problems much more effectively: https://openai.com/blog/grade-school-math/#samples
* Introducing the 540 billion parameter Pathways Language Model. Trained on two Cloud #TPU v4 pods, it achieves state-of-the-art performance on benchmarks and shows exciting capabilities like mathematical reasoning, code writing, and even explaining jokes.
  * https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf
  * Examples on page 38
* True general intelligence requires models that can not only read and write, but act in a way that is helpful to users. That’s why we’re starting Adept: we’re training neural networks to use every software tool and API in the world.  
  * https://twitter.com/jluan/status/1519035169537093632  
  * https://twitter.com/AdeptAILabs/status/1518975477917962245
* https://www.heise.de/hintergrund/Missing-Link-Was-wir-ueber-die-Fairness-der-Welt-von-moderner-KI-lernen-koennen-6351026.html
* Gato🐈a scalable generalist agent that uses a single transformer with exactly the same weights to play Atari, follow
text instructions, caption images, chat with people, control a real robot arm, and more: https://dpmd.ai/Gato (https://twitter.com/DeepMind/status/1524770016259887107)

</section> -->

<section data-markdown class="fragments">
### Are Foundation Models Conscious?

* Phenomenal consciousness = Does it have an inner cinema?
* Self-consciousness = Is it aware of itself?
* Sentience = Can it have positive or negative experiences?
* Moral patienthood = Should we care about what we do to it?
* Moral agency = Should we hold it accountable for what it does?

* https://twitter.com/AmandaAskell/status/1493086389549862915
* https://www.heise.de/hintergrund/Hat-KI-bereits-eine-Art-Bewusstsein-entwickelt-Forscher-streiten-darueber-6522868.html         
* https://askellio.substack.com/p/ai-consciousness
</section>
  
<section data-markdown>
<textarea data-template>

<img src='img/transformers/twitter-fchollet-org-conscious.png'>

https://twitter.com/fchollet/status/1522982582219988992
        </textarea>
</section>


<!-- <section data-markdown class="fragments">
### More Examples for applications in the corporate context

* Are nasty things circulating on social media about your company?
* Summary of (scientific) articles
* Classification of incoming mail (email)
* Summarization: long on short texts (product description)
* What is your example?

</section>
 -->

 <!-- <section data-markdown>
  <textarea data-template>
### Big number of new large models emerging

<img src="img/transformers/large-models.png">

<small>

https://www.reddit.com/r/GPT3/comments/ub7g19/7_new_large_language_models_released_in_the_last/
</small>
</textarea>
</section>

<section data-markdown class="fragments" style="font-size: xx-large;">
    <textarea data-template>
### New approaches emerging

* large language models (like GPT-3) to solve grade school math problems much more effectively: https://openai.com/blog/grade-school-math/#samples
* Pathways Language Model shows exciting capabilities like mathematical reasoning, code writing, and even explaining jokes.
  * https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf
  * Examples on page 38
* Humans give written feedback on tasks (here summarization): https://ethanperez.net/feedback.pdf
</textarea>
</section> -->

<section data-markdown>
<textarea data-template>
### The Future according to the head of OpenAI

<img src='img/transformers/altman-twitter.png' style="height: 450px;">

<small>

https://twitter.com/sama/status/1520798948562141184
</small>
        </textarea>
</section>

<section data-markdown>
<textarea data-template>
<img src='img/transformers/prompting-artist.jpg' style="height: 450px;">

<small>

  https://twitter.com/LimnDigital/status/1528025223974596608
</small>
        </textarea>
</section>

<section data-markdown>
<textarea data-template>
### Prompt Engineering as the new way of Software Development?

<img src='img/prompt-engineering.jpg' style="height: 450px;">

<small>

https://twitter.com/karpathy/status/1526386672165744640
<br>
https://beta.openai.com/docs/guides/completion/prompt-design
</small>
        </textarea>
</section>

<section data-markdown>
  <textarea data-template>
<img src='img/transformers/mikio-prompting.png' style="height: 450px;">

<small>

https://twitter.com/mikiobraun/status/1530463798318059520</small>
      </textarea>
</section>
  
<section data-markdown>
  <textarea data-template>
<img src='img/transformers/karpathy-llm-alien-artefact.png' style="height: 450px;">

<small>

https://twitter.com/karpathy/status/1529514197121384448
<br>
https://twitter.com/arankomatsuzaki/status/1529285884817707008
</small>
        </textarea>
</section>
  

<section data-markdown class="fragments">
  <textarea data-template>
## What could be working soon?

- Detecting Code Smells with Deep Learning
- Better software through ML and DS
  - 2 aspects: replace by ML, better analysis by DS
- GPT-3 writes code
  - in a few years the IDE can also tell if the comment should be adjusted when you change the code, so they don't diverge
  - or when reading, give a warning: comment seems not up to date
<!-- - Has anyone ever heard of someone training a supervised learning model to reproduce the results of some legacy code they don't want to maintain, and then replacing the code with the model? (https://twitter.com/seanjtaylor/status/1443361903229562881) -->
- But: building or analyzing a complete application is a different story than generating a button or commenting on a code snippet. 
- And: Could such a model ever be innovative?
</textarea>
</section>
  
<!-- 
  
<section data-markdown>
  <textarea data-template>
### Call to Action

Überlegt euch
* Wo sind wir Menschen besser als Maschinen?
* Wie kann mit solchen Modellen die Softwareentwicklung verändern? 
* Wo sind die Grenzen und die Gefahren?
</textarea>
</section>
 -->

 <section data-markdown>
  <textarea data-template>
### Feedback an den Veranstalter

<img src="img/QR_Code_nordic_Coding.png">

https://app.couchsurvey.com/s/ngkz8kD
  </textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Vielen Dank, Zeit für Fragen und Diskussion

Paradigmenwechsel im Bereich Machine Learning - Sprachmodelle und mehr

Nordic Coding

Bleibt gern im Kontakt

Oliver Zeigermann

https://www.linkedin.com/in/oliver-zeigermann-34989773/

oliver.zeigermann@openknowledge.de

Twitter: @DJCordhose

Diese Folien: https://bit.ly/nordic-ml-2022

    </textarea>
</section>
</div>
</div>

<script src="reveal.js/dist/reveal.js"></script>
<script src="lib/jquery.js"></script>
<script>
  const printMode = window.location.search.match(/print-pdf/gi);
  const isLocal = window.location.hostname.indexOf('localhost') !== -1 ||
    window.location.hostname.indexOf('127.0.0.1') !== -1;
  const isPresentation = isLocal && !printMode;
  const isPublic = !isPresentation;

  $('.hide').remove();

  if (isPresentation) {
  } else {
    // only applies to public version
    $('.todo').remove();
    $('.preparation').remove();
    $('.local').remove();
  }

  Reveal.addEventListener('ready', function (event) {
    // applies to all versions
    $('code').addClass('line-numbers');

    $('.fragments li').addClass('fragment')

    // make all links open in new tab
    $('a').attr('target', '_blank')

    if (isPresentation) {
      // only applies to presentation version
      Reveal.configure({ controls: false });
    } else {
      // only applies to public version
      $('.fragment').removeClass('fragment');
    }

    // we do not like fragments
    // $('.fragment').removeClass('fragment');

  });

</script>

<script src="reveal.js/plugin/notes/notes.js"></script>
<script src="reveal.js/plugin/markdown/markdown.js"></script>
<script src="reveal.js/plugin/highlight/highlight.js"></script>
<script>
  // More info about initialization & config:
  // - https://revealjs.com/initialization/
  // - https://revealjs.com/config/
  Reveal.initialize({
    hash: true,
    controls: true,
    progress: true,
    history: true,
    center: true,
    width: 1100,
    slideNumber: true,
    hideInactiveCursor: false,


    // Learn about plugins: https://revealjs.com/plugins/
    plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
  });
</script>


</body>

</html>